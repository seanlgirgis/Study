# Kubernetes
### The English Version

---

## What is Kubernetes and Why Does it Exist?

When microservices became the dominant architecture pattern, companies suddenly had a new problem. Instead of one monolith to deploy and manage, they had dozens or hundreds of individual services. Each service needed to be deployed, kept running, scaled up when traffic increased, scaled down when traffic decreased, restarted when it crashed, updated without downtime, and connected to other services reliably.

Doing this manually was impossible at scale. Imagine a Capital One engineer manually SSHing into servers to restart crashed containers, manually spinning up new instances when transaction volume spiked on payday, manually routing traffic away from failed instances. At hundreds of services and thousands of container instances this becomes a full-time job for an army of people.

Google had been running containers at massive scale internally since 2003 using an internal system called Borg. In 2014 they open-sourced a new version of those ideas and called it Kubernetes — from the Greek word for helmsman or ship pilot. The logo is a ship's wheel. The metaphor is deliberate — Kubernetes steers your containers to their destination and keeps them there.

The core promise of Kubernetes is simple — **you tell it what you want, and it makes it happen and keeps it that way.** You say "I want 10 instances of my fraud detection service always running." Kubernetes starts 10 instances, watches them, restarts any that crash, and maintains exactly 10 at all times. You never SSH into a server. You declare the desired state and Kubernetes continuously reconciles reality to match it.

---

## The Container Problem Kubernetes Solves

Before understanding Kubernetes you need to understand why containers created the problem in the first place.

Docker made it trivially easy to package a service and all its dependencies into a portable container image. The fraud detection service, its Python runtime, its ML libraries, its configuration — all packaged into one image that runs identically everywhere.

But Docker alone only runs containers on one machine. In production you need containers running across many machines for redundancy and scale. You need to decide which machine each container runs on. You need to move containers when machines fail. You need to distribute traffic across multiple container instances. You need to roll out updates without downtime.

Docker Compose handles multiple containers on one machine. Docker Swarm handles basic multi-machine orchestration. But neither scales to the complexity and reliability requirements of a company like Capital One running hundreds of services across thousands of machines in multiple AWS regions.

Kubernetes was built specifically for this level of complexity.

---

## Core Kubernetes Concepts in Plain English

**Cluster** — the entire Kubernetes environment. A collection of machines — called nodes — that Kubernetes manages as a single unit. Capital One would run multiple clusters — one per environment (development, staging, production) and potentially one per AWS region.

**Node** — a single machine in the cluster. Could be a physical server or a virtual machine. In AWS this is an EC2 instance. Nodes are where containers actually run. Nodes come in two types — worker nodes run your application containers, control plane nodes run Kubernetes itself.

**Pod** — the smallest deployable unit in Kubernetes. A pod wraps one or more containers that always run together on the same node and share network and storage. In practice most pods contain one container. Think of a pod as a thin wrapper around your container that gives it a Kubernetes identity.

**Deployment** — tells Kubernetes how many copies of a pod to run and how to update them. "Run 10 instances of the fraud detection service, always." The deployment controller watches the cluster and reconciles — if one pod crashes it starts a new one, if you ask for 15 instead of 10 it starts 5 more.

**Service** — gives a stable network address to a set of pods. Pods are ephemeral — they crash and restart with new IP addresses. A Kubernetes Service provides a consistent DNS name and IP that other services use to find and talk to your pods, regardless of which specific pod instances are running at any moment.

**Namespace** — logical partitioning within a cluster. Capital One might have namespaces for fraud-detection, transaction-processing, and customer-profiles — keeping resources organized and isolated within one cluster.

**ConfigMap and Secret** — configuration management. ConfigMaps store non-sensitive configuration. Secrets store sensitive data like database passwords and API keys. Both are injected into pods at runtime so your container image contains no environment-specific configuration.

**Ingress** — manages external access to services within the cluster. Routes incoming HTTP and HTTPS traffic from outside the cluster to the correct internal service based on URL paths or hostnames.

---

## The Control Plane — Kubernetes Brain

The control plane is the set of components that make cluster-wide decisions — scheduling, detecting and responding to cluster events, managing state.

**API Server** — the front door to Kubernetes. Every interaction with the cluster goes through the API server — kubectl commands, internal components, monitoring systems. It validates requests and updates cluster state in etcd.

**etcd** — the cluster's distributed key-value store. The single source of truth for all cluster state — what pods exist, what nodes are available, what deployments are defined. If etcd goes down the cluster stops making decisions but running workloads continue.

**Scheduler** — watches for new pods that have no assigned node and selects the best node for them based on resource availability, affinity rules, and constraints.

**Controller Manager** — runs the controllers that reconcile desired state with actual state. The ReplicaSet controller ensures the right number of pod replicas are running. The Node controller monitors node health. If you asked for 10 fraud detection pods and one crashes, the controller manager notices and creates a replacement.

---

## Worker Nodes — Where Your Code Runs

**kubelet** — the agent running on every worker node. It receives instructions from the control plane and manages pods on that node — starting containers, stopping them, reporting their health back to the control plane.

**kube-proxy** — handles network routing on each node. Maintains network rules that allow communication to pods from inside and outside the cluster.

**Container Runtime** — the software that actually runs containers. containerd is the standard. Docker was previously used but Kubernetes removed direct Docker support in favor of containerd which is lighter and purpose-built for production container workloads.

---

## Key Kubernetes Patterns for Data Engineering

**Horizontal Pod Autoscaling** — automatically scales the number of pod replicas based on CPU, memory, or custom metrics like Kafka consumer lag. On payday when transaction volume spikes, Kubernetes automatically adds more transaction processing pods. When volume drops overnight it scales back down, saving cost.

**Rolling Updates** — deploy a new version of your service with zero downtime. Kubernetes starts new pods with the updated version, waits for them to become healthy, then terminates old pods — one batch at a time. If the new version has a bug, rollback is one command.

**Resource Requests and Limits** — every pod declares how much CPU and memory it needs and how much it's allowed to use. This lets the scheduler pack pods efficiently onto nodes and prevents one runaway service from starving others of resources.

**Liveness and Readiness Probes** — Kubernetes periodically checks if each pod is healthy. A liveness probe restarts the pod if it's stuck. A readiness probe removes the pod from load balancing if it's not ready to serve traffic — for example, during startup while a model is loading.

---

## Kubernetes at Capital One

Capital One runs Kubernetes on AWS EKS — Elastic Kubernetes Service. EKS manages the control plane entirely — Capital One never manages API servers or etcd directly. Worker nodes run on EC2 instances or AWS Fargate for serverless containers.

The data engineering platform runs on this Kubernetes foundation. Spark jobs run as Kubernetes-native applications — the Spark driver runs as a pod, executor pods are dynamically provisioned and terminated. Kafka Connect connectors run as pods. Flink jobs run as pods. The entire data pipeline from ingestion to storage is orchestrated by Kubernetes.

This is why Kubernetes is on the Capital One job description. As a Lead Data Engineer you're not just writing Spark code — you're deploying and operating data pipelines in a Kubernetes environment. Understanding deployments, resource management, autoscaling, and observability in Kubernetes is essential to your role.

---

## Kubernetes vs Traditional VM Deployment

```
TRADITIONAL VM                      KUBERNETES
──────────────                      ──────────
Manual provisioning                 Declarative configuration
Static capacity                     Dynamic autoscaling
SSH to manage                       kubectl to manage
Hours to deploy                     Seconds to deploy
Manual failover                     Automatic self-healing
One app per VM typically            Many pods per node
Environment inconsistency           Identical environments
Expensive overprovisioning          Efficient bin-packing
```

---


