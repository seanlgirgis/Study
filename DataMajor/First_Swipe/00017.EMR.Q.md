# Mock Interview â€” Amazon EMR

Same rules â€” answer in the room, model answer and diagrams after each response.

---

## Question 1 â€” Warming Up

**"I see you've used Hadoop and Spark. Can you explain how running Spark on Amazon EMR is different from running it on an on-premise Hadoop cluster? Specifically, how does the architecture of storage and compute change?"**

# Feedback First

## What You Nailed âœ…
*   **Decoupled Storage and Compute**: This is the single most important concept. On-prem, they are coupled (data is on the worker nodes). On EMR, they are separate (S3 for storage, EC2 for compute).
*   **Transient vs Persistent**: Correctly identified that on-prem clusters run 24/7, while EMR clusters can be spun up for a job and killed immediately.
*   **Cost Model**: "Paying only for what you use" vs "CapEx investment in racks of servers."
*   **S3 acts as HDFS**: Correctly noted that S3 replaces HDFS as the primary storage layer.

## What to Tighten Up ğŸ”§
*   *EMRFS vs S3*: You said "reads from S3." Technically, EMR uses **EMRFS** (EMR File System), which is an implementation of HDFS that acts as a connector to S3. It provides strong consistency features (consistent view) which raw S3 sometimes lacks (though S3 is now strongly consistent, EMRFS has optimizations).
*   *The HDFS Role on EMR*: Don't imply HDFS is *gone* on EMR. It still exists on the Core Nodes for intermediate storage (shuffle data, spill to disk), even if the persistent data is in S3.
*   *Spot Instances*: Mentioning usage of Spot Instances on Task Nodes is a huge "senior engineer" detail for EMR. It saves massive amounts of money.

# Model Answer

---

"The biggest shift from on-prem Hadoop to EMR is the **decoupling of storage and compute**.

**On-Premise (Coupled):**
In a traditional Cloudera/Hortonworks cluster, storage and compute are bound together. The worker nodes run the computation (Spark Executors) AND hold the data (HDFS DataNodes).
*   **Pro:** Data locality (compute runs where data lives).
*   **Con:** You cannot scale them independently. If you need more storage but have enough CPU, you still have to buy expensive servers with both. You also typically leave the cluster running 24/7.

**Amazon EMR (Decoupled):**
On EMR, **S3** is the persistent storage layer, effectively replacing HDFS for the source of truth.
*   **Compute (EMR):** We spin up EC2 instances to process data.
*   **Storage (S3):** We store petabytes of data cheaply in S3.
*   **The Win:** We treat the cluster as **transient**. We can spin up a 100-node cluster for a heavy nightly batch job, run it for 2 hours, and terminate it. We pay for 200 compute-hours, not 24/7 usage.

**Architecture Nuance:**
While S3 holds the source data, EMR still uses HDFS on the Core Nodes for **intermediate data** (shuffling, temporary spills). We often use **Spot Instances** for Task Nodes (pure compute) to reduce costs by up to 80%, knowing that if they are reclaimed by AWS, the job can recover because the persistent data allows is safe in S3"

# Diagrams

```text
COUPLED VS DECOUPLED
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ON-PREMISE (Coupled)              AMAZON EMR (Decoupled)             â•‘
â•‘                                                                       â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â•‘
â•‘  â”‚ SERVER        â”‚                 â”‚ EC2 INSTANCE  â”‚                  â•‘
â•‘  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                 â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                  â•‘
â•‘  â”‚ â”‚ COMPUTE   â”‚ â”‚                 â”‚ â”‚ COMPUTE   â”‚ â”‚                  â•‘
â•‘  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                 â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                  â•‘
â•‘  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”˜                  â•‘
â•‘  â”‚ â”‚ STORAGE   â”‚ â”‚                         â”‚ network (slower)         â•‘
â•‘  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                         â–¼                          â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â•‘
â•‘                                    â”‚ S3 BUCKET     â”‚                  â•‘
â•‘  If Server dies,                   â”‚ (Storage)     â”‚                  â•‘
â•‘  Data is gone âŒ                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â•‘
â•‘  (Need 3x replicas)                                                   â•‘
â•‘                                    If EC2 dies,                       â•‘
â•‘                                    Data is safe âœ…                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## Question 2 â€” Data Engineer Level

**"Explain the different node types in an EMR clusterâ€”Master, Core, and Task. Why would I choose to put Spot Instances on Task nodes but not master nodes?"**

# Feedback First

## What You Nailed âœ…
*   **Master Node definitions**: Correctly identified it runs the ResourceManager and NameNode. "The Brain."
*   **Core vs Task distinction**: Correctly noted that Core nodes have storage (HDFS) and Task nodes do not.
*   **Spot Instance logic**: "Master node dies, cluster dies." Perfect. "Task node dies, job just retries." Perfect.

## What to Tighten Up ğŸ”§
*   *Data Loss on Core Nodes*: If you lose a Core Node (which runs HDFS), you *risk* data loss if the replication factor isn't high enough or if too many failing nodes take down the HDFS volume. That's why we generally don't use Spot for Core nodes unless we are very careful.
*   *YARN Container Rescheduling*: Be specific about the mechanism. If a Task node (Spot) is reclaimed, YARN notices the container failure and simply reschedules that specific task on a healthy node. It slows the job down slightly but doesn't crash it.

# Model Answer

---

"An EMR cluster consists of three node types with distinct roles:

1.  **Master Node:**
    *   **Role:** The coordinator. Runs the YARN ResourceManager and HDFS NameNode. It manages the cluster health and job scheduling.
    *   **Instance Type:** Usually On-Demand or Reserved.
    *   **Why not Spot?** If the Master node goes down, the entire cluster is lost. There is no recovery state for the cluster itself (though data in S3 is safe).

2.  **Core Nodes:**
    *   **Role:** Workers that also provide HDFS storage. They run DataNodes and YARN NodeManagers.
    *   **Instance Type:** Usually On-Demand.
    *   **Why not Spot?** Removing a Core node effectively removes HDFS disk space. If a Core node is reclaimed abruptly, you might lose intermediate shuffle data or corrupt the namespace if replication is insufficient, causing the Spark job to fail.

3.  **Task Nodes:**
    *   **Role:** Pure computation workers. No HDFS storage. They only run YARN NodeManagers/Spark Executors.
    *   **Instance Type:** **Spot Instances**.
    *   **Why Spot?** This is the sweet spot for cost optimization. If AWS reclaims a Task node, YARN simply detects the failed container/task and reschedules it on another node. The job continues with slightly reduced capacity. This allows Capital One to run massive scale jobs for a fraction of the price."

# Diagrams

```text
EMR NODE TYPES & SPOT STRATEGY
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  MASTER NODE (1)               CORE NODES (Min 2)                     â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â•‘
â•‘  â”‚ ResourceManager â”‚           â”‚ DataNode     â”‚   â”‚ DataNode     â”‚    â•‘
â•‘  â”‚ NameNode        â”‚           â”‚ NodeManager  â”‚   â”‚ NodeManager  â”‚    â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”˜    â•‘
â•‘           â”‚ controls                  â”‚ stores data       â”‚           â•‘
â•‘           â–¼                           â–¼                   â–¼           â•‘
â•‘  MUST BE ON-DEMAND âœ…          MUST BE ON-DEMAND âœ…                   â•‘
â•‘  (Brain death =              (Storage death =                         â•‘
â•‘   Cluster death)              Data loss)                              â•‘
â•‘                                                                       â•‘
â•‘  -------------------------------------------------------------------  â•‘
â•‘                                                                       â•‘
â•‘  TASK NODES (0 to 1000+)                                              â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â•‘
â•‘  â”‚ NodeManager  â”‚   â”‚ NodeManager  â”‚   â”‚ NodeManager  â”‚               â•‘
â•‘  â”‚ (Compute)    â”‚   â”‚ (Compute)    â”‚   â”‚ (Compute)    â”‚               â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜               â•‘
â•‘                                                â”‚ AWS RECLAIMS         â•‘
â•‘  CAN BE SPOT INSTANCES âœ…                      â–¼                      â•‘
â•‘  (Node death = Just reschedule          Instance vanishes ğŸ’¥          â•‘
â•‘   task on another node)                 Job continues! âœ…             â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## Question 3 â€” Lead Data Engineer Level

**"At Capital One, we use both Glue and EMR. How do you decide which one to use for a new pipeline?"**

# Feedback First

## What You Nailed âœ…
*   **Serverless vs Managed**: Glue is serverless (hands-off), EMR is managed EC2 (hands-on).
*   **Maintenance Overhead**: Glue needs no patching/AMI updates. EMR requires you to pick versions and manage the cluster lifecycle.
*   **Flexibility**: EMR lets you tune JVM settings and install weird dependencies. Glue is more locked down.

## What to Tighten Up ğŸ”§
*   *Cost at Scale*: This is the big differentiator for Lead roles. Glue charges a premium per DPU (Data Processing Unit). EMR is just raw EC2 costs + a small EMR fee.
    *   *Insight:* For constant, massive workloads (running 24/7 or massive batches), **EMR is significantly cheaper**. Glue becomes expensive at high scale. EMR requires devops effort, but that effort pays for itself in raw compute savings for huge jobs.
*   *Startup Time*: Glue has historically had slow "cold start" times (though Glue 3.0/4.0 improved this). EMR can start faster if you keep a persistent cluster, or take 5-10 mins to bootstrap.

# Model Answer

---

"It comes down to a trade-off between **Control/Cost** vs. **Convenience**.

**Choose AWS Glue when:**
*   **The job is Standard ETL:** Standard transformations, reading from S3/JDBC, writing to S3.
*   **Sporadic/Unpredictable Workloads:** You don't want to manage scaling logic. You just want code to run when data arrives.
*   **Team Ops Capacity is Low:** The team focuses on SQL/Python and doesn't want to manage infrastructure, rotation, or patching.
*   **Integration:** You rely heavily on the Glue Data Catalog and Glue Crawlers.

**Choose Amazon EMR when:**
*   **Cost at Scale:** For massive, long-running jobs, Glue's premium pricing adds up. EMR (especially with Spot instances) is far cheaper per compute-hour.
*   **Deep Customization:** You need specific kernel versions, custom YARN tuning, or specific libraries that are hard to bundle in Glue.
*   **Long-Running Clusters:** You want a persistent cluster for ad-hoc querying (via Zeppelin/Jupyter) or low-latency start times.
*   **Migration:** You are lifting and shifting legacy Hadoop/Spark code that implies specific HDFS paths or configurations that Glue doesn't support easily.

**Summary:** Start with Glue for simplicity. Graduate to EMR if costs spiral or technical constraints force you to take control of the metal."

# Diagrams

```text
DECISION MATRIX: EMR VS GLUE
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  FACTOR              AWS GLUE (Serverless)    AMAZON EMR (Managed)    â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â•‘
â•‘  Ops Effort          Zero (None) âœ…           Medium (AMI, Scaling)   â•‘
â•‘                                                                       â•‘
â•‘  Cost (Small Scale)  Low                      High (Min cluster size) â•‘
â•‘                                                                       â•‘
â•‘  Cost (Huge Scale)   High (DPU Premium)       Low (Spot Instances) âœ… â•‘
â•‘                                                                       â•‘
â•‘  Flexibility         Restricted               Full Root Access âœ…     â•‘
â•‘                                                                       â•‘
â•‘  Start Time          Cold Start (~1 min)      Instant (if persistent) â•‘
â•‘                                                                       â•‘
â•‘  Ideal For           Standard ETL,            Massive Batch,          â•‘
â•‘                      Event-driven             Machine Learning,       â•‘
â•‘                      pipelines                Cost optimization       â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```
