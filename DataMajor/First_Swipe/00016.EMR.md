# Amazon EMR (Elastic MapReduce)
### The English Version

---

## What is Amazon EMR?

**Short Answer:** **EMR is 100% an AWS-specific product.** You cannot download "EMR" and run it on your laptop or on Azure.

However, **what EMR runs is NOT AWS-specific.**

EMR is simply AWS's managed platform for running open-source big data frameworks like **Apache Spark, Apache Hadoop, Apache Hive, Apache Flink, and Presto**.

Think of EMR like a "Spark-as-a-Service" rental.
- **Without EMR:** You buy 50 servers, install Linux, install Java, install Hadoop, configure the cluster, handle security, patch the OS, and fix it when the namenode crashes.
- **With EMR:** You click a button (or run a script), and AWS gives you a fully configured cluster with Spark and Hadoop installed in 5 minutes. When you're done, you terminate it and stop paying.

---

## Why Does EMR Exist?

In the early days of Big Data (2008-2012), companies bought racks of physical servers to run Hadoop. This was:
1.  **Expensive:** You had to buy enough servers for your peak load (e.g., Black Friday), which sat idle the rest of the year.
2.  **Hard:** Managing a physical Hadoop cluster requires a team of specialized ops engineers.
3.  **Slow:** Adding capacity took months of procurement.

AWS launched EMR (Elastic MapReduce) to solve this. It decouples **Compute** (EC2) from **Storage** (S3).

**The Old Way (HDFS on Premise):**
Data lives on the hard drives of the worker nodes. If you shut down the cluster, you lose the data. You must keep the cluster running 24/7 just to keep the data safe.

**The EMR Way (S3-backed):**
Data lives in S3 (cheap, durable, infinite). The EMR cluster is just for *processing*.
1.  Spin up EMR cluster.
2.  Read data from S3.
3.  Process it (Spark/Hive).
4.  Write results back to S3.
5.  **Terminate the cluster.**

This "Transient Cluster" model is the defining characteristic of EMR. You don't keep it running. You treat the cluster as a temporary resource, just like a function call.

---

## Architecture: EMR vs Serverless (Glue)

At Capital One, you will see both **EMR** and **AWS Glue**. You need to know the difference.

**Amazon EMR:**
- **You control the hardware:** You choose the EC2 instance types (e.g., r5.4xlarge for memory, c5.4xlarge for compute).
- **You control the configuration:** You can tune `spark.executor.memory`, install custom libraries, and SSH into the master node.
- **Best for:** Heavy, predictable, massive-scale jobs where you need total control to optimize cost and performance.

**AWS Glue:**
- **Serverless:** You just write code. AWS picks the hardware.
- **No infrastructure:** You cannot choose instance types or SSH into nodes.
- **Best for:** ETL jobs, unpredictable workloads, or teams that don't want to manage *any* infra.

**Capital One Rule of Thumb:**
- Standard ETL pipelines -> **Glue** (easier).
- Massive, complex data crunching or ML processing -> **EMR** (cheaper at scale, more control).

---

## Core EMR Concepts

**Master Node:**
The brain. Runs the YARN ResourceManager and the HDFS NameNode. Keeps track of the cluster.

**Core Nodes:**
The workers that *also* store data. They run tasks (Spark executors) and run the HDFS DataNode daemon. If you use HDFS on EMR (rare now, mostly S3), data lives here.

**Task Nodes:**
Pure workers. They run tasks but *do not* store data in HDFS. You can add/remove these instantly without risking data loss. EMR uses these for **Spot Instances** (cheap, fleeting instances) to save 80% on cost.

---

## EMR Computing Options

1.  **EMR on EC2:** The classic mode. Clusters of VMs. You manage the cluster (kinda).
2.  **EMR on EKS:** Runs Spark/Flink on Kubernetes. Modern approach. Capital One uses this heavily to consolidate resources.
3.  **EMR Serverless:** A newer option similar to Glue but with EMR runtime features.

---

## Typical Capital One EMR Workflow

1.  **Ingest:** Data lands in S3 bucket `raw-zone` from Kafka.
2.  **Trigger:** Airflow (managed workflow) adds a Step to an existing EMR cluster or creates a new transient cluster.
3.  **Process:** Spark job reads `raw-zone`, cleans/transforms/aggregates data.
4.  **Enrich:** Job joins with static data (customer dimensions) also in S3.
5.  **Store:** Spark writes final Parquet files to `conformed-zone` in S3.
6.  **Catalog:** EMR updates the AWS Glue Data Catalog (metastore) so the new data is queryable via SQL.
7.  **Terminate:** Cluster shuts down (if transient).

---

## Competitors / Equivalents

If you leave AWS, you'll see the same open-source tools (Spark/Hadoop) managed by different vendors:

- **Google Cloud:** Dataproc
- **Azure:** HDInsight or Azure Databricks
- **On-Premise:** Cloudera Data Platform (CDP)

**EMR is just AWS's wrapper around the open-source ecosystem.**
