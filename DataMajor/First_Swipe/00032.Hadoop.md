# Hadoop (HDFS / MapReduce / YARN)
### The English Version

---

## What is Hadoop and Why Does it Exist?

**The Problem (2006):**
Google invented the internet search engine. They had to store the *entire internet* (Petabytes of HTML).
*   Hard Drive sizes were small (100GB).
*   To store 1 Petabyte, you needed 10,000 hard drives.
*   **The Nightmare:** Hard drives fail every day. Managing 10,000 drives manually is impossible. If one drive fails, you lose that part of the internet.

**The Solution:**
**Hadoop** is an Operating System for a cluster of commodity (cheap) servers. It makes 10,000 broken computers look like **One Giant Computer**.
*   **HDFS (Storage):** "I will chop your file into pieces and store 3 copies of each piece on different servers. If a server burns down, I don't care. I have 2 other copies."
*   **MapReduce (Compute):** "I will send the code to the data, instead of moving the data to the code."

**Analogy:**
*   **Postgres:** A single super-computer. Fast, but if it fills up, you're dead.
*   **Hadoop:** An army of ants. Individually weak and unreliable, but together they can move a mountain.

---

## Core Concepts in Plain English

**HDFS (Hadoop Distributed File System):**
*   **NameNode (The Brain):** Keeps the phone book. "File A, Part 1 is on Server 5." It holds metadata in RAM. *Single Point of Failure.*
*   **DataNode (The Worker):** Holds the actual hard drives and data blocks.
*   **Block Size:** Hadoop chops files into massive blocks (128MB). (Standard OS blocks are 4KB). This is optimized for streaming large files, not small random reads.

**YARN (Yet Another Resource Negotiator):**
*   The "Traffic Cop" of the cluster.
*   It decides who gets to use the CPU and RAM.
*   *Before YARN:* You could only run MapReduce.
*   *After YARN:* You can run Spark, Flink, and MapReduce simultaneously on the same hardware.

**MapReduce (The Old Engine):**
*   **Map:** Process each record independently (Filter, Transform).
*   **Shuffle:** Sort and group data by key (Network Heavy).
*   **Reduce:** Aggregate the groups (Count, Sum).
*   *Why it died:* It writes to disk after *every* step. It is extremely slow compared to Spark (In-Memory).

---

## Architecture / Deep Dive

**The 3-Replica Rule:**
Hadoop assumes hardware *will* fail.
1.  **Replica 1:** Written to the local node.
2.  **Replica 2:** Written to a different rack (in case the Rack Switch fails).
3.  **Replica 3:** Written to the same rack as Replica 2 (for bandwidth optimization).

**The "Small File" Problem (NameNode Memory Limit):**
*   The NameNode stores metadata for every file in RAM.
*   If you have 1 million files of 100MB, the NameNode is happy.
*   If you have 100 million files of 1KB, the NameNode runs out of RAM and the entire cluster crashes.
*   *Lesson:* HDFS hates small files.

---

## Hadoop at Capital One (Historical Context)

**1. The Data Lake before S3:**
*   Before we moved to AWS, Capital One had massive on-premise Hadoop clusters.
*   We dumped all mainframe logs and transaction history into HDFS.

**2. Legacy ETL Pipelines:**
*   Old risk models were written in Java MapReduce.
*   *Migration:* We rewrote them in PySpark on EMR.
*   *Benefit:* Jobs that took 14 hours in MapReduce now take 2 hours in Spark.

**3. Why we moved away:**
*   **Cost:** Maintaining 2,000 physical servers in a datacenter is expensive (Electricity, Cooling, Admins).
*   **Elasticity:** In Hadoop, if you need more space, you have to buy a truckload of hard drives and install them. In AWS S3, you just upload more files.

---

## Comparison Table

| Feature | Hadoop (On-Prem) | Modern Cloud (EMR + S3) |
| :--- | :--- | :--- |
| **Storage** | HDFS (Hard Drives on Nodes) | **S3** (Object Storage) |
| **Compute** | MapReduce (Disk-based) | **Spark** (Memory-based) |
| **Scaling** | Add Physical Nodes (Weeks) | **Auto-Scaling** (Minutes) |
| **Cost Model** | CapEx (Buy hardware upfront) | **OpEx** (Pay per second) |
| **Separation** | Coupled (Storage + Compute together) | **Decoupled** (Scale independently) |

---

## Visuals

```text
HDFS ARCHITECTURE (The NameNode BottleNeck)
╔═══════════════════════════════════════════════════════════════════════╗
║                                                                       ║
║  THE NAMENODE (Master)                                                ║
║  [RAM: Metadata Map]                                                  ║
║  "File A -> Block 1 -> Node 1, 2, 3"                                  ║
║  "File B -> Block 1 -> Node 4, 5, 6"                                  ║
║          │                                                            ║
║          │ (Heartbeats)                                               ║
║          ▼                                                            ║
║  THE DATANODES (Workers)                                              ║
║  ┌──────────────┐      ┌──────────────┐      ┌──────────────┐         ║
║  │ Node 1       │      │ Node 2       │      │ Node 3       │         ║
║  │ [Block A.1]  │◄────►│ [Block A.1]  │◄────►│ [Block A.1]  │         ║
║  │ [Block C.2]  │      │ [Block B.1]  │      │ [Block D.3]  │         ║
║  └──────────────┘      └──────────────┘      └──────────────┘         ║
║                                                                       ║
║  Replication Factor = 3                                               ║
║  If Node 1 dies, the data is safe on Node 2 and 3.                    ║
║                                                                       ║
╚═══════════════════════════════════════════════════════════════════════╝
```
