# Data Lakes (S3 / ADLS / Delta Lake)
### The English Version

---

## What is a Data Lake and Why Does it Exist?

**The Problem:**
Databases and Data Warehouses are expensive and "needy."
*   To store data in a Warehouse, you must first define a table, decide on data types, and clean the data to match that schema (**Schema-on-Write**).
*   If you have a 10TB JSON file from a new web app, you can't just "save it." You have to spend weeks writing an ETL pipeline to parse it first.

**The Solution:**
A **Data Lake** is a centralized repository that allows you to store all your structured and unstructured data at any scale.
*   **The Rule:** "Store everything, discard nothing."
*   **The Mechanism:** Just dump the file into S3/Blob Storage. You figure out how to read it later (**Schema-on-Read**).

**Analogy:**
*   **Data Warehouse:** A filing cabinet. You must label every document and put it in the right folder.
*   **Data Lake:** A Google Drive folder labeled "Misc". You just drag and drop everything there and search for it when you need it.

---

## Core Concepts in Plain English

**Schema-on-Read:**
*   You don't define the schema when you *save* the data.
*   You define the schema when you *query* the data (e.g., using Spark or Athena).
*   *Benefit:* Instant ingestion. Zero bottleneck.

**File Formats (The Secret Sauce):**
*   **CSV/JSON:** Human readable, but slow and heavy. Good for raw landing.
*   **Parquet/ORC:** Columnar, compressed, binary. Machine readable. **Fast**. This is the standard for analytics layers.
*   **Avro:** Row-based, great for streaming (Kafka) and schema evolution.

**Partitioning:**
*   Organizing files into folders to speed up queries.
*   `s3://my-bucket/sales/year=2023/month=01/day=01/data.parquet`
*   If you query "Jan 1st 2023", Spark only reads that one folder and skips the other 364. This is called **Partition Pruning**.

**The Medallion Architecture (Databricks standard):**
1.  **Bronze (Raw):** Exact copy of the source data. Dirty, duplicate, messy.
2.  **Silver (Clean):** Deduplicated, validated, types cast. Usable by Data Engineers.
3.  **Gold (Curated):** Aggregated, business-level data. Ready for Tableau/PowerBI.

---

## Architecture / Deep Dive

**Modern Data Lakehouse (Delta Lake / Iceberg / Hudi):**
Historically, Data Lakes were "swamps" because you couldn't update data (S3 is immutable). If you needed to fix one row, you had to rewrite the whole file.
New technologies (Table Formats) bring Database features to the Data Lake:
*   **ACID Transactions:** You can `UPDATE` and `DELETE` rows in a Parquet file.
*   **Time Travel:** You can query "What did this table look like yesterday?"
*   **Schema Enforcement:** It stops you from writing bad data to the Silver layer.

**The Catalog (Glue / Hive Metastore):**
*   S3 is just files. It doesn't know what a "Table" is.
*   The **Catalog** stores the metadata: "The table `users` is located in `s3://bucket/users/` and has columns A, B, C."
*   Engines like Spark, Athena, and Presto look at the Catalog to know where to find the files.

---

## Data Lakes at Capital One

**1. Raw Data Landing Zone:**
*   Every application dumps its raw logs to the Bronze Layer S3 bucket.
*   *Example:* Web server logs, mobile app clickstreams, raw mainframe EBCDIC files.

**2. Image & Document Storage:**
*   You can't store a PDF in a Database.
*   We store millions of Check Images and Monthly Statements (PDFs) in S3. Using object tags, we can enforce retention policies (delete after 7 years).

**3. Machine Learning Training Data:**
*   ML models need to see the "raw" data to find patterns.
*   Data Scientists read directly from the Data Lake using PySpark because Warehouses (Snowflake) can be too slow/expensive for scanning Petabytes of raw text.

---

## Comparison Table

| Feature | Data Lake (S3) | Data Warehouse (Snowflake) | Data Lakehouse (Delta) |
| :--- | :--- | :--- | :--- |
| **Data Types** | Any (Text, Video, CSV) | Structured Only | Structured + Semi |
| **Schema** | Schema-on-Read (Flexible) | Schema-on-Write (Strict) | Schema Enforcement |
| **Cost** | **$ (Lowest)** | $$$$ (High) | $$ (Medium) |
| **Users** | Data Scientists / Engineers | Business Analysts | Both |
| **Updates** | Difficult (Immutable) | Easy (SQL) | **Easy (Merge/Update)** |
| **Speed** | Slow (Scan) | Fast (Index) | Fast (Metadata) |

---

## Visuals

```text
THE MEDALLION ARCHITECTURE (Refining Oil into Gas)
╔═══════════════════════════════════════════════════════════════════════╗
║                                                                       ║
║  1. BRONZE (The Swamp)                                                ║
║  [Raw JSON] [Dirty CSV] [Trash?]                                      ║
║  "Just land the data so we don't lose it."                            ║
║         │                                                             ║
║         ▼ (Spark Job: Clean, Dedupe, Cast Types)                      ║
║                                                                       ║
║  2. SILVER (The Warehouse Interface)                                  ║
║  [Parquet / Delta Files]                                              ║
║  "Clean, tabular data. No duplicates. Correct Integers."              ║
║         │                                                             ║
║         ▼ (Spark Job: Aggregate, Join, Business Logic)                ║
║                                                                       ║
║  3. GOLD (The Boardroom)                                              ║
║  [Aggregated Sales] [KPI Dashboard Data]                              ║
║  "Ready for the CEO. Highly curated. Small size."                     ║
║                                                                       ║
╚═══════════════════════════════════════════════════════════════════════╝
```
