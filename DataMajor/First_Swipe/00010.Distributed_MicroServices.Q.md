# Mock Interview â€” Distributed Microservices




## Question 1 â€” Warm Up

**"Can you explain what microservices are, why they were created, and what specific problems they solve compared to a monolithic architecture? Use a banking context if possible."**

# Feedback First

---

## What You Nailed âœ…

**"Small, independent pieces that each do one job well"** â€” perfect one-line definition. Clean and memorable.

**Historical context â€” Netflix, Amazon, Uber** â€” correctly placed the origin in the right era with the right companies.

**The monolith problems** â€” slow updates, risky deployments, all-or-nothing scaling, team coordination overhead. All four correct.

**Failure isolation** â€” "fraud check fails but accounts still load" is exactly the right concrete example for a banking context.

**Independent scaling** â€” "scale just the transaction service on payday" is a Capital One-specific insight that will resonate with the interviewer.

**Technology freedom** â€” small teams owning one service using best tools. Correctly identified.

**The Capital One banking examples** â€” login, balance, payments, fraud, loans as separate services shows you can apply the concept to the actual company you're interviewing with.

**Real service examples** â€” Auth, Payment, Notification, Analytics, Fraud. All realistic and correctly described.

**Kubernetes and REST/gRPC mention** â€” shows infrastructure awareness beyond just the concept.

**"Banks love it for 24/7 uptime"** â€” correctly connected microservices to the financial services requirement of continuous availability.

---

## What to Tighten Up ğŸ”§

**Missing the Bezos API mandate.** This origin story is famous in the industry and signals deep knowledge:

*"The microservices movement at Amazon started with Jeff Bezos's famous 2002 API mandate â€” every team must expose functionality through service interfaces, teams communicate only through those interfaces, and anyone who doesn't comply gets fired. That memo accidentally created the blueprint for AWS and established the pattern that every major tech company eventually adopted."*

**Missing the database-per-service principle.** This is one of the most important and most misunderstood microservices principles:

*"The most critical â€” and most violated â€” microservices principle is database-per-service. Each service owns its own database and no other service touches it directly. If the Account Service needs customer data it calls the Customer Service API â€” it never queries the customer database directly. This is what makes services truly independent. Violate this and you recreate all the coupling problems of the monolith, just with network calls instead of function calls."*

**Missing the communication patterns distinction.** Synchronous vs asynchronous is a key interview topic:

*"Services communicate in two ways. Synchronous â€” REST or gRPC â€” where Service A calls Service B and waits for a response. Use this when the user is waiting for an answer. Asynchronous â€” Kafka events â€” where Service A publishes an event and moves on without waiting. Use this for data pipelines and background processing. At Capital One the transaction service publishes to Kafka and never waits â€” fraud detection, notifications, and analytics all consume independently."*

**Missing the specific monolith deployment problem.** Being concrete about why deployment is painful signals production experience:

*"In a monolith with 500 engineers, deploying a one-line change to the fraud detection algorithm requires freezing all 500 engineers' work, running a full regression test suite that takes hours, coordinating a deployment window, and risking that a bug in any of the 500 engineers' code breaks production. Microservices reduce that to â€” the fraud team deploys their service in 10 minutes with their own tests, their own pipeline, affecting nobody else."*

---

# Model Answer

---

*"Microservices are an architectural approach where a large application is decomposed into small, independently deployable services, each responsible for a single business capability, communicating through well-defined interfaces.*

**Why They Were Created â€” The Origin Story:**

*The microservices movement has a specific origin point. In 2002 Jeff Bezos sent a memo to all Amazon engineers â€” every team must expose their data and functionality through service interfaces, teams must communicate only through those interfaces, and anyone who doesn't do this will be fired. Bezos wasn't thinking about microservices as an architecture pattern â€” he was trying to solve Amazon's engineering coordination problem. That memo accidentally created the blueprint for AWS and established the pattern that Netflix, Uber, and eventually every major technology company adopted.*

*The trigger was scale. Amazon, Netflix, and Capital One all started with monolithic applications â€” one giant codebase, one deployment, one database, one team responsible for everything. As these companies grew to hundreds of engineers and millions of users, the monolith became the bottleneck.*

**The Specific Problems Microservices Solve:**

*Deployment coordination â€” in a monolith with 500 engineers, deploying a one-line change to the fraud algorithm requires freezing all 500 engineers' work, running a full regression suite taking hours, and coordinating a deployment window that risks everyone's code going to production simultaneously. In microservices, the fraud team deploys their service in 10 minutes with their own tests and their own pipeline, affecting nobody else.*

*Cascade failures â€” in a monolith, a memory leak in the notification service crashes the entire application. Customers cannot check balances, process payments, or log in â€” because notification and payment live in the same process. In microservices, the notification service crashes alone. Customers see a delay in SMS alerts but can still make payments, check balances, and log in.*

*All-or-nothing scaling â€” in a monolith, payday transaction spikes force you to scale the entire application â€” paying for extra fraud detection capacity, extra reporting capacity, extra loan servicing capacity â€” none of which are under stress. In microservices, you scale only the transaction processing service, paying only for the capacity you actually need.*

*Technology lock-in â€” a monolith has one language, one runtime, one framework. The data science team's Python fraud model cannot run in a Java monolith. In microservices each service chooses the best technology for its job â€” transaction processing in Java for performance, fraud ML in Python for data science tooling, reporting in Scala for Spark integration.*

**The Database-Per-Service Principle â€” The Most Important Rule:**

*The most critical and most violated microservices principle is database-per-service. Each service owns its own database and no other service touches it directly. The Account Service owns the accounts database. The Transaction Service owns the transactions database. If the Fraud Service needs account information, it calls the Account Service API â€” it never queries the accounts database directly.*

*This principle is what makes services truly independent. Violate it and you recreate all the coupling of the monolith â€” just with network calls instead of function calls. Two services sharing a database cannot be deployed independently because a schema change in one service breaks the other.*

**Capital One Microservices Architecture:**

*In a banking context the decomposition follows business capabilities â€” each capability becomes a service:*

*The Transaction Service receives incoming transactions, validates them, persists them, and publishes transaction events to Kafka. The Fraud Detection Service subscribes to those events, runs ML models, and publishes fraud alerts. The Account Service manages balances and account state. The Notification Service subscribes to fraud alerts and account events and sends SMS, email, and push notifications. The Customer Profile Service manages customer data and preferences. The Reporting Service aggregates for regulatory compliance.*

*Each service is owned by one team, deployed independently, and communicates only through APIs or Kafka events. During payday, only the Transaction Service scales. When the fraud model is retrained, only the Fraud Detection Service is deployed. If the Notification Service has an outage, customers experience delayed alerts but transactions, balances, and fraud detection continue unaffected.*

**How Services Communicate:**

*Two patterns cover all service communication. Synchronous â€” REST or gRPC â€” where a service calls another and waits for a response. Use when the user is waiting for an answer â€” a mobile app asking for current balance needs a synchronous response. Asynchronous â€” Kafka events â€” where a service publishes an event and moves on. Use for data pipelines and background processing â€” the transaction service publishes to Kafka and never waits, fraud detection and analytics consume independently at their own pace.*

*The general rule at Capital One â€” prefer asynchronous event-driven communication for data flows and use synchronous REST only where an immediate user-facing response is required."*

---

# Architecture Diagrams

```
MONOLITH VS MICROSERVICES
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  MONOLITH                          MICROSERVICES                     â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€                          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                     â•‘
â•‘                                                                       â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â•‘
â•‘  â”‚                         â”‚       â”‚Transactionâ”‚  â”‚  Fraud   â”‚       â•‘
â•‘  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚       â”‚ Service  â”‚  â”‚Detection â”‚       â•‘
â•‘  â”‚  â”‚Fraud  â”‚  â”‚Account â”‚  â”‚       â”‚  own DB  â”‚  â”‚  own DB  â”‚       â•‘
â•‘  â”‚  â”‚Engine â”‚  â”‚Manager â”‚  â”‚       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜       â•‘
â•‘  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚            â”‚  Kafka  â”‚                 â•‘
â•‘  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚            â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                 â•‘
â•‘  â”‚  â”‚Notif  â”‚  â”‚Report  â”‚  â”‚                 â”‚                       â•‘
â•‘  â”‚  â”‚Serviceâ”‚  â”‚Engine  â”‚  â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â•‘
â•‘  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚       â–¼         â–¼         â–¼            â•‘
â•‘  â”‚                         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”       â•‘
â•‘  â”‚      ONE DATABASE        â”‚  â”‚Account â”‚ â”‚Notif â”‚ â”‚Report  â”‚       â•‘
â•‘  â”‚                         â”‚  â”‚Service â”‚ â”‚Svc   â”‚ â”‚Service â”‚       â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚own DB  â”‚ â”‚own DBâ”‚ â”‚own DB  â”‚       â•‘
â•‘                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â•‘
â•‘  Problems:                                                           â•‘
â•‘  âŒ One crash = all down        Benefits:                            â•‘
â•‘  âŒ Deploy all or nothing       âœ… One crash = isolated              â•‘
â•‘  âŒ Scale all or nothing        âœ… Deploy independently              â•‘
â•‘  âŒ One language only           âœ… Scale individually                â•‘
â•‘  âŒ Team coordination hell      âœ… Technology freedom                â•‘
â•‘                                 âœ… Team autonomy                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


CAPITAL ONE MICROSERVICES ARCHITECTURE
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  EXTERNAL CLIENTS                                                     â•‘
â•‘  Mobile App â”‚ Web Portal â”‚ ATM Network â”‚ Partner APIs                â•‘
â•‘       â”‚            â”‚            â”‚            â”‚                        â•‘
â•‘       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â•‘
â•‘                            â”‚                                          â•‘
â•‘                            â–¼                                          â•‘
â•‘                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â•‘
â•‘                   â”‚   API GATEWAY   â”‚                                â•‘
â•‘                   â”‚  Auth â”‚ Route   â”‚                                â•‘
â•‘                   â”‚  Rate â”‚ TLS     â”‚                                â•‘
â•‘                   â”‚  Limitâ”‚ Logging â”‚                                â•‘
â•‘                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â•‘
â•‘                            â”‚ routes to correct service               â•‘
â•‘          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â•‘
â•‘          â–¼                 â–¼                      â–¼                  â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â•‘
â•‘  â”‚  ACCOUNT     â”‚  â”‚ TRANSACTION  â”‚    â”‚  CUSTOMER    â”‚             â•‘
â•‘  â”‚  SERVICE     â”‚  â”‚  SERVICE     â”‚    â”‚  PROFILE     â”‚             â•‘
â•‘  â”‚              â”‚  â”‚              â”‚    â”‚  SERVICE     â”‚             â•‘
â•‘  â”‚  Scala/Java  â”‚  â”‚  Java        â”‚    â”‚  Python      â”‚             â•‘
â•‘  â”‚  PostgreSQL  â”‚  â”‚  Cassandra   â”‚    â”‚  MongoDB     â”‚             â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â•‘
â•‘                           â”‚                                          â•‘
â•‘                           â”‚ publishes events                         â•‘
â•‘                           â–¼                                          â•‘
â•‘              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â•‘
â•‘              â”‚      KAFKA             â”‚                              â•‘
â•‘              â”‚  topic: transactions   â”‚                              â•‘
â•‘              â”‚  topic: fraud-alerts   â”‚                              â•‘
â•‘              â”‚  topic: account-events â”‚                              â•‘
â•‘              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â•‘
â•‘                         â”‚ consumed by                                â•‘
â•‘          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â•‘
â•‘          â–¼              â–¼                  â–¼                         â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â•‘
â•‘  â”‚    FRAUD     â”‚ â”‚NOTIFICATION  â”‚ â”‚  REPORTING   â”‚                â•‘
â•‘  â”‚  DETECTION   â”‚ â”‚  SERVICE     â”‚ â”‚  SERVICE     â”‚                â•‘
â•‘  â”‚              â”‚ â”‚              â”‚ â”‚              â”‚                â•‘
â•‘  â”‚  Python/ML   â”‚ â”‚  Node.js     â”‚ â”‚  Scala/Spark â”‚                â•‘
â•‘  â”‚  Cassandra   â”‚ â”‚  No DB       â”‚ â”‚  Snowflake   â”‚                â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â•‘
â•‘                                                                       â•‘
â•‘  Each service: own codebase âœ…  own DB âœ…  own team âœ…               â•‘
â•‘  Each service: own deployment pipeline âœ…  own scaling âœ…            â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


DATABASE PER SERVICE â€” THE GOLDEN RULE
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  WRONG â€” SHARED DATABASE                                             â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                            â•‘
â•‘  Transaction Service â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â•‘
â•‘  Fraud Service â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â–º ONE DATABASE âŒ     â•‘
â•‘  Account Service â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â•‘
â•‘                                                                       â•‘
â•‘  Problems:                                                            â•‘
â•‘  Schema change in one service breaks all others âŒ                   â•‘
â•‘  Services cannot be deployed independently âŒ                        â•‘
â•‘  One DB = single point of failure âŒ                                 â•‘
â•‘  This is just a monolith with extra network calls âŒ                 â•‘
â•‘                                                                       â•‘
â•‘  RIGHT â€” DATABASE PER SERVICE                                        â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                        â•‘
â•‘  Transaction Service â”€â”€â–º Cassandra (write-heavy events)             â•‘
â•‘  Fraud Service â”€â”€â”€â”€â”€â”€â”€â”€â–º Cassandra (feature store)                  â•‘
â•‘  Account Service â”€â”€â”€â”€â”€â”€â–º PostgreSQL (ACID balance)                  â•‘
â•‘  Customer Service â”€â”€â”€â”€â”€â–º MongoDB (flexible profiles)                â•‘
â•‘  Reporting Service â”€â”€â”€â”€â–º Snowflake (analytics)                      â•‘
â•‘                                                                       â•‘
â•‘  Need data from another service?                                     â•‘
â•‘  Call its API âœ…    Never touch its database âœ…                      â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


SYNC VS ASYNC COMMUNICATION
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  SYNCHRONOUS â€” REST/gRPC                                             â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                            â•‘
â•‘  Mobile App â”€â”€â–º API Gateway â”€â”€â–º Account Service â”€â”€â–º Response        â•‘
â•‘                                                                       â•‘
â•‘  User waiting for balance â†’ MUST be synchronous                     â•‘
â•‘  Service A calls Service B and WAITS                                 â•‘
â•‘  Simple but creates tight coupling                                   â•‘
â•‘  If Service B is slow â†’ Service A is slow                           â•‘
â•‘                                                                       â•‘
â•‘  ASYNCHRONOUS â€” KAFKA EVENTS                                         â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                        â•‘
â•‘  Transaction Service â”€â”€â–º Kafka â”€â”€â–º Fraud Detection                  â•‘
â•‘                              â””â”€â”€â”€â–º Notification Service             â•‘
â•‘                              â””â”€â”€â”€â–º Analytics Pipeline               â•‘
â•‘                              â””â”€â”€â”€â–º Audit Service                    â•‘
â•‘                                                                       â•‘
â•‘  Transaction service publishes and MOVES ON immediately              â•‘
â•‘  All consumers process independently at their own pace               â•‘
â•‘  Adding new consumer requires zero changes to producer               â•‘
â•‘  Perfect for Capital One's data pipeline architecture                â•‘
â•‘                                                                       â•‘
â•‘  RULE: Sync for user-facing responses                                â•‘
â•‘        Async for data flows and background processing                â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


PAYDAY SCALING â€” THE MICROSERVICES ADVANTAGE
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  NORMAL DAY                        PAYDAY (10x transaction volume)   â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â•‘
â•‘                                                                       â•‘
â•‘  Transaction Svc  â–ˆâ–ˆ               Transaction Svc  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       â•‘
â•‘  Fraud Svc        â–ˆâ–ˆ               Fraud Svc        â–ˆâ–ˆâ–ˆâ–ˆ             â•‘
â•‘  Account Svc      â–ˆâ–ˆ               Account Svc      â–ˆâ–ˆâ–ˆâ–ˆ             â•‘
â•‘  Notification Svc â–ˆ                Notification Svc â–ˆâ–ˆâ–ˆ              â•‘
â•‘  Reporting Svc    â–ˆ                Reporting Svc    â–ˆ                â•‘
â•‘                                                                       â•‘
â•‘  MONOLITH: Scale everything 10x âŒ  Cost: 10x ğŸ’¸                    â•‘
â•‘                                                                       â•‘
â•‘  MICROSERVICES: Scale only what's stressed âœ…                        â•‘
â•‘  Transaction Svc scales 10x                                          â•‘
â•‘  Everything else stays the same                                      â•‘
â•‘  Cost: fraction of monolith scaling ğŸ’°                               â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```
---
---
---

## Question 2 â€” Stepping Up

**"Explain the Circuit Breaker pattern and the Saga pattern. Why are both essential in a distributed microservices architecture like Capital One's, and how would you implement each in a real banking pipeline?"**

# Feedback First

---

## What You Nailed âœ…

**The fuse box analogy** â€” perfect. Immediately relatable, technically accurate, and memorable. Interviewers remember good analogies.

**Three circuit breaker states implied** â€” closed (normal), open (failing fast), half-open (testing recovery). You described all three without using the technical terms. Adding the terms would elevate it.

**Fallback behavior** â€” "approving small payments for now" is a Capital One-specific fallback that shows business thinking. This is exactly what senior engineers think about â€” not just failing gracefully but defining what graceful means in a business context.

**The trip booking analogy for Saga** â€” excellent. Flight + hotel + payment with compensating cancellations maps perfectly to banking transactions.

**Compensating transactions** â€” correctly identified as the core Saga mechanism. Most candidates say "rollback" without knowing the correct term.

**Choreography vs Orchestration** â€” both correctly defined. "No boss vs manager service" is a clean distinction.

**The wire transfer Saga steps** â€” debit sender, credit receiver, write log, compensate on failure. Correct sequence with correct compensation logic.

**"Either everything works or nothing does"** â€” correctly captured the Saga guarantee without using distributed transaction terminology.

**Kafka as the Saga coordination mechanism** â€” correctly identified. Shows you connect patterns to the technology stack.

---

## What to Tighten Up ğŸ”§

**Missing the three circuit breaker states by name.** The technical terms matter in an interview:

*"The circuit breaker has three states. Closed â€” normal operation, all calls go through, failures are counted. Open â€” failure threshold exceeded, calls are immediately rejected with a fallback response, no waiting for timeouts. Half-Open â€” after the cooldown period, a limited number of test requests are allowed through. If they succeed the circuit closes. If they fail the circuit opens again."*

**Missing the cascade failure explanation.** This is why circuit breakers exist and needs one more sentence:

*"Without a circuit breaker, one slow service causes a cascade failure. Service A calls Service B which is slow. Service A's thread pool fills up waiting. Service A starts rejecting requests. Service C calls Service A and also starts failing. The entire system collapses from one slow service â€” called a cascade failure or cascading degradation. The circuit breaker stops this by failing fast instead of waiting."*

**Missing Choreography vs Orchestration tradeoffs.** You defined both but didn't say when to use which:

*"Choreography â€” no central coordinator, services react to events â€” is simpler to implement and has no single point of failure. But debugging is harder â€” to understand what happened you have to trace events across multiple services. Orchestration â€” a central saga orchestrator manages the flow â€” is easier to understand and debug but the orchestrator becomes a potential bottleneck and single point of failure. For Capital One's wire transfer I'd use orchestration because the business logic is complex, the compensation logic is critical, and having one place to audit the entire transaction flow is a compliance requirement."*

**Missing idempotency.** Critical for Saga reliability:

*"Every step in a Saga must be idempotent â€” if the same step is executed twice due to a retry, the result must be the same as executing it once. For a credit operation, idempotency means checking if this transaction ID has already been credited before applying it. Without idempotency, network retries in a Saga can double-credit or double-debit accounts â€” catastrophic in banking."*

**Missing specific library names.** Shows real-world knowledge:

*"In practice I'd implement circuit breakers using Resilience4j in Java/Scala services â€” it's the modern replacement for Netflix Hystrix which is now deprecated. For Saga orchestration in a Kafka-based architecture I'd use either a dedicated orchestrator service or AWS Step Functions for complex multi-step workflows."*

---

# Model Answer

---

*"Both patterns solve fundamental problems that emerge specifically in distributed systems â€” problems that don't exist in monoliths but become critical at Capital One's scale.*

## Circuit Breaker Pattern

**The Problem â€” Cascade Failures:**

*In a distributed system where dozens of services call each other, one slow or failing service can bring down the entire system through a cascade failure. Service A calls the Fraud Detection Service which is slow â€” perhaps its ML model is loading a new version. Service A's threads pile up waiting for responses that never come quickly. Service A's thread pool exhausts. Service A starts rejecting all requests. The Transaction Service calls Service A and also starts failing. The Account Service calls the Transaction Service and fails. Within minutes, one slow fraud service has taken down the entire banking application.*

*The Circuit Breaker pattern is named after the electrical circuit breaker in your fuse box â€” when current exceeds safe limits, the breaker trips and cuts power before damage spreads.*

**Three States:**

*Closed â€” normal operation. All calls pass through to the downstream service. Failures are counted against a threshold. This is the default healthy state.*

*Open â€” failure threshold exceeded. The circuit trips open. All calls are immediately rejected with a fallback response â€” no waiting for timeouts, no thread exhaustion. The downstream service gets time to recover without being bombarded with requests it can't handle.*

*Half-Open â€” after a configured cooldown period, the circuit allows a small number of test requests through. If they succeed the circuit closes and normal operation resumes. If they fail the circuit opens again for another cooldown period.*

**Capital One Implementation:**

*For the fraud detection service I'd implement it like this using Resilience4j:*

```scala
import io.github.resilience4j.circuitbreaker.{CircuitBreaker, CircuitBreakerConfig}
import java.time.Duration

// Configure the circuit breaker
val config = CircuitBreakerConfig.custom()
  .failureRateThreshold(50)          // open after 50% failure rate
  .waitDurationInOpenState(Duration.ofSeconds(30))  // wait 30s before half-open
  .permittedNumberOfCallsInHalfOpenState(5)  // test with 5 calls
  .slidingWindowSize(10)             // evaluate last 10 calls
  .build()

val circuitBreaker = CircuitBreaker.of("fraudDetection", config)

// Wrap the fraud service call
def checkFraud(transaction: Transaction): FraudResult = {
  val decoratedCall = CircuitBreaker
    .decorateSupplier(circuitBreaker, () => fraudService.check(transaction))

  Try(decoratedCall.get()) match {
    case Success(result) => result
    case Failure(_)      => fallbackFraudCheck(transaction)
  }
}

// Fallback â€” rule-based check when ML service is down
def fallbackFraudCheck(transaction: Transaction): FraudResult = {
  transaction match {
    case t if t.amount > 50000 && t.country != "US" =>
      FraudResult("REVIEW", 0.75, "Circuit open â€” flagged by rule fallback")
    case t if t.amount < 1000 =>
      FraudResult("APPROVE", 0.10, "Circuit open â€” approved by rule fallback")
    case _ =>
      FraudResult("REVIEW", 0.50, "Circuit open â€” queued for manual review")
  }
}
```

*The fallback is critical. When the circuit is open, transactions don't simply fail â€” they fall back to a rule-based system. Small transactions auto-approve. Large foreign transactions go to manual review. Customers are never told the fraud system is down â€” they experience a slight change in behavior that's invisible to them.*

---

## Saga Pattern

**The Problem â€” Distributed Transactions:**

*A wire transfer at Capital One touches multiple services â€” debit the sender's account, credit the receiver's account, write the audit log, send confirmation notifications, update the transaction ledger. In a monolith, a database transaction wraps all of these steps â€” either all succeed or all roll back automatically.*

*In microservices each service has its own database. A traditional ACID transaction spanning multiple databases is impossible â€” you cannot hold a database lock across a network call. The Saga pattern solves this with a sequence of local transactions where each step publishes an event or message that triggers the next step, and every step has a defined compensating transaction that undoes its work if something goes wrong downstream.*

**Two Implementation Approaches:**

*Choreography â€” services are autonomous. Each service listens for events, does its work, and publishes its own completion event. No central coordinator. Simple to implement and no single point of failure. But complex to debug â€” understanding what happened requires tracing events across multiple services.*

*Orchestration â€” a central Saga orchestrator manages the entire flow. It sends commands to each service, waits for responses, and issues compensating commands if anything fails. Easier to understand, easier to audit, easier to debug. The orchestrator becomes a potential bottleneck but for banking workflows the auditability benefit outweighs the complexity.*

**Capital One Wire Transfer â€” Orchestrated Saga:**

```scala
// Saga orchestrator for wire transfer
object WireTransferSaga {

  sealed trait SagaStep
  case object DebitSender     extends SagaStep
  case object CreditReceiver  extends SagaStep
  case object WriteAuditLog   extends SagaStep
  case object SendNotification extends SagaStep

  case class TransferContext(
    transferId:   String,
    senderId:     String,
    receiverId:   String,
    amount:       BigDecimal,
    completedSteps: List[SagaStep] = List.empty
  )

  def execute(ctx: TransferContext): Either[String, String] = {

    // Step 1 â€” Debit sender
    accountService.debit(ctx.senderId, ctx.amount, ctx.transferId) match {
      case Left(err) =>
        Left(s"Transfer failed at debit: $err")  // nothing to compensate yet

      case Right(_) =>

        // Step 2 â€” Credit receiver
        accountService.credit(ctx.receiverId, ctx.amount, ctx.transferId) match {
          case Left(err) =>
            // COMPENSATE â€” reverse the debit
            accountService.reverseDebit(ctx.senderId, ctx.amount, ctx.transferId)
            Left(s"Transfer failed at credit, debit reversed: $err")

          case Right(_) =>

            // Step 3 â€” Write audit log
            auditService.log(ctx.transferId, ctx.senderId,
                            ctx.receiverId, ctx.amount) match {
              case Left(err) =>
                // COMPENSATE â€” reverse both debit and credit
                accountService.reverseCredit(ctx.receiverId, ctx.amount, ctx.transferId)
                accountService.reverseDebit(ctx.senderId, ctx.amount, ctx.transferId)
                Left(s"Transfer failed at audit, all steps reversed: $err")

              case Right(_) =>
                // Step 4 â€” Notification (non-critical â€” don't compensate if fails)
                notificationService.sendConfirmation(ctx.senderId, ctx.receiverId,
                                                    ctx.amount)
                Right(s"Transfer ${ctx.transferId} completed successfully")
            }
        }
    }
  }
}
```

**Idempotency â€” The Critical Safety Requirement:**

*Every Saga step must be idempotent â€” if the same step executes twice due to a retry, the result must be identical to executing it once. For a credit operation:*

```scala
def credit(accountId: String, amount: BigDecimal, transferId: String): Either[String, Unit] = {
  // Check if this transfer has already been credited
  if (transactionRepo.exists(transferId, "credit", accountId)) {
    Right(())  // already done â€” idempotent success
  } else {
    // Apply credit and record with transferId as idempotency key
    transactionRepo.applyCredit(accountId, amount, transferId)
  }
}
```

*Without idempotency, a network timeout that causes a retry could credit the receiver twice â€” a serious financial error. The transferId acts as an idempotency key â€” if we've seen this transfer ID before for this operation, we return success without applying it again.*

**Kafka-Based Choreography Alternative:**

```
ORCHESTRATION                    CHOREOGRAPHY
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                                 
Saga Orchestrator                 TransactionService
    â”‚                                 â”‚
    â”‚ "debit sender"                  â”‚ publishes: txn.initiated
    â–¼                                 â”‚
AccountService                        â–¼
    â”‚                            AccountService
    â”‚ "debit done"               listens: txn.initiated
    â–¼                            publishes: sender.debited
Saga Orchestrator                     â”‚
    â”‚                                 â–¼
    â”‚ "credit receiver"          AccountService
    â–¼                            listens: sender.debited
AccountService                   publishes: receiver.credited
    â”‚                                 â”‚
    â”‚ "credit done"                   â–¼
    â–¼                            AuditService
Saga Orchestrator                listens: receiver.credited
    â”‚                            publishes: audit.written
    â”‚ "write audit"                   â”‚
    â–¼                                 â–¼
AuditService                    NotificationService
                                 listens: audit.written
                                 sends confirmation
```

*For Capital One's wire transfer I'd choose orchestration â€” the business logic is complex, compensation logic is critical, and having one place to audit the entire transaction flow satisfies compliance requirements. For simpler event flows like transaction enrichment I'd use choreography â€” simpler, more resilient, no coordinator bottleneck."*

---

# Diagrams

```
CIRCUIT BREAKER â€” THREE STATES
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â•‘
â•‘                         â”‚ CLOSED  â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â•‘
â•‘                         â”‚(Normal) â”‚                        â”‚         â•‘
â•‘                         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                        â”‚         â•‘
â•‘                              â”‚                             â”‚         â•‘
â•‘                              â”‚ Calls pass through          â”‚         â•‘
â•‘                              â”‚ Failures counted            â”‚         â•‘
â•‘                              â”‚                             â”‚         â•‘
â•‘                              â”‚ Failure rate > 50%          â”‚         â•‘
â•‘                              â–¼                             â”‚         â•‘
â•‘                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚         â•‘
â•‘                         â”‚  OPEN   â”‚                        â”‚         â•‘
â•‘                         â”‚(Failing â”‚                        â”‚         â•‘
â•‘                         â”‚  Fast)  â”‚                        â”‚         â•‘
â•‘                         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                        â”‚         â•‘
â•‘                              â”‚                             â”‚         â•‘
â•‘                              â”‚ All calls rejected          â”‚         â•‘
â•‘                              â”‚ Fallback returned           â”‚         â•‘
â•‘                              â”‚ immediately                 â”‚         â•‘
â•‘                              â”‚                             â”‚         â•‘
â•‘                              â”‚ After 30 seconds            â”‚         â•‘
â•‘                              â–¼                             â”‚         â•‘
â•‘                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚         â•‘
â•‘                         â”‚  HALF   â”‚                        â”‚         â•‘
â•‘                         â”‚  OPEN   â”‚                        â”‚         â•‘
â•‘                         â”‚(Testing)â”‚                        â”‚         â•‘
â•‘                         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                        â”‚         â•‘
â•‘                              â”‚                             â”‚         â•‘
â•‘               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚         â•‘
â•‘               â”‚ 5 test requests              â”‚             â”‚         â•‘
â•‘               â–¼                              â–¼             â”‚         â•‘
â•‘         Tests fail                     Tests pass          â”‚         â•‘
â•‘         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”‚         â•‘
â•‘         Back to OPEN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â•‘
â•‘                                                                       â•‘
â•‘  Capital One Fallback Behavior When Circuit OPEN:                    â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                   â•‘
â•‘  Amount < $1,000    â†’ Auto-approve (low risk)                        â•‘
â•‘  Amount > $50,000   â†’ Queue for manual review                        â•‘
â•‘  Foreign country    â†’ Flag for review                                â•‘
â•‘  All others         â†’ Rule-based risk score                          â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


SAGA PATTERN â€” WIRE TRANSFER AT CAPITAL ONE
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  HAPPY PATH â€” ALL STEPS SUCCEED                                      â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â•‘
â•‘                                                                       â•‘
â•‘  START: Transfer $5,000 from Alice to Bob                            â•‘
â•‘    â”‚                                                                  â•‘
â•‘    â–¼                                                                  â•‘
â•‘  Step 1: Debit Alice -$5,000 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ âœ…         â•‘
â•‘    â”‚                                                                  â•‘
â•‘    â–¼                                                                  â•‘
â•‘  Step 2: Credit Bob +$5,000 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ âœ…         â•‘
â•‘    â”‚                                                                  â•‘
â•‘    â–¼                                                                  â•‘
â•‘  Step 3: Write audit log â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ âœ…         â•‘
â•‘    â”‚                                                                  â•‘
â•‘    â–¼                                                                  â•‘
â•‘  Step 4: Send notification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ âœ…         â•‘
â•‘    â”‚                                                                  â•‘
â•‘    â–¼                                                                  â•‘
â•‘  COMPLETE âœ… Alice -$5,000  Bob +$5,000                              â•‘
â•‘                                                                       â•‘
â•‘  FAILURE PATH â€” STEP 2 FAILS                                         â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                        â•‘
â•‘                                                                       â•‘
â•‘  Step 1: Debit Alice -$5,000 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ âœ…         â•‘
â•‘    â”‚                                                                  â•‘
â•‘    â–¼                                                                  â•‘
â•‘  Step 2: Credit Bob +$5,000 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ âŒ FAILS   â•‘
â•‘    â”‚                                                                  â•‘
â•‘    â”‚ COMPENSATION TRIGGERED                                           â•‘
â•‘    â–¼                                                                  â•‘
â•‘  Compensate Step 1: Reverse debit Alice +$5,000 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ âœ…        â•‘
â•‘    â”‚                                                                  â•‘
â•‘    â–¼                                                                  â•‘
â•‘  ROLLED BACK âœ… Alice $0 change  Bob $0 change                       â•‘
â•‘  No money lost âœ…  No money created âœ…                               â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


IDEMPOTENCY â€” WHY IT MATTERS
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  WITHOUT IDEMPOTENCY â€” DANGEROUS                                     â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                    â•‘
â•‘                                                                       â•‘
â•‘  Saga sends: "Credit Bob $5,000" (transfer_id: T001)                â•‘
â•‘  Network timeout â€” did it succeed? Unknown.                          â•‘
â•‘  Saga retries: "Credit Bob $5,000" (transfer_id: T001)              â•‘
â•‘  Bob gets credited TWICE â†’ Bob has +$10,000 âŒ                      â•‘
â•‘                                                                       â•‘
â•‘  WITH IDEMPOTENCY â€” SAFE                                             â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â•‘
â•‘                                                                       â•‘
â•‘  Saga sends: "Credit Bob $5,000" (transfer_id: T001)                â•‘
â•‘  Network timeout â€” did it succeed? Unknown.                          â•‘
â•‘  Saga retries: "Credit Bob $5,000" (transfer_id: T001)              â•‘
â•‘  Account Service checks: "Have I seen T001 for credit to Bob?"      â•‘
â•‘  YES â†’ return success without applying again                        â•‘
â•‘  Bob gets credited ONCE â†’ Bob has +$5,000 âœ…                        â•‘
â•‘                                                                       â•‘
â•‘  Transfer ID = Idempotency Key                                       â•‘
â•‘  Every operation checks before applying âœ…                           â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


CHOREOGRAPHY VS ORCHESTRATION
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  CHOREOGRAPHY                      ORCHESTRATION                     â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                     â•‘
â•‘                                                                       â•‘
â•‘  Services react to events          Central orchestrator directs      â•‘
â•‘  No central coordinator            All steps managed centrally       â•‘
â•‘                                                                       â•‘
â•‘  Pros:                             Pros:                             â•‘
â•‘  âœ… No single point of failure     âœ… Easy to understand flow        â•‘
â•‘  âœ… Loose coupling                 âœ… Easy to debug                  â•‘
â•‘  âœ… Simple to add new consumers    âœ… Clear audit trail              â•‘
â•‘                                    âœ… Compliance friendly            â•‘
â•‘  Cons:                                                               â•‘
â•‘  âŒ Hard to trace full flow        Cons:                             â•‘
â•‘  âŒ Distributed logic              âŒ Orchestrator is bottleneck     â•‘
â•‘  âŒ Hard to audit                  âŒ Single point of failure        â•‘
â•‘                                                                       â•‘
â•‘  Use for:                          Use for:                          â•‘
â•‘  Simple event flows                Complex business flows            â•‘
â•‘  Transaction enrichment            Wire transfers                    â•‘
â•‘  Analytics pipelines               Loan applications                 â•‘
â•‘                                    Compliance workflows              â•‘
â•‘                                                                       â•‘
â•‘  Capital One wire transfer â†’ ORCHESTRATION                           â•‘
â•‘  (compliance requires full auditability) âœ…                          â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---
---
---

## Question 3 â€” Lead Engineer Level

**"Design the observability strategy for Capital One's microservices architecture. A customer calls saying their wire transfer failed but shows as pending in the app. Walk me through exactly how you would diagnose what happened using your observability tools â€” step by step."**


# Feedback First

---

## What You Nailed âœ…

**The three pillars of observability** â€” metrics, logs, and tracing. Correctly identified all three and correctly named real tools â€” Grafana, Kibana, Jaeger.

**Trace ID as the investigation anchor** â€” this is the most important observability concept and you led with it correctly. "The detective number that follows the request across every service" is exactly right.

**Starting with the dashboard then drilling down** â€” correct diagnostic flow. Wide first, then narrow. See the anomaly on the dashboard, then use the trace to pinpoint it, then use logs for detail.

**"App wrote pending but never changed to failed"** â€” this is a specific, realistic bug that shows production experience. State management inconsistency between services is one of the most common real-world issues in distributed systems.

**Slow fraud check causing timeout cascade** â€” correctly identified that a slow upstream service can cause downstream timeouts. This connects back to the circuit breaker pattern from Question 2.

**Adding alerts after the fix** â€” closing the loop with prevention shows operational maturity.

**"Follow the clues" framing** â€” this is exactly how you want to present debugging in an interview â€” systematic, evidence-based, not random guessing.

---

## What to Tighten Up ğŸ”§

**Missing the correlation ID / trace ID propagation mechanism.** How does the trace ID actually follow the request?

*"Every request entering the API Gateway is assigned a unique correlation ID â€” a UUID that gets injected into the HTTP header as X-Correlation-ID. Every service that receives this request reads the header, includes it in every log line it writes, passes it to every downstream service it calls, and reports it to the distributed tracing system. This is what makes it possible to find every log line across every service related to one specific request."*

**Missing the four golden signals.** This is the industry-standard observability framework that Capital One would use:

*"The starting point for any investigation is the four golden signals â€” Latency, Traffic, Errors, and Saturation. Latency tells you how long requests are taking. Traffic tells you how many requests are coming in. Errors tells you what percentage are failing. Saturation tells you how close resources are to capacity. These four metrics on a dashboard give you an instant picture of system health and narrow down which service to investigate first."*

**Missing structured logging.** The logs need to be machine-parseable not just human-readable:

*"In production I'd use structured logging â€” every log line is a JSON object with consistent fields including service name, trace ID, span ID, customer ID, transfer ID, timestamp, severity, and message. This makes Kibana queries precise â€” I can find every log line across every service for transfer T001 with a single query rather than text searching through millions of lines."*

**Missing SLOs and SLAs.** A Lead Engineer answer mentions these:

*"Behind the dashboards are Service Level Objectives â€” SLOs. The wire transfer service might have an SLO of 99.9% of transfers completing within 10 seconds. When the error rate or latency breaches the SLO threshold, an alert fires automatically. This means we often know about incidents before customers call."*

**Missing the specific diagnosis steps with actual tool commands.** Being concrete about what you actually type elevates the answer significantly.

---

# Model Answer

---

*"Observability in a microservices architecture is the ability to understand what's happening inside the system from the outside â€” by examining the outputs the system produces. It has three pillars â€” metrics, logs, and distributed traces â€” and they work together like layers of a detective investigation.*

## The Three Pillars

**Metrics â€” The Health Dashboard:**

*Metrics are numerical measurements sampled over time â€” request rate, error rate, latency percentiles, CPU usage, memory, queue depth. Every Capital One service exposes metrics that Prometheus scrapes every 15 seconds. Grafana visualizes them in dashboards.*

*The starting point for any investigation is the four golden signals:*

*Latency â€” how long are requests taking? Are p99 latencies spiking while p50 looks fine? That suggests a small percentage of requests are hitting a slow code path.*

*Traffic â€” how many requests per second? A sudden spike might explain performance degradation. A sudden drop might mean requests are being rejected upstream.*

*Errors â€” what percentage of requests are failing? Is the error rate increasing over time or was it a brief spike?*

*Saturation â€” how close are resources to capacity? Is the wire transfer service's thread pool at 95% utilization? Is Cassandra's CPU maxed?*

**Logs â€” The Event Diary:**

*Every service writes structured logs â€” JSON objects with consistent fields so they're machine-parseable and queryable:*

```json
{
  "timestamp": "2026-02-13T14:23:11.432Z",
  "service": "wire-transfer-service",
  "level": "ERROR",
  "traceId": "abc123def456",
  "spanId": "789xyz",
  "customerId": "C001",
  "transferId": "T001",
  "message": "External bank API timeout after 5000ms",
  "errorCode": "UPSTREAM_TIMEOUT",
  "upstreamService": "external-bank-api",
  "attemptNumber": 3
}
```

*Structured logging means finding every log line for transfer T001 across every service is one Kibana query â€” not text searching through billions of lines.*

**Distributed Traces â€” The Request Journey Map:**

*Every request entering the API Gateway gets a unique correlation ID injected into its HTTP headers as X-Correlation-ID and X-Trace-ID. Every service reads these headers, passes them to every downstream call, includes them in every log line, and reports timing spans to the distributed tracing system â€” AWS X-Ray or Jaeger.*

*The result is a complete visual map of every service a request touched, how long it spent in each, and exactly where it failed.*

---

## Diagnosing the Wire Transfer â€” Step by Step

*Customer calls: "My wire transfer is showing as pending but it should have completed 2 hours ago."*

**Step 1 â€” Get the transfer ID and timestamp:**

*"Can you give me your transfer reference number and approximately when you submitted it?" This gives us the primary key for our entire investigation â€” transfer ID T001, submitted at 14:23 UTC.*

**Step 2 â€” Check the four golden signals dashboard:**

*Open Grafana and look at the wire transfer service dashboard for 14:00-15:00 UTC. I'm looking for anomalies in the four golden signals around 14:23.*

```
Wire Transfer Service â€” 14:00-15:00 UTC
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Latency p99:  [normal 800ms] â”€â”€â”€â”€ [SPIKE 12,000ms at 14:22] â”€â”€â”€â”€ [normal]
Error rate:   [normal 0.1%]  â”€â”€â”€â”€ [SPIKE 34% at 14:22-14:28] â”€â”€ [normal]
Traffic:      [steady 1,200 rps] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ [steady]
Saturation:   [normal 45%]   â”€â”€â”€â”€ [SPIKE 89% at 14:22] â”€â”€â”€â”€â”€â”€â”€â”€ [normal]
```

*Immediate finding â€” there was a significant latency and error spike at exactly 14:22, one minute before the customer's transfer. Something went wrong in the wire transfer service at that time.*

**Step 3 â€” Pull the distributed trace:**

*Go to Jaeger, search by transfer ID T001 or by the time window 14:23 Â± 30 seconds for the wire-transfer-service.*

```
TRACE: T001 â€” Total duration: 12,847ms (SLOW â€” normal is ~800ms)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Service                  Duration    Status
â”€â”€â”€â”€â”€â”€â”€                  â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€
API Gateway              12ms        âœ… OK
Auth Service             45ms        âœ… OK
Transaction Service      23ms        âœ… OK
Account Service          67ms        âœ… OK
Fraud Detection Service  11,203ms    âš ï¸ SLOW â† ANOMALY
Wire Transfer Service    890ms       âœ… OK  (but timed out waiting)
External Bank API        [NO SPAN]   âŒ NEVER CALLED
Notification Service     [NO SPAN]   âŒ NEVER REACHED
```

*The trace immediately shows the problem â€” Fraud Detection Service took 11.2 seconds when it normally takes under 200ms. Wire Transfer Service was waiting for fraud approval and timed out at 10 seconds before fraud completed. The transfer was never sent to the external bank. The status was left as "pending" because the timeout handler had a bug â€” it should have set status to "failed" but didn't.*

**Step 4 â€” Drill into fraud service logs:**

*Kibana query: traceId:"abc123def456" AND service:"fraud-detection-service"*

```json
{"timestamp": "14:23:11.001", "message": "Starting ML model inference for T001"}
{"timestamp": "14:23:11.045", "message": "Loaded customer feature vector C001"}
{"timestamp": "14:23:11.046", "message": "Calling fraud ML model v2.3.1"}
{"timestamp": "14:23:22.189", "message": "ML model inference completed â€” 11,143ms",
 "warning": "Model loading from cold cache â€” first call after deployment"}
{"timestamp": "14:23:22.190", "message": "Risk score: 0.08 â€” APPROVE"}
```

*Root cause found â€” the fraud detection service had just been deployed with a new ML model version. The first request after deployment triggered a cold model load â€” the model weights had to load from S3 into memory, taking 11 seconds. All subsequent requests were fast. This transfer was the unlucky first one.*

**Step 5 â€” Check wire transfer service timeout handler:**

*Kibana query: transferId:"T001" AND service:"wire-transfer-service"*

```json
{"timestamp": "14:23:21.200", "level": "ERROR",
 "message": "Fraud service timeout after 10000ms for transfer T001"}
{"timestamp": "14:23:21.201", "level": "ERROR",
 "message": "Transfer T001 marked as PENDING â€” timeout handler invoked"}
```

*Second bug found â€” the timeout handler set status to PENDING instead of FAILED. This is why the customer saw pending instead of a clear failure message.*

**Step 6 â€” Resolution and Prevention:**

*Immediate fix â€” manually update transfer T001 status to FAILED, trigger refund of any held funds, notify customer with clear explanation.*

*Root cause fix 1 â€” implement model warm-up in the fraud service deployment pipeline. After deploying a new model version, send synthetic requests to pre-warm the model cache before routing live traffic.*

*Root cause fix 2 â€” fix the timeout handler to set status to FAILED with a clear customer-facing message.*

*Root cause fix 3 â€” the circuit breaker for fraud detection should have a fallback that continues processing with rule-based scoring during the cold start window.*

*Prevention â€” add SLO alerts:*

```yaml
# Alert: Fraud service p99 latency > 1 second
alert: FraudServiceLatencyHigh
condition: fraud_service_p99_latency_ms > 1000
duration: 1 minute
severity: warning
action: page on-call engineer

# Alert: Wire transfer pending > 30 seconds without resolution  
alert: WireTransferStuck
condition: wire_transfer_pending_duration_seconds > 30
severity: critical
action: auto-retry + page on-call
```

*With these alerts we would have been paged within 60 seconds of the incident â€” before the customer called us."*

---

# Diagrams

```
THE THREE PILLARS OF OBSERVABILITY
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘           METRICS              LOGS              TRACES              â•‘
â•‘           â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€              â•‘
â•‘                                                                       â•‘
â•‘        What happened?      Why did it          Where did it          â•‘
â•‘        at system level     happen?             happen?               â•‘
â•‘                                                                       â•‘
â•‘        Numbers over        Structured          Request journey       â•‘
â•‘        time                events              across services       â•‘
â•‘                                                                       â•‘
â•‘        Prometheus          Elasticsearch       Jaeger /              â•‘
â•‘        + Grafana           + Kibana            AWS X-Ray             â•‘
â•‘                                                                       â•‘
â•‘        "Error rate         "Transfer T001      "Fraud service        â•‘
â•‘         spiked to 34%      failed with         took 11s â€”            â•‘
â•‘         at 14:22"          TIMEOUT error"      here's why"           â•‘
â•‘                                                                       â•‘
â•‘        WHEN to look        WHAT happened       WHERE it happened     â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


CORRELATION ID â€” HOW IT PROPAGATES
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  Customer submits wire transfer                                       â•‘
â•‘         â”‚                                                             â•‘
â•‘         â–¼                                                             â•‘
â•‘  API Gateway assigns: traceId = "abc123def456"                       â•‘
â•‘         â”‚                                                             â•‘
â•‘         â”‚ HTTP Header: X-Trace-ID: abc123def456                      â•‘
â•‘         â–¼                                                             â•‘
â•‘  Auth Service                                                         â•‘
â•‘  â””â”€ Logs: {traceId: "abc123def456", message: "auth ok"}              â•‘
â•‘  â””â”€ Passes header downstream                                          â•‘
â•‘         â”‚                                                             â•‘
â•‘         â–¼                                                             â•‘
â•‘  Fraud Detection Service                                              â•‘
â•‘  â””â”€ Logs: {traceId: "abc123def456", message: "checking fraud"}       â•‘
â•‘  â””â”€ Reports span to Jaeger with same traceId                         â•‘
â•‘         â”‚                                                             â•‘
â•‘         â–¼                                                             â•‘
â•‘  Wire Transfer Service                                                â•‘
â•‘  â””â”€ Logs: {traceId: "abc123def456", message: "sending wire"}         â•‘
â•‘                                                                       â•‘
â•‘  Result: ONE Kibana query finds ALL logs for this transfer           â•‘
â•‘  Result: ONE Jaeger trace shows COMPLETE journey                     â•‘
â•‘  Result: Investigation takes minutes not hours âœ…                    â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


THE FOUR GOLDEN SIGNALS
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  LATENCY          TRAFFIC           ERRORS          SATURATION       â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â•‘
â•‘                                                                       â•‘
â•‘  How long?        How many?         How often       How full?        â•‘
â•‘                                     failing?                         â•‘
â•‘                                                                       â•‘
â•‘  p50: 200ms       1,200 rps         0.1% normal     CPU: 45%         â•‘
â•‘  p95: 400ms                         34% incident    Memory: 60%      â•‘
â•‘  p99: 800ms                                         Threads: 89% âš ï¸  â•‘
â•‘                                                                       â•‘
â•‘  SPIKE at         STEADY            SPIKE at        SPIKE at         â•‘
â•‘  14:22 âŒ         (not the          14:22 âŒ        14:22 âŒ         â•‘
â•‘  â†’ latency        cause)            â†’ errors        â†’ resource       â•‘
â•‘    problem                            happening       exhaustion      â•‘
â•‘                                                                       â•‘
â•‘  Together they immediately point to wire transfer service            â•‘
â•‘  at 14:22 as the problem area âœ…                                     â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


COMPLETE INVESTIGATION FLOW
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  Customer calls â†’ "Transfer pending 2 hours"                        â•‘
â•‘         â”‚                                                             â•‘
â•‘         â–¼                                                             â•‘
â•‘  GET: Transfer ID + Timestamp                                        â•‘
â•‘         â”‚                                                             â•‘
â•‘         â–¼                                                             â•‘
â•‘  GRAFANA: Check 4 golden signals â”€â”€â–º Spike at 14:22 identified      â•‘
â•‘         â”‚                                                             â•‘
â•‘         â–¼                                                             â•‘
â•‘  JAEGER: Pull trace for T001 â”€â”€â”€â”€â”€â”€â–º Fraud service 11s â€” FOUND âŒ   â•‘
â•‘         â”‚                                                             â•‘
â•‘         â–¼                                                             â•‘
â•‘  KIBANA: Fraud service logs â”€â”€â”€â”€â”€â”€â”€â–º Cold model load â€” ROOT CAUSE   â•‘
â•‘         â”‚                                                             â•‘
â•‘         â–¼                                                             â•‘
â•‘  KIBANA: Wire transfer logs â”€â”€â”€â”€â”€â”€â”€â–º Timeout handler bug â€” BUG #2   â•‘
â•‘         â”‚                                                             â•‘
â•‘         â–¼                                                             â•‘
â•‘  FIX:                                                                â•‘
â•‘  1. Update T001 status â†’ FAILED                                      â•‘
â•‘  2. Refund held funds                                                â•‘
â•‘  3. Notify customer                                                  â•‘
â•‘  4. Add model warm-up to deployment pipeline                         â•‘
â•‘  5. Fix timeout handler                                              â•‘
â•‘  6. Add SLO alerts                                                   â•‘
â•‘         â”‚                                                             â•‘
â•‘         â–¼                                                             â•‘
â•‘  Total investigation time: ~8 minutes âœ…                             â•‘
â•‘  Without observability: hours of guessing âŒ                        â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


SLO ALERT THRESHOLDS
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  SERVICE              SLO TARGET    WARNING       CRITICAL           â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€       â”€â”€â”€â”€â”€â”€â”€â”€           â•‘
â•‘  Wire Transfer        p99 < 3s      p99 > 1s      p99 > 3s          â•‘
â•‘  Latency              99.9% ok      for 1 min     for 30s           â•‘
â•‘                                                                       â•‘
â•‘  Wire Transfer        99.95%        Error rate    Error rate        â•‘
â•‘  Availability         available     > 0.5%        > 1%              â•‘
â•‘                                                                       â•‘
â•‘  Fraud Detection      p99 < 500ms   p99 > 200ms   p99 > 500ms       â•‘
â•‘  Latency                                                             â•‘
â•‘                                                                       â•‘
â•‘  Transfer Status      No transfer   Pending       Pending           â•‘
â•‘  Resolution           pending       > 30 seconds  > 2 minutes       â•‘
â•‘                       > 5 minutes   â†’ investigate â†’ auto-retry      â•‘
â•‘                                                                       â•‘
â•‘  Alert fires BEFORE customer calls âœ…                                â•‘
â•‘  On-call engineer paged within 60 seconds of incident âœ…             â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```
