# Mock Interview â€” Kubernetes

Same rules â€” answer in the room, model answer and diagrams after each response.

---

## Question 1 â€” Warm Up

**"Can you explain what Kubernetes is, why it was created, and what core problem it solves in a microservices environment like Capital One's?"**

# Feedback First

---

## What You Nailed âœ…

**"Smart manager for your app's boxes"** â€” perfect opening analogy. Immediately clear, memorable, and accurate.

**Google 2014 origin from internal Borg system** â€” correctly placed. Mentioning the internal predecessor shows depth beyond surface knowledge.

**"Open-sourced so everyone could use a free powerful way to run apps like pros"** â€” correctly captured the strategic reason Google open-sourced it.

**Four core capabilities** â€” scaling, healing, updates, balancing. All four correct and all relevant to Capital One.

**Payday rush scaling example** â€” Capital One-specific and exactly right. Shows you can apply concepts to the business context.

**Zero downtime updates** â€” correctly identified rolling updates as a core Kubernetes value proposition.

**"24/7, handles crazy traffic spikes safely"** â€” correctly connected Kubernetes to banking's continuous availability requirement.

**"Teams can add features quick without breaking stuff"** â€” correctly identified independent deployability as a Kubernetes benefit in a microservices context.

---

## What to Tighten Up ğŸ”§

**Missing the desired state / reconciliation loop concept.** This is the single most important Kubernetes concept and the one that makes it fundamentally different from previous tools:

*"The core innovation of Kubernetes is declarative desired state. You don't tell Kubernetes what to do â€” you tell it what you want. I want 10 fraud detection pods always running. Kubernetes continuously watches the cluster and reconciles reality to match your declaration. If a pod crashes, Kubernetes doesn't wait for you to notice â€” it sees the count dropped to 9 and immediately starts a replacement. This reconciliation loop runs constantly, making Kubernetes self-healing by design."*

**Missing the Pod concept specifically.** The interview will almost certainly ask about pods:

*"The smallest deployable unit in Kubernetes is not a container â€” it's a Pod. A pod wraps one or more containers that always run together on the same node and share networking. In practice most pods contain one container but the pod abstraction gives Kubernetes a consistent unit to schedule, monitor, and manage regardless of what's inside."*

**Missing the control plane vs worker node distinction.** Lead Engineer level requires knowing the architecture:

*"Kubernetes has two types of nodes. Control plane nodes run Kubernetes itself â€” the API server that receives all commands, the scheduler that decides which node each pod runs on, etcd which stores all cluster state, and the controller manager that runs the reconciliation loops. Worker nodes are where your actual application containers run â€” managed by an agent called kubelet that receives instructions from the control plane."*

**Missing AWS EKS specifically.** Capital One context requires this:

*"At Capital One, Kubernetes runs on AWS EKS â€” Elastic Kubernetes Service. EKS manages the entire control plane â€” Capital One never patches API servers or manages etcd. The engineering team only manages worker nodes and application deployments. This is the standard enterprise pattern â€” use a managed Kubernetes service and focus engineering effort on applications not infrastructure."*

---

# Model Answer

---

*"Kubernetes is a container orchestration platform â€” a system that manages the deployment, scaling, healing, and networking of containerized applications across a cluster of machines.*

**The Origin â€” Why Google Built It:**

*Google has been running containerized workloads internally since 2003 using an internal system called Borg. By 2014 the container revolution had gone mainstream â€” Docker made containers accessible to every engineer, and microservices architectures meant companies suddenly had hundreds of containerized services to manage. The manual approach â€” SSHing into servers, manually restarting crashed containers, manually scaling instances â€” was impossible at scale.*

*Google open-sourced Kubernetes in 2014, donating the lessons from a decade of running Borg at planetary scale to the industry. The name comes from the Greek word for helmsman â€” the person who steers a ship. The metaphor is intentional. Kubernetes steers your containers to where they need to be and keeps them there.*

**The Core Innovation â€” Desired State and Reconciliation:**

*The fundamental concept that makes Kubernetes different from everything before it is declarative desired state. You don't tell Kubernetes what to do. You tell it what you want â€” and it figures out how to make that happen and keep it that way.*

*You declare: I want 10 instances of the fraud detection service always running, each with 2GB of memory and 500 millicores of CPU, using image version 2.4.0. Kubernetes reads that declaration and continuously reconciles reality to match it. If a pod crashes, Kubernetes doesn't wait for an alert or a human â€” the controller manager sees the count dropped to 9 and immediately starts a replacement. If a node fails, Kubernetes reschedules all its pods onto healthy nodes automatically. This reconciliation loop runs every few seconds, making the system self-healing by design.*

**The Core Problems it Solves for Capital One:**

*In a microservices architecture Capital One has dozens of independent services â€” transaction processing, fraud detection, account management, notifications, customer profiles, reporting. Without Kubernetes, managing these services across hundreds of machines requires constant manual intervention.*

*Self-healing â€” the fraud detection service crashes at 3am. Without Kubernetes, a pager goes off and an engineer logs in to restart it. With Kubernetes, the pod is restarted automatically in seconds. The on-call engineer sleeps through it.*

*Autoscaling â€” transaction volume on payday is 10 times normal. Without Kubernetes, someone has to manually provision extra capacity in advance and guess how much. With Kubernetes, Horizontal Pod Autoscaling watches transaction queue depth or CPU utilization and automatically adds pods when volume spikes, removes them when it drops. Capital One pays only for the capacity it actually uses.*

*Zero-downtime deployments â€” a new fraud ML model needs to go to production. Without Kubernetes, the service has to be taken offline to deploy. With Kubernetes rolling updates, new pods with the updated model start alongside old pods. Traffic gradually shifts to new pods as they pass health checks. Old pods terminate only after new ones are confirmed healthy. Customers experience no interruption.*

*Service discovery â€” the transaction service needs to call the fraud detection service. Pods are ephemeral â€” they crash and restart with new IP addresses constantly. Kubernetes Services provide a stable DNS name that always resolves to healthy fraud detection pods regardless of which specific instances are running.*

**The Architecture:**

*Kubernetes has two tiers. The control plane makes cluster-wide decisions â€” the API server receives all commands, the scheduler places pods on nodes, etcd stores all cluster state as the single source of truth, and the controller manager runs reconciliation loops. Worker nodes run application workloads â€” each has a kubelet agent that receives instructions from the control plane and manages pods on that node.*

*At Capital One this runs on AWS EKS â€” Elastic Kubernetes Service. EKS manages the entire control plane. Capital One never patches API servers, never manages etcd, never worries about control plane availability. Engineering effort focuses entirely on application deployments and data pipelines.*

**The Data Engineering Connection:**

*As a Lead Data Engineer at Capital One, Kubernetes is the platform your data pipelines run on. Spark jobs run natively on Kubernetes â€” the Spark driver is a pod, executor pods are dynamically provisioned for each job and terminated when complete. Kafka Connect connectors run as pods. Flink streaming jobs run as pods. Airflow workers run as pods. The entire data platform from ingestion through transformation to storage is orchestrated by Kubernetes.*

*Understanding Kubernetes means you can deploy your Spark jobs with the right resource requests, configure autoscaling for variable workloads, debug failed jobs by reading pod logs, and design pipelines that are resilient to node failures â€” all without depending on a separate infrastructure team for every change."*

---

# Diagrams

```
THE CORE KUBERNETES CONCEPT â€” DESIRED STATE
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  YOU DECLARE:                                                         â•‘
â•‘  "I want 10 fraud detection pods always running"                     â•‘
â•‘                                           â”‚                          â•‘
â•‘                                           â–¼                          â•‘
â•‘                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â•‘
â•‘                              â”‚  KUBERNETES         â”‚                â•‘
â•‘                              â”‚  RECONCILIATION     â”‚                â•‘
â•‘                              â”‚  LOOP               â”‚                â•‘
â•‘                              â”‚  (runs every few    â”‚                â•‘
â•‘                              â”‚   seconds forever)  â”‚                â•‘
â•‘                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â•‘
â•‘                                         â”‚                            â•‘
â•‘                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â•‘
â•‘                    â–¼                                         â–¼       â•‘
â•‘             Desired: 10                              Actual: 9       â•‘
â•‘             Actual:  9                               (one crashed)   â•‘
â•‘                    â”‚                                                  â•‘
â•‘                    â–¼                                                  â•‘
â•‘             GAP DETECTED                                             â•‘
â•‘             Start 1 new pod                                          â•‘
â•‘                    â”‚                                                  â•‘
â•‘                    â–¼                                                  â•‘
â•‘             Desired: 10  âœ…                                          â•‘
â•‘             Actual:  10  âœ…                                          â•‘
â•‘                                                                       â•‘
â•‘  This happens automatically, continuously, 24/7                      â•‘
â•‘  No human intervention required âœ…                                   â•‘
â•‘  No pager going off at 3am âœ…                                        â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


KUBERNETES ARCHITECTURE
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â•‘
â•‘  â”‚  CONTROL PLANE (managed by AWS EKS at Capital One)          â”‚    â•‘
â•‘  â”‚                                                             â”‚    â•‘
â•‘  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â•‘
â•‘  â”‚  â”‚  API Server â”‚  â”‚ Scheduler â”‚  â”‚  Controller Manager  â”‚ â”‚    â•‘
â•‘  â”‚  â”‚             â”‚  â”‚           â”‚  â”‚                      â”‚ â”‚    â•‘
â•‘  â”‚  â”‚ Front door  â”‚  â”‚ Decides   â”‚  â”‚ Runs reconciliation  â”‚ â”‚    â•‘
â•‘  â”‚  â”‚ All kubectl â”‚  â”‚ which nodeâ”‚  â”‚ loops â€” ensures      â”‚ â”‚    â•‘
â•‘  â”‚  â”‚ commands go â”‚  â”‚ each pod  â”‚  â”‚ desired state always â”‚ â”‚    â•‘
â•‘  â”‚  â”‚ through hereâ”‚  â”‚ runs on   â”‚  â”‚ matches actual state â”‚ â”‚    â•‘
â•‘  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â•‘
â•‘  â”‚                                                             â”‚    â•‘
â•‘  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â•‘
â•‘  â”‚  â”‚  etcd â€” Single Source of Truth                       â”‚  â”‚    â•‘
â•‘  â”‚  â”‚  Stores all cluster state: pods, nodes,              â”‚  â”‚    â•‘
â•‘  â”‚  â”‚  deployments, secrets, configmaps                    â”‚  â”‚    â•‘
â•‘  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â•‘
â•‘                              â”‚ instructs                             â•‘
â•‘         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â•‘
â•‘         â–¼                    â–¼                    â–¼                  â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â•‘
â•‘  â”‚ WORKER NODE â”‚    â”‚ WORKER NODE â”‚    â”‚ WORKER NODE â”‚             â•‘
â•‘  â”‚             â”‚    â”‚             â”‚    â”‚             â”‚             â•‘
â•‘  â”‚ kubelet     â”‚    â”‚ kubelet     â”‚    â”‚ kubelet     â”‚             â•‘
â•‘  â”‚ (agent)     â”‚    â”‚ (agent)     â”‚    â”‚ (agent)     â”‚             â•‘
â•‘  â”‚             â”‚    â”‚             â”‚    â”‚             â”‚             â•‘
â•‘  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚             â•‘
â•‘  â”‚ â”‚  Pod    â”‚ â”‚    â”‚ â”‚  Pod    â”‚ â”‚    â”‚ â”‚  Pod    â”‚ â”‚             â•‘
â•‘  â”‚ â”‚ fraud-1 â”‚ â”‚    â”‚ â”‚ fraud-2 â”‚ â”‚    â”‚ â”‚ fraud-3 â”‚ â”‚             â•‘
â•‘  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚             â•‘
â•‘  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚             â•‘
â•‘  â”‚ â”‚  Pod    â”‚ â”‚    â”‚ â”‚  Pod    â”‚ â”‚    â”‚ â”‚  Pod    â”‚ â”‚             â•‘
â•‘  â”‚ â”‚ txn-1   â”‚ â”‚    â”‚ â”‚ txn-2   â”‚ â”‚    â”‚ â”‚ txn-3   â”‚ â”‚             â•‘
â•‘  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚             â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


ZERO DOWNTIME ROLLING UPDATE
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  BEFORE UPDATE: 4 pods running v2.3                                  â•‘
â•‘  [fraud:v2.3] [fraud:v2.3] [fraud:v2.3] [fraud:v2.3]               â•‘
â•‘  All serving traffic âœ…                                               â•‘
â•‘                                                                       â•‘
â•‘  STEP 1: Start 1 new pod v2.4, wait for health check                â•‘
â•‘  [fraud:v2.3] [fraud:v2.3] [fraud:v2.3] [fraud:v2.3] [fraud:v2.4] â•‘
â•‘                                                          (starting)  â•‘
â•‘                                                                       â•‘
â•‘  STEP 2: v2.4 healthy â€” terminate 1 old pod                         â•‘
â•‘  [fraud:v2.3] [fraud:v2.3] [fraud:v2.3] [fraud:v2.4]               â•‘
â•‘                                                                       â•‘
â•‘  STEP 3: Repeat until all updated                                    â•‘
â•‘  [fraud:v2.3] [fraud:v2.3] [fraud:v2.4] [fraud:v2.4]               â•‘
â•‘  [fraud:v2.3] [fraud:v2.4] [fraud:v2.4] [fraud:v2.4]               â•‘
â•‘  [fraud:v2.4] [fraud:v2.4] [fraud:v2.4] [fraud:v2.4]                 â•‘
â•‘                                                                      â•‘
â•‘  Traffic served throughout entire process âœ…                         â•‘
â•‘  Zero downtime âœ…                                                    â•‘
â•‘  Instant rollback if v2.4 fails health checks âœ…                     â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


PAYDAY AUTOSCALING â€” CAPITAL ONE
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  NORMAL DAY 9am          PAYDAY 12pm            NIGHT 11pm           â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â•‘
â•‘                                                                       â•‘
â•‘  Transaction pods: 5     Transaction pods: 50   Transaction pods: 3  â•‘
â•‘  Fraud pods:       3     Fraud pods:       30   Fraud pods:       2  â•‘
â•‘                                                                       â•‘
â•‘  HPA watches:            HPA sees:              HPA sees:            â•‘
â•‘  CPU < 40% âœ…            CPU > 80% â—           CPU < 20% âœ…         â•‘
â•‘  Queue depth normal      Queue depth spiking    Queue depth empty    â•‘
â•‘                          SCALE UP â†’             SCALE DOWN â†’         â•‘
â•‘                                                                       â•‘
â•‘  Kubernetes adds pods    Kubernetes removes     Only pay for what    â•‘
â•‘  automatically âœ…        pods automatically âœ…  you use âœ…           â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


CAPITAL ONE DATA PIPELINE ON KUBERNETES
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  ALL OF THIS RUNS AS KUBERNETES PODS                                 â•‘
â•‘                                                                       â•‘
â•‘  Kafka Connect pods  â†’  ingest from source systems                  â•‘
â•‘         â”‚                                                             â•‘
â•‘         â–¼                                                             â•‘
â•‘  Flink pods         â†’  real-time stream processing                  â•‘
â•‘         â”‚                                                             â•‘
â•‘         â–¼                                                             â•‘
â•‘  Spark Driver pod   â†’  orchestrates batch jobs                      â•‘
â•‘  Spark Executor pods â†’ dynamically provisioned per job              â•‘
â•‘         â”‚                                                             â•‘
â•‘         â–¼                                                             â•‘
â•‘  Airflow Worker pods â†’ schedule and monitor pipelines               â•‘
â•‘         â”‚                                                             â•‘
â•‘         â–¼                                                             â•‘
â•‘  Stored in Cassandra, Snowflake, S3                                  â•‘
â•‘                                                                       â•‘
â•‘  Every component:                                                     â•‘
â•‘  Auto-healed if crashed âœ…                                           â•‘
â•‘  Auto-scaled on load âœ…                                              â•‘
â•‘  Zero-downtime updates âœ…                                            â•‘
â•‘  Resource governed âœ…                                                â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## Question 2 â€” Stepping Up

**"Explain Pods, Deployments, and Services in Kubernetes. How do they relate to each other, and how would you use all three to deploy Capital One's fraud detection service?"**


# Feedback First

---

## What You Nailed âœ…

**"Tiniest worker bee"** â€” perfect pod analogy. Small, does one job, part of a larger system.

**"Pods are fragile â€” if one dies it's gone forever"** â€” this is the critical pod insight that most beginners miss. Pods are ephemeral by design. Correctly identified.

**"Deployment is the boss that manages a group of identical pods"** â€” clean and accurate. The management layer above pods.

**"Service is the phone number for your pods"** â€” excellent analogy. Stable address that doesn't change even when pods behind it do. Memorable and precise.

**The relationship chain** â€” Deployment creates pods â†’ Service points to them â†’ everything stays running and reachable. This is the exact mental model interviewers want to hear.

**10 pods for fraud detection scale** â€” correctly sized for Capital One's millions of checks per second requirement.

**Gradual rollout for ML model updates** â€” correctly applied rolling updates to the specific Capital One use case of retraining fraud models.

**"Payment service calls fraud-detector"** â€” correctly showed service-to-service communication using the Kubernetes Service DNS name. This is exactly how microservices find each other in Kubernetes.

**Load balancing across 10 pods** â€” correctly identified as a Service responsibility.

---

## What to Tighten Up ğŸ”§

**Missing the label selector mechanism.** This is how Deployments and Services actually find their pods â€” and it's a frequent interview question:

*"The connection between a Deployment, its pods, and the Service that routes to them is managed entirely through labels and selectors. The Deployment adds a label to every pod it creates â€” app: fraud-detection. The Service has a selector that says route traffic to any pod with label app: fraud-detection. This is how Kubernetes loosely couples these three objects â€” they don't reference each other by name, they find each other through matching labels."*

**Missing the three Service types.** ClusterIP, NodePort, and LoadBalancer are fundamental:

*"Kubernetes has three Service types. ClusterIP â€” internal only, accessible only within the cluster. This is what the transaction service uses to call the fraud detection service â€” internal service-to-service communication. NodePort â€” exposes the service on a port on every node, primarily for debugging. LoadBalancer â€” provisions an external AWS load balancer and gives you a public IP. This is what the API Gateway uses for external traffic. For Capital One's internal fraud service I'd use ClusterIP â€” it should never be directly exposed externally."*

**Missing ReplicaSet.** The layer between Deployment and Pod:

*"There's actually a layer between Deployment and Pod called a ReplicaSet. When you create a Deployment, it creates a ReplicaSet which in turn creates and manages the pods. The Deployment manages ReplicaSets â€” when you do a rolling update, Kubernetes creates a new ReplicaSet for the new version and scales it up while scaling down the old one. You rarely interact with ReplicaSets directly but knowing they exist shows architectural depth."*

**Missing resource requests and limits on the pod spec.** Critical for production:

*"Every pod in production must declare resource requests and limits. Requests tell the scheduler how much CPU and memory this pod needs â€” used to decide which node to place it on. Limits cap how much it can consume â€” preventing one runaway fraud detection pod from starving other pods on the same node. Without these, pods get placed randomly and can consume unbounded resources."*

---

# Model Answer

---

*"Pods, Deployments, and Services are three distinct Kubernetes objects that work together â€” each solving a different layer of the container management problem.*

## Pod â€” The Atomic Unit

*A pod is the smallest deployable unit in Kubernetes. It wraps one or more containers that always run together on the same node, share a network namespace â€” they communicate via localhost â€” and share storage volumes. In practice the vast majority of pods contain a single container.*

*The critical characteristic of a pod is that it is ephemeral by design. Pods are not meant to be permanent. They crash, they get evicted, nodes fail and their pods are rescheduled. Every time a pod restarts it gets a new IP address. You never rely on a pod's IP address or assume a specific pod will always exist. This ephemerality is intentional â€” it forces you to design stateless services and manage state in dedicated storage systems.*

*A raw pod definition for the fraud detection service:*

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: fraud-detection-pod
  labels:
    app: fraud-detection        # label â€” how Deployment and Service find this pod
    version: v2.4.0
spec:
  containers:
  - name: fraud-detection
    image: 123456789.dkr.ecr.us-east-1.amazonaws.com/fraud-detection:v2.4.0
    ports:
    - containerPort: 8080
    resources:
      requests:                 # minimum guaranteed resources
        memory: "2Gi"
        cpu: "500m"
      limits:                   # maximum allowed resources
        memory: "4Gi"
        cpu: "2000m"
    livenessProbe:              # restart pod if this fails
      httpGet:
        path: /health
        port: 8080
      initialDelaySeconds: 30
      periodSeconds: 10
    readinessProbe:             # remove from load balancing if not ready
      httpGet:
        path: /ready
        port: 8080
      initialDelaySeconds: 15
```

*You would never create a raw pod like this in production â€” if it crashes it stays dead. That's what Deployments are for.*

## Deployment â€” The Manager

*A Deployment tells Kubernetes the desired state for a set of identical pods â€” how many replicas to run, which container image to use, and how to update them. The Deployment controller runs continuously, comparing desired state to actual state and reconciling any differences.*

*The Deployment creates and manages a ReplicaSet â€” an intermediate object that maintains the specified number of pod replicas. When you do a rolling update the Deployment creates a new ReplicaSet for the new version and gradually scales it up while scaling down the old one. You rarely interact with ReplicaSets directly but they're the mechanism behind rolling updates.*

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fraud-detection
  namespace: fraud
spec:
  replicas: 10                  # always run exactly 10 pods
  selector:
    matchLabels:
      app: fraud-detection      # manages pods with this label
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 2               # start up to 2 extra pods during update
      maxUnavailable: 0         # never reduce below 10 during update
  template:                     # pod template â€” all pods created from this
    metadata:
      labels:
        app: fraud-detection    # this label connects pods to the Service
        version: v2.4.0
    spec:
      containers:
      - name: fraud-detection
        image: 123456789.dkr.ecr.us-east-1.amazonaws.com/fraud-detection:v2.4.0
        resources:
          requests:
            memory: "2Gi"
            cpu: "500m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
```

*With maxUnavailable: 0 Kubernetes never drops below 10 healthy pods during an update â€” the fraud detection service never loses capacity even during a model deployment.*

## Service â€” The Stable Address

*A Service provides a stable DNS name and IP address for a set of pods. Since pods are ephemeral and their IPs change constantly, every other service that needs to call fraud detection uses the Service DNS name â€” never a pod IP directly.*

*The connection between a Service and its pods is through label selectors. The Service says route traffic to any pod with label app: fraud-detection. The Deployment stamps that label on every pod it creates. Kubernetes maintains a list of pod IPs behind the Service â€” called Endpoints â€” and updates it automatically as pods come and go.*

```yaml
apiVersion: v1
kind: Service
metadata:
  name: fraud-detection         # DNS name â€” other services call this
  namespace: fraud
spec:
  type: ClusterIP               # internal only â€” never exposed externally
  selector:
    app: fraud-detection        # routes to pods with this label
  ports:
  - port: 80                    # port the service listens on
    targetPort: 8080            # port the pod listens on
```

*With ClusterIP the fraud detection service is reachable at fraud-detection.fraud.svc.cluster.local from any pod in the cluster. The transaction service calls this DNS name â€” Kubernetes handles routing to one of the 10 healthy fraud pods, load balancing automatically.*

*Three Service types for different exposure needs:*

```
ClusterIP    â€” Internal only. Service-to-service within cluster.
               fraud-detection calling account-service.
               
NodePort     â€” Exposes on a port on every node. Mainly for debugging.
               Rarely used in production.
               
LoadBalancer â€” Provisions AWS ALB with public IP. External traffic.
               The API Gateway uses this to receive mobile app traffic.
```

## Connecting All Three â€” Capital One Fraud Detection

*The complete deployment:*

```
kubectl apply -f deployment.yaml   # creates Deployment â†’ ReplicaSet â†’ 10 Pods
kubectl apply -f service.yaml      # creates Service pointing to those Pods

Result:
â”€â”€â”€â”€â”€â”€â”€â”€â”€
10 fraud-detection pods running across worker nodes
Service "fraud-detection" load balancing across all 10
Transaction service calls "fraud-detection.fraud.svc.cluster.local"
If any pod crashes â†’ Deployment replaces it â†’ Service updates endpoints
Deploy new ML model â†’ Deployment rolls out â†’ zero downtime
Traffic spikes â†’ HPA adds pods â†’ Service routes to new pods automatically
```

*"*

---

# Diagrams

```
HOW LABELS CONNECT ALL THREE OBJECTS
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  DEPLOYMENT                                                           â•‘
â•‘  selector:                                                            â•‘
â•‘    matchLabels:                                                       â•‘
â•‘      app: fraud-detection  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â•‘
â•‘                                                           â”‚          â•‘
â•‘  Creates pods with label:                                 â”‚          â•‘
â•‘      app: fraud-detection  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚          â•‘
â•‘                                      â”‚                   â”‚          â•‘
â•‘                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚          â•‘
â•‘                           â–¼          â–¼          â–¼        â”‚          â•‘
â•‘                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚          â•‘
â•‘                      â”‚  Pod   â”‚ â”‚  Pod   â”‚ â”‚  Pod   â”‚   â”‚          â•‘
â•‘                      â”‚ app:   â”‚ â”‚ app:   â”‚ â”‚ app:   â”‚   â”‚          â•‘
â•‘                      â”‚ fraud- â”‚ â”‚ fraud- â”‚ â”‚ fraud- â”‚   â”‚          â•‘
â•‘                      â”‚detect  â”‚ â”‚detect  â”‚ â”‚detect  â”‚   â”‚          â•‘
â•‘                      â”‚192.168 â”‚ â”‚192.169 â”‚ â”‚192.170 â”‚   â”‚          â•‘
â•‘                      â”‚.1.10   â”‚ â”‚.1.11   â”‚ â”‚.1.12   â”‚   â”‚          â•‘
â•‘                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚          â•‘
â•‘                           â–²          â–²          â–²        â”‚          â•‘
â•‘                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚          â•‘
â•‘                                      â”‚                   â”‚          â•‘
â•‘  SERVICE                             â”‚                   â”‚          â•‘
â•‘  selector:                           â”‚                   â”‚          â•‘
â•‘    app: fraud-detection  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚          â•‘
â•‘                          matches pods âœ…                  â”‚          â•‘
â•‘  name: fraud-detection   â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â•‘
â•‘  DNS: fraud-detection.fraud.svc.cluster.local                        â•‘
â•‘                                                                       â•‘
â•‘  LABELS are the glue â€” Deployment and Service                        â•‘
â•‘  find pods through matching labels, not by name âœ…                   â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


POD VS DEPLOYMENT â€” THE CRITICAL DIFFERENCE
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  RAW POD                           DEPLOYMENT                        â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€                          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                        â•‘
â•‘                                                                       â•‘
â•‘  fraud-pod crashes                 fraud-pod-x7k2p crashes           â•‘
â•‘         â”‚                                  â”‚                         â•‘
â•‘         â–¼                                  â–¼                         â•‘
â•‘  Pod is gone âŒ                    Controller detects:               â•‘
â•‘  Gone forever âŒ                   desired=10, actual=9              â•‘
â•‘  Manual restart required âŒ        Creates new pod immediately âœ…    â•‘
â•‘                                    Back to 10 in seconds âœ…          â•‘
â•‘                                                                       â•‘
â•‘  Use raw pods: Never in production                                   â•‘
â•‘  Use Deployments: Always âœ…                                          â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


THREE SERVICE TYPES â€” WHEN TO USE EACH
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  CLUSTERIP (default)                                                 â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                   â•‘
â•‘  Internal only. No external access.                                  â•‘
â•‘                                                                       â•‘
â•‘  Transaction Service â”€â”€â–º fraud-detection:80 â”€â”€â–º 10 fraud pods       â•‘
â•‘                          (internal DNS only)                         â•‘
â•‘                                                                       â•‘
â•‘  Use for: All internal microservice communication âœ…                 â•‘
â•‘  Capital One fraud-detection â†’ ClusterIP âœ…                          â•‘
â•‘                                                                       â•‘
â•‘  LOADBALANCER                                                        â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                        â•‘
â•‘  Provisions AWS ALB. Public IP. External traffic.                    â•‘
â•‘                                                                       â•‘
â•‘  Mobile App â”€â”€â–º AWS ALB (public IP) â”€â”€â–º API Gateway pods            â•‘
â•‘                                                                       â•‘
â•‘  Use for: External-facing services only                              â•‘
â•‘  Capital One API Gateway â†’ LoadBalancer âœ…                           â•‘
â•‘                                                                       â•‘
â•‘  NODEPORT                                                            â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€                                                            â•‘
â•‘  Exposes port on every node. Mainly debugging.                       â•‘
â•‘  Avoid in production âŒ                                              â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


REPLICASET â€” THE HIDDEN MIDDLE LAYER
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  DEPLOYMENT                                                           â•‘
â•‘  (manages ReplicaSets)                                               â•‘
â•‘       â”‚                                                               â•‘
â•‘       â”œâ”€â”€â–º ReplicaSet v2.3  (old version â€” scaling down)            â•‘
â•‘       â”‚    â””â”€â”€ Pod v2.3                                              â•‘
â•‘       â”‚    â””â”€â”€ Pod v2.3  (being terminated)                         â•‘
â•‘       â”‚                                                               â•‘
â•‘       â””â”€â”€â–º ReplicaSet v2.4  (new version â€” scaling up)             â•‘
â•‘            â””â”€â”€ Pod v2.4  âœ…                                          â•‘
â•‘            â””â”€â”€ Pod v2.4  âœ…                                          â•‘
â•‘            â””â”€â”€ Pod v2.4  âœ… (starting)                              â•‘
â•‘                                                                       â•‘
â•‘  During rolling update:                                              â•‘
â•‘  New ReplicaSet scales UP                                            â•‘
â•‘  Old ReplicaSet scales DOWN                                          â•‘
â•‘  Deployment coordinates the transition                               â•‘
â•‘  Service routes to healthy pods from either RS âœ…                   â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


COMPLETE FRAUD DETECTION DEPLOYMENT
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  kubectl apply -f fraud-detection-deployment.yaml                    â•‘
â•‘  kubectl apply -f fraud-detection-service.yaml                       â•‘
â•‘                                                                       â•‘
â•‘  RESULT:                                                             â•‘
â•‘                                                                       â•‘
â•‘  Transaction Service                                                  â•‘
â•‘       â”‚                                                               â•‘
â•‘       â”‚ calls fraud-detection.fraud.svc.cluster.local               â•‘
â•‘       â–¼                                                               â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â•‘
â•‘  â”‚  SERVICE: fraud-detection (ClusterIP)           â”‚                â•‘
â•‘  â”‚  Load balances across healthy pods              â”‚                â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â•‘
â•‘                         â”‚                                            â•‘
â•‘     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â•‘
â•‘     â–¼                   â–¼                   â–¼                        â•‘
â•‘  [Pod:v2.4]         [Pod:v2.4]         [Pod:v2.4]  Ã— 10 total      â•‘
â•‘  healthy âœ…          healthy âœ…          healthy âœ…                  â•‘
â•‘                                                                       â•‘
â•‘  Pod crashes â†’ Deployment replaces â†’ Service updates âœ…              â•‘
â•‘  Model update â†’ Rolling deploy â†’ Zero downtime âœ…                   â•‘
â•‘  Traffic spike â†’ HPA adds pods â†’ Service routes to all âœ…           â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---
---
---

## Question 3 â€” Lead Engineer Level

**"Walk me through how you would deploy a Spark batch job and a Flink streaming job on Kubernetes at Capital One. What are the differences in how Kubernetes manages each, and what would you configure for resource management, autoscaling, and failure handling?"**


# Feedback First

---

## What You Nailed âœ…

**"Batch is fire and forget, streaming is always-on"** â€” the single most important distinction and you led with it cleanly.

**Spark Operator and Flink Operator** â€” correctly identified that both use Kubernetes operators rather than raw deployments. This is production knowledge. Most candidates would miss this entirely.

**Driver/executor model for Spark** â€” correctly described. Driver bosses the job, executors do the work. Accurate.

**JobManager/TaskManager model for Flink** â€” correctly described. The parallel to Spark's driver/executor shows you understand both architectures.

**Checkpoints for Flink** â€” correctly identified as the state management mechanism for streaming jobs. "Save points for restarts" is accurate.

**Dynamic allocation for Spark** â€” correctly identified. Spark adds and removes executors based on workload rather than fixed count.

**Flink Reactive Mode** â€” mentioning this specifically shows real Flink depth. Most candidates only know basic Flink concepts.

**Exactly-once for Flink** â€” correctly connected to the banking requirement of no data loss or duplicates.

**Resource requests and limits per pod** â€” correctly specified driver vs executor with different resource profiles. Shows production experience.

**"Over-provision a bit for spikes"** â€” pragmatic production thinking. Correct for financial services workloads.

---

## What to Tighten Up ğŸ”§

**Missing the Spark on Kubernetes architecture specifics.** The driver pod creates executor pods dynamically â€” this is the key Kubernetes-native Spark behavior:

*"In Kubernetes-native Spark, the driver pod is created first by the Spark operator. The driver then talks directly to the Kubernetes API server to dynamically create executor pods as needed. When the job completes, the driver terminates all executor pods and exits. Kubernetes never had to know about executor count upfront â€” the driver manages its own workers through the Kubernetes API. This is fundamentally different from YARN where a resource manager pre-allocates containers."*

**Missing Flink's savepoint vs checkpoint distinction.** Critical for production:

*"Flink has two state persistence mechanisms. Checkpoints are automatic, periodic snapshots taken every few minutes â€” used for automatic recovery from failures. Savepoints are manually triggered snapshots â€” used for intentional operations like upgrading the Flink application, changing parallelism, or migrating to a new cluster. At Capital One before deploying a new fraud detection streaming job version I'd trigger a savepoint, deploy the new version, and resume from the savepoint â€” zero data loss, no reprocessing."*

**Missing the Job vs Session cluster modes.** Both Spark and Flink have different deployment modes:

*"Flink on Kubernetes has two cluster modes. Session mode â€” a long-running Flink cluster that multiple jobs share. Application mode â€” one dedicated cluster per job, starts when the job starts and terminates when it ends. For Capital One's fraud detection streaming job I'd use Application mode â€” dedicated resources, no noisy neighbor problems, and clean failure isolation. If the fraud job has a bug it doesn't affect other streaming jobs."*

**Missing Persistent Volume Claims for state storage.** Checkpoints need storage:

*"Flink checkpoints need durable storage â€” if the pod restarts the checkpoint data must survive. I'd configure checkpoints to write to S3 via a PersistentVolumeClaim or directly to an S3 path. Without durable checkpoint storage, a pod restart means reprocessing from the beginning of the Kafka topic â€” potentially hours of data."*

**Missing the KEDA mention for Kafka-based autoscaling.** This is the modern approach:

*"Standard HPA scales on CPU and memory â€” useful but not ideal for data pipelines. KEDA â€” Kubernetes Event-Driven Autoscaling â€” scales based on external metrics like Kafka consumer lag. If the fraud detection Flink job's Kafka consumer lag exceeds 100,000 messages, KEDA automatically increases Flink task manager replicas. When lag clears, it scales back down. This is far more responsive than CPU-based scaling for streaming workloads."*

---

# Model Answer

---

*"Spark and Flink represent two fundamentally different execution models â€” batch and streaming â€” and Kubernetes manages them differently because of this. Let me walk through both deployments and then compare how Kubernetes handles each.*

## Deploying Spark Batch on Kubernetes

**Architecture â€” How Kubernetes-Native Spark Works:**

*In Kubernetes-native Spark, the Spark Operator watches for SparkApplication resources. When it sees one, it creates the driver pod. The driver pod connects directly to the Kubernetes API server and dynamically creates executor pods as needed. When the job completes, the driver terminates all executor pods and the driver pod exits with either Completed or Failed status. Kubernetes cleans up all resources.*

*This is fundamentally different from YARN â€” there's no pre-allocated resource pool. Executors are created and destroyed dynamically per job.*

```yaml
# spark-batch-job.yaml
apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: daily-transaction-summary
  namespace: data-engineering
spec:
  type: Scala
  mode: cluster
  image: 123456789.dkr.ecr.us-east-1.amazonaws.com/spark-jobs:v1.2.0
  mainClass: com.capitalone.TransactionSummaryJob
  mainApplicationFile: "local:///app/transaction-summary.jar"
  arguments:
    - "--date=2026-02-13"
    - "--input=s3://capital-one-raw/transactions/"
    - "--output=s3://capital-one-processed/summaries/"

  sparkConf:
    "spark.sql.adaptive.enabled": "true"           # AQE for skew handling
    "spark.dynamicAllocation.enabled": "true"       # scale executors dynamically
    "spark.dynamicAllocation.minExecutors": "2"
    "spark.dynamicAllocation.maxExecutors": "50"
    "spark.dynamicAllocation.initialExecutors": "10"

  driver:
    cores: 2
    memory: "4g"
    serviceAccount: spark-driver
    labels:
      job: daily-transaction-summary

  executor:
    cores: 4
    instances: 10                 # initial count â€” dynamic allocation adjusts
    memory: "8g"
    memoryOverhead: "2g"          # off-heap for shuffle operations

  restartPolicy:
    type: OnFailure
    onFailureRetries: 3
    onFailureRetryInterval: 60    # retry after 60 seconds
```

*Dynamic allocation means Spark starts with 10 executors and scales to 50 if the job has enough parallelism. Overnight batch jobs that process 90 days of transaction history get the executors they need. Smaller daily jobs use fewer, keeping costs down.*

## Deploying Flink Streaming on Kubernetes

**Architecture â€” How Kubernetes-Native Flink Works:**

*The Flink Operator creates a JobManager pod â€” the brain that coordinates the job â€” and a configurable number of TaskManager pods â€” the workers that execute operators. Unlike Spark batch, these pods run indefinitely. The JobManager continuously monitors TaskManagers, takes periodic checkpoints to S3, and restarts failed TaskManagers automatically.*

```yaml
# flink-fraud-detection.yaml
apiVersion: flink.apache.org/v1beta1
kind: FlinkDeployment
metadata:
  name: fraud-detection-stream
  namespace: fraud
spec:
  image: 123456789.dkr.ecr.us-east-1.amazonaws.com/flink-fraud:v3.1.0
  flinkVersion: v1.18
  flinkConfiguration:
    taskmanager.numberOfTaskSlots: "4"

    # Checkpointing â€” state durability
    execution.checkpointing.interval: "60s"        # checkpoint every 60 seconds
    execution.checkpointing.mode: EXACTLY_ONCE
    state.backend: rocksdb                          # incremental checkpoints
    state.checkpoints.dir: s3://capital-one-checkpoints/fraud/
    state.savepoints.dir: s3://capital-one-savepoints/fraud/

    # Exactly-once end-to-end
    execution.checkpointing.externalized-checkpoint-retention: RETAIN_ON_CANCELLATION

  serviceAccount: flink-fraud

  jobManager:
    resource:
      memory: "2048m"
      cpu: 1
    replicas: 1

  taskManager:
    resource:
      memory: "4096m"
      cpu: 2
    replicas: 5                   # KEDA scales this based on Kafka lag

  job:
    jarURI: local:///app/fraud-detection.jar
    entryClass: com.capitalone.FraudDetectionJob
    args:
      - "--kafka-bootstrap=kafka.internal:9092"
      - "--input-topic=transactions"
      - "--output-topic=fraud-alerts"
      - "--parallelism=20"
    upgradeMode: savepoint         # use savepoint for zero-loss upgrades
```

**Savepoint-Based Deployment for Model Updates:**

```bash
# Before deploying new fraud model version:

# 1. Trigger savepoint â€” snapshot current state to S3
kubectl exec -n fraud fraud-detection-jobmanager -- \
  flink savepoint fraud-job-id s3://capital-one-savepoints/fraud/pre-upgrade/

# 2. Deploy new version â€” Flink operator handles graceful shutdown
kubectl apply -f flink-fraud-detection-v3.2.yaml

# 3. New job resumes from savepoint â€” zero data loss
# All in-flight transaction windows preserved âœ…
# Kafka offsets preserved âœ…
# No transactions reprocessed âœ…
```

## KEDA â€” Kafka-Driven Autoscaling

*Standard HPA scales on CPU and memory â€” not ideal for streaming. KEDA scales on Kafka consumer lag â€” exactly what streaming jobs need:*

```yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: fraud-detection-scaler
  namespace: fraud
spec:
  scaleTargetRef:
    name: fraud-detection-stream    # the FlinkDeployment to scale
  minReplicaCount: 3
  maxReplicaCount: 20
  triggers:
  - type: kafka
    metadata:
      bootstrapServers: kafka.internal:9092
      consumerGroup: fraud-detection-flink
      topic: transactions
      lagThreshold: "50000"         # scale up when lag > 50k messages
```

*When transaction volume spikes on payday and Kafka lag exceeds 50,000 messages, KEDA automatically adds TaskManager pods. When lag clears, it scales back down. The fraud detection pipeline never falls behind.*

## Spark vs Flink â€” How Kubernetes Manages Each

```
                    SPARK BATCH              FLINK STREAMING
                    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Kubernetes          Job â€” runs once          Deployment â€” runs forever
resource type       then exits

Pod lifecycle       Driver: minutes          JobManager: months
                    Executors: created       TaskManagers: days-weeks
                    and destroyed per job    (restart on failure)

State management    Stateless â€” read         Stateful â€” RocksDB
                    from S3, write           state backend with
                    to S3                    incremental checkpoints

Failure behavior    Retry whole job          Restart from last
                    or failed stage          checkpoint â€” seconds
                    (3 retries configured)   of data reprocessed

Scaling mechanism   Dynamic allocation       KEDA on Kafka lag
                    (Spark controls          (external metric
                    executor count)          controls replica count)

Upgrade strategy    Replace job              Savepoint â†’ upgrade
                    (batch can restart)      â†’ resume (zero loss)

Resource pattern    Burst â€” high CPU         Steady â€” consistent
                    and memory during        resource consumption
                    job, zero after          24/7
```

## Complete Resource and Failure Configuration

```yaml
# Resource governance â€” both job types
resources:
  requests:           # scheduler uses this for placement
    memory: "4Gi"
    cpu: "2000m"
  limits:             # hard cap â€” prevents resource starvation
    memory: "6Gi"
    cpu: "4000m"

# Liveness probe â€” restart pod if stuck
livenessProbe:
  httpGet:
    path: /health
    port: 8081
  initialDelaySeconds: 60
  periodSeconds: 30
  failureThreshold: 3

# Pod Disruption Budget â€” maintain minimum healthy pods
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: fraud-detection-pdb
spec:
  minAvailable: 3       # always keep at least 3 TaskManagers running
  selector:
    matchLabels:
      app: fraud-detection
```

*The PodDisruptionBudget prevents Kubernetes from evicting too many pods simultaneously during node maintenance â€” critical for the fraud detection streaming job that must process every transaction."*

---

# Diagrams

```
SPARK BATCH ON KUBERNETES â€” LIFECYCLE
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  kubectl apply -f spark-batch-job.yaml                               â•‘
â•‘         â”‚                                                             â•‘
â•‘         â–¼                                                             â•‘
â•‘  Spark Operator sees SparkApplication resource                       â•‘
â•‘         â”‚                                                             â•‘
â•‘         â–¼                                                             â•‘
â•‘  Creates DRIVER POD                                                  â•‘
â•‘  driver.capitalone.fraud/daily-tx-summary                            â•‘
â•‘         â”‚                                                             â•‘
â•‘         â”‚ Driver talks to Kubernetes API                              â•‘
â•‘         â–¼                                                             â•‘
â•‘  Creates EXECUTOR PODS dynamically                                   â•‘
â•‘  executor-1  executor-2  executor-3 ... executor-50                  â•‘
â•‘  (dynamic allocation â€” scales 2 to 50 based on work)                â•‘
â•‘         â”‚                                                             â•‘
â•‘         â”‚ Job runs â€” processes S3 data                               â•‘
â•‘         â–¼                                                             â•‘
â•‘  Job COMPLETES                                                        â•‘
â•‘         â”‚                                                             â•‘
â•‘         â–¼                                                             â•‘
â•‘  Driver terminates all executor pods                                 â•‘
â•‘  Driver pod exits: Completed âœ…                                      â•‘
â•‘  All resources released â€” zero cost until next run âœ…                â•‘
â•‘                                                                       â•‘
â•‘  FAILURE: OnFailureRetries: 3                                        â•‘
â•‘  Retry 1 after 60s â†’ Retry 2 after 60s â†’ Retry 3 â†’ Alert            â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


FLINK STREAMING ON KUBERNETES â€” ALWAYS ON
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  kubectl apply -f flink-fraud-detection.yaml                         â•‘
â•‘         â”‚                                                             â•‘
â•‘         â–¼                                                             â•‘
â•‘  Flink Operator creates:                                             â•‘
â•‘                                                                       â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â•‘
â•‘  â”‚   JOB MANAGER POD   â”‚  â—„â”€â”€ Brain: coordinates job,               â•‘
â•‘  â”‚   Always running    â”‚       manages checkpoints,                  â•‘
â•‘  â”‚   1 replica         â”‚       monitors TaskManagers                 â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â•‘
â•‘             â”‚ manages                                                 â•‘
â•‘    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â•‘
â•‘    â–¼        â–¼        â–¼                                               â•‘
â•‘  [TM-1]  [TM-2]  [TM-3]  ... [TM-20]  â—„â”€â”€ KEDA scales              â•‘
â•‘  Worker  Worker  Worker                    on Kafka lag              â•‘
â•‘    â”‚        â”‚        â”‚                                               â•‘
â•‘    â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                               â•‘
â•‘             â”‚                                                         â•‘
â•‘             â”‚ Checkpoint every 60s to S3                             â•‘
â•‘             â–¼                                                         â•‘
â•‘    s3://capital-one-checkpoints/fraud/                               â•‘
â•‘                                                                       â•‘
â•‘  TaskManager crashes:                                                 â•‘
â•‘  K8s restarts it â†’ resumes from last checkpoint âœ…                   â•‘
â•‘  Seconds of reprocessing, zero data loss âœ…                          â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


KEDA â€” KAFKA LAG BASED AUTOSCALING
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  NORMAL: Kafka lag = 5,000 messages                                  â•‘
â•‘  TaskManagers: 5  â–ˆâ–ˆâ–ˆâ–ˆ                                               â•‘
â•‘  Processing: keeping up âœ…                                           â•‘
â•‘                                                                       â•‘
â•‘  PAYDAY SPIKE: Kafka lag = 200,000 messages                          â•‘
â•‘  KEDA detects: lag > 50,000 threshold                                â•‘
â•‘  KEDA scales: 5 â†’ 10 â†’ 15 â†’ 20 TaskManagers                         â•‘
â•‘  Processing: catching up âœ…                                          â•‘
â•‘                                                                       â•‘
â•‘  AFTER SPIKE: Kafka lag = 1,000 messages                             â•‘
â•‘  KEDA scales down: 20 â†’ 10 â†’ 5 TaskManagers                         â•‘
â•‘  Cost: reduced back to baseline âœ…                                   â•‘
â•‘                                                                       â•‘
â•‘  CPU-based HPA would be WRONG here:                                  â•‘
â•‘  Flink keeps CPU steady regardless of lag âŒ                         â•‘
â•‘  Kafka lag is the RIGHT signal for streaming scale âœ…                â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


SAVEPOINT UPGRADE â€” ZERO DATA LOSS MODEL UPDATE
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  CURRENT: Fraud model v3.1 running, processing transactions          â•‘
â•‘         â”‚                                                             â•‘
â•‘         â–¼                                                             â•‘
â•‘  STEP 1: Trigger savepoint                                           â•‘
â•‘  s3://capital-one-savepoints/fraud/pre-v3.2/                        â•‘
â•‘  Captures: Kafka offsets, window state, aggregations                 â•‘
â•‘         â”‚                                                             â•‘
â•‘         â–¼                                                             â•‘
â•‘  STEP 2: Deploy v3.2 with savepoint restore path                    â•‘
â•‘  kubectl apply -f flink-fraud-v3.2.yaml                             â•‘
â•‘         â”‚                                                             â•‘
â•‘         â–¼                                                             â•‘
â•‘  STEP 3: v3.2 starts from exact savepoint state                     â•‘
â•‘  No transactions reprocessed âœ…                                      â•‘
â•‘  No duplicate fraud alerts âœ…                                        â•‘
â•‘  No missed transactions âœ…                                           â•‘
â•‘  Model updated âœ…                                                    â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---


Ready to move to the next topic?

| # | Topic | Status |
|---|---|---|
| 1 | Scala | âœ… Done |
| 2 | Kafka / Real-time streaming | âœ… Done |
| 3 | NoSQL â€” MongoDB | âœ… Done |
| 4 | NoSQL â€” Cassandra | âœ… Done |
| 5 | Distributed Microservices | âœ… Done |
| 6 | Kubernetes | âœ… Done |
| 7 | EMR | â¬œ Next |
| 8 | PySpark / Spark Refresher | â¬œ |
| 9 | Hive / Hadoop Refresher | â¬œ |
| 10 | Snowflake / Redshift Refresher | â¬œ |
| 11 | Agile Refresher | â¬œ |
| 12 | Docker Refresher | â¬œ |

**EMR next â€” same format?** ğŸ¯