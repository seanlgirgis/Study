# Mock Interview — Data Lakehouse (Delta / Iceberg)

Same rules — answer in the room, model answer and diagrams after each response.

---

## Question 1 — Warming Up (The "Evolution" Check)

**"I see you listed 'Data Lakehouse' on your resume. Is this just a marketing term for a Data Lake? What is the actual technical difference between a Lake and a Lakehouse?"**

# Feedback First

## What You Nailed ✅
*   **Transactions:** Correctly identified that a Lakehouse adds ACID transactions to a Data Lake. S3 alone cannot do `UPDATE WHERE id=5`.
*   **Metadata:** Mentioned that a Lakehouse uses a metadata layer (like `_delta_log` or Iceberg manifest) to track files, whereas a Lake just lists files.
*   **Quality:** Correctly stated that a Lakehouse allows for schema enforcement, preventing "Data Swamps".

## What to Tighten Up 🔧
*   **Performance:** You missed the performance angle.
    *   *Lake:* To count rows, you must list and open every file.
    *   *Lakehouse:* The metadata has the count pre-calculated. `SELECT COUNT(*)` is instant.
*   **Time Travel:** This is a killer feature. "I can query the table as it looked at 9:00 AM yesterday." A standard Lake cannot do this.

# Model Answer

---

"A Data Lakehouse is a **Data Lake with a Brain**.

**Data Lake (The Body):**
It stores massive amounts of data in open formats (Parquet/ORC) on cheap object storage (S3). But it's dumb. It doesn't know what's in the files until you open them. It has no transactions.

**Data Lakehouse (The Brain):**
It adds a **Transactional Metadata Layer** (like Delta Log or Iceberg Manifests) on top of those files.
1.  **ACID Transactions:** I can safely `UPDATE` or `DELETE` rows without race conditions.
2.  **Schema Enforcement:** It rejects bad data before it corrupts the lake.
3.  **Performance:** It uses the metadata to skip huge chunks of data (Data Skipping) during queries, making it almost as fast as a Warehouse."

# Diagrams

```text
DATA LAKE vs LAKEHOUSE
╔═══════════════════════════════════════════════════════════════════════╗
║  DATA LAKE (Dumb Storage)         LAKEHOUSE (Smart Storage)           ║
║  ┌───────────────────┐            ┌───────────────────┐               ║
║  │ S3 Bucket         │            │ S3 Bucket         │               ║
║  │ [file1.parquet]   │            │ [file1.parquet]   │               ║
║  │ [file2.parquet]   │            │ [file2.parquet]   │               ║
║  └───────────────────┘            └─────────▲─────────┘               ║
║                                             │                         ║
║  Query: "SELECT * WHERE ID=5"               │                         ║
║  Action: Open ALL files. Scan.    ┌─────────┴─────────┐               ║
║  Result: Slow.                    │ TRANSACTION LOG   │               ║
║                                   │ (Metadata Layer)  │               ║
║                                   └───────────────────┘               ║
║                                   Query: "SELECT * WHERE ID=5"        ║
║                                   Action: Log says ID=5 is in File 2. ║
║                                           Open ONLY File 2.           ║
║                                   Result: Fast (Data Skipping).       ║
╚═══════════════════════════════════════════════════════════════════════╝
```

---

## Question 2 — Data Engineer Level (The "Log" Check)

**"How does Delta Lake guarantee ACID transactions on S3, which is an eventually consistent object store? Walk me through what happens when two people try to write to the same table at the same time."**

# Feedback First

## What You Nailed ✅
*   **The Log:** Correctly identified the `_delta_log` folder that tracks the state of the table.
*   **Atomic Commits:** Mentioned that a commit is only valid if the log file is successfully written.
*   **Optimistic Concurrency:** Correctly stated that Delta checks for conflicts before committing.

## What to Tighten Up 🔧
*   **Mutual Exclusion:** You must mention how it handles simultaneous writes.
    *   *User A* reads version 10, writes version 11.
    *   *User B* reads version 10, writes version 11.
    *   **The Conflict:** Who wins?
    *   **The Solution:** Delta uses "PutIfAbsent" (or atomic renames). If User A writes `000011.json`, User B's attempt to write `000011.json` will fail. User B must then read the new state (v11) and try again (write v12).

# Model Answer

---

"Delta Lake implements **Optimistic Concurrency Control** using a transaction log.

**The Workflow:**
1.  **Read:** Both users read the current state of the table (Version 10).
2.  **Write:** They both do their work and prepare commit files.
3.  **Commit Attempt:**
    *   User A tries to write the log entry `000011.json` to S3. **Success.** The table is now at Version 11.
    *   User B tries to write `000011.json`. **Failure (File Already Exists).**

**The Resolution:**
User B's job catches the error. It checks: *'Did User A change anything that conflicts with what I just did?'*
*   **No Conflict:** (e.g., A updated partition 1, B updated partition 2). Delta automatically retries and writes `000012.json`.
*   **Conflict:** (e.g., Both updated partition 1). The job throws a `ConcurrentModificationException`."

# Diagrams

```text
OPTIMISTIC CONCURRENCY (The Race Condition)
╔═══════════════════════════════════════════════════════════════════════╗
║  Start: Table Version 10                                              ║
║                                                                       ║
║  User A (Update Part 1)           User B (Update Part 1)              ║
║  │                                │                                   ║
║  │ 1. Read v10                    │ 1. Read v10                       ║
║  │ 2. Process Data                │ 2. Process Data                   ║
║  │ 3. Writes 11.json              │ 3. Writes 11.json                 ║
║  ▼                                ▼                                   ║
║  [S3 Log Folder]                  [S3 Log Folder]                     ║
║  Success! ✅                     Fail! ❌ (File Exists)              ║
║                                   │                                   ║
║                                   │ 4. Check for Conflict             ║
║                                   │ "Did v11 touch Partition 1?"      ║
║                                   │ YES.                              ║
║                                   ▼                                   ║
║                                   THROW EXCEPTION 💥                  ║
╚═══════════════════════════════════════════════════════════════════════╝
```

---

## Question 3 — Lead Level (Optimization - Z-Ordering)

**"We have a massive Delta table (1PB) partitioned by Date. Queries filtering by `Customer_ID` are still slow. Why? How do we fix this without re-partitioning the whole table?"**

# Feedback First

## What You Nailed ✅
*   **Partition Pruning:** Correctly identified that while partitioning by Date helps date queries, it doesn't help `Customer_ID` queries. Spark has to scan *every* file in the date folder.
*   **Secondary Index:** Mentioned that we need something like an index.

## What to Tighten Up 🔧
*   **Z-Ordering / Space Filling Curves:** This is the specific technique Delta uses. It's not a traditional B-Tree index. It physically rearranges the data in the files to co-locate similar IDs.
*   **Data Skipping:** Explain *how* it works. Delta stores Min/Max statistics for each column in the metadata.
    *   If a file has `ID_Min=100` and `ID_Max=200`, and I query `ID=500`, Spark skips the file entirely.
*   **Compaction:** Mention running `OPTIMIZE` to combine small files into larger ones while Z-Ordering.

# Model Answer

---

"Partitioning by Date is good, but for `Customer_ID`, we are suffering from a **Scan Storm**. Inside each Date folder, customer IDs are randomly scattered across thousands of files. Spark has to open every single one.

**The Solution: Z-Ordering (Multi-Dimensional Clustering)**
I would run `OPTIMIZE table_name ZORDER BY (customer_id)`.

**How it works:**
1.  **Co-locality:** It physically sorts and rewrites the Parquet files so that all Customer IDs near '1000' are in File A, and all near '9000' are in File B.
2.  **Data Skipping:** Delta automatically collecting min/max stats (`_min_id`, `_max_id`) for every file in the log.
3.  **The Query:** When I query `WHERE ID=500`, Delta checks the metadata.
    *   File A (Min 100, Max 200)? **Skip.**
    *   File B (Min 400, Max 600)? **Read.**

This drastically reduces I/O without changing the directory structure (Partitioning)."

# Diagrams

```text
Z-ORDERING (Data Skipping)
╔═══════════════════════════════════════════════════════════════════════╗
║  WITHOUT Z-ORDER (Random Sort)                                        ║
║  File 1: [ID 5, ID 90, ID 4]  -> Min:4, Max:90                        ║
║  File 2: [ID 8, ID 12, ID 60] -> Min:8, Max:60                        ║
║  Query: "WHERE ID = 5"                                                ║
║  Result: Must read BOTH files because 5 is between Min/Max of both.   ║
║                                                                       ║
║  WITH Z-ORDER (Sorted)                                                ║
║  File 1: [ID 4, ID 5, ID 8]   -> Min:4, Max:8                         ║
║  File 2: [ID 12, ID 60, ID 90]-> Min:12, Max:90                       ║
║  Query: "WHERE ID = 5"                                                ║
║  Result:                                                              ║
║  - Check File 1? Yes (5 is in [4, 8])                                 ║
║  - Check File 2? NO (5 is NOT in [12, 90]) -> SKIP! ⏩               ║
╚═══════════════════════════════════════════════════════════════════════╝
```
