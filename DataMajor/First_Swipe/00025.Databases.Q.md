# Mock Interview â€” Databases (SQL vs NoSQL vs NewSQL)

Same rules â€” answer in the room, model answer and diagrams after each response.

---

## Question 1 â€” Warming Up (The "CAP" Check)

**"I see you have Postgres, Cassandra, and Snowflake on your resume. Can you explain the CAP Theorem? Which two letters does each database choose, and why can't you have all three?"**

# Feedback First

## What You Nailed âœ…
*   **Definitions:** Correctly defined Consistency (C), Availability (A), and Partition Tolerance (P).
*   **Trade-offs:** Correctly identified that "All 3 is impossible" because networks *will* fail (Partitions happen).
*   **Examples:** Correctly mapped Postgres to CA (Consistent/Available but fails when network breaks) and Cassandra to AP (Always Available but data might be old).

## What to Tighten Up ğŸ”§
*   *The "CA" Myth:* Technically, no distributed system is CA. If the network breaks, you *must* choose between C (Wait for sync) or A (Keep serving old data). RDBMS is actually CP or AP depending on configuration (e.g., synchronous replication vs async).
*   *Eventual Consistency:* This is the key phrase for AP systems. "The data will be correct *eventually*, usually within milliseconds."

# Model Answer

---

"The CAP Theorem states that in a distributed system, when a network failure occurs (Partition), you must choose between **Consistency** (Returns error or waits) and **Availability** (Returns potentially stale data).

**Postgres (RDBMS) â€” Typically CA/CP:**
We prioritize Consistency. If the primary node loses connection to the replica, we stop accepting writes to avoid split-brain scenarios. Financial transactions cannot tolerate inconsistency.

**Cassandra (NoSQL) â€” Strictly AP:**
We prioritize Availability. If a node goes down, the system keeps accepting writes. The data might be inconsistent for a few milliseconds, but it will converge eventually (Eventual Consistency).

**Snowflake (OLAP):**
Itâ€™s a bit different because it decouples compute from storage (S3). It acts like a CP systemâ€”queries see a consistent snapshot of the data at a point in time, even if it means waiting for metadata updates."

# Diagrams

```text
CAP THEOREM TRIANGLE
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                  Consistency (C)                                      â•‘
â•‘                       â–²                                               â•‘
â•‘                      / \                                              â•‘
â•‘                     /   \                                             â•‘
â•‘         (RDBMS)    /     \   (HBase/Mongo)                            â•‘
â•‘        Postgres   /       \                                           â•‘
â•‘                  /_________\                                          â•‘
â•‘    Availability (A)      Partition Tolerance (P)                      â•‘
â•‘                                                                       â•‘
â•‘    AP Systems (Cassandra/DynamoDB):                                   â•‘
â•‘    "I will answer your request even if the network is broken,         â•‘
â•‘     but the data might be 5ms old."                                   â•‘
â•‘                                                                       â•‘
â•‘    CP Systems (HBase/Redis):                                          â•‘
â•‘    "I will refuse your request if I can't guarantee the data is       â•‘
â•‘     perfectly up to date."                                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## Question 2 â€” Data Engineer Level (The "OLTP vs OLAP" Check)

**"Why can't I just use my production Postgres database for analytics? Why do I need to ETL the data into Snowflake? What technically makes Snowflake faster for `SELECT COUNT(*)`?"**

# Feedback First

## What You Nailed âœ…
*   **Row vs Column:** Correctly identified that Postgres is Row-Oriented (writes fast) and Snowflake is Column-Oriented (reads fast).
*   **Locking:** Mentioned that running heavy queries on Prod DB will lock tables and slow down the app.
*   **Compression:** Mentioned that Columnar storage optimizes compression (10x smaller).

## What to Tighten Up ğŸ”§
*   *I/O Patterns:* Be specific.
    *   *Row Store:* To read "Age", it must read [ID, Name, **Age**, Address] from disk. It's reading 4x the data needed.
    *   *Column Store:* It reads ONLY the [Age, Age, Age] block. It skips 90% of the I/O.
*   *Vectorized Execution:* Snowflake processes data in batches (vectors) using SIMD instructions, not row-by-row like Postgres.

# Model Answer

---

"It comes down to **Storage Layout** and **I/O Efficiency**.

**Postgres (OLTP - Row Oriented)**
It stores data like `[ID, Name, Age, Salary]`. If I want to calculate `AVG(Salary)`, the database has to read the *entire row* from disk just to extract the salary. I am reading 90% useless data.
Also, running a massive aggregation locks rows, potentially blocking a user from logging in.

**Snowflake (OLAP - Column Oriented)**
It stores data like `[Salary, Salary, Salary]`. When I run `AVG(Salary)`, it only reads that specific block from disk. It ignores ID, Name, and Age. This reduces I/O by 90%+.
Additionally, because all 'Salary' values are numbers stored together, compression is incredibly efficient (Run-Length Encoding), making scans even faster."

# Diagrams

```text
ROW vs COLUMN I/O
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  QUERY: SELECT AVG(Salary) FROM Users                                 â•‘
â•‘                                                                       â•‘
â•‘  POSTGRES (Row Store)                                                 â•‘
â•‘  Disk Read:                                                           â•‘
â•‘  [ID,Name,Age,Salary] [ID,Name,Age,Salary] [ID,Name,Age,Salary]       â•‘
â•‘               ^^^^^^               ^^^^^^               ^^^^^^        â•‘
â•‘  Result: Reads entire disk blocks. Cache fills with "Name" junk.      â•‘
â•‘                                                                       â•‘
â•‘  SNOWFLAKE (Column Store)                                             â•‘
â•‘  Disk Read:                                                           â•‘
â•‘  [Salary, Salary, Salary]                                             â•‘
â•‘   ^^^^^^  ^^^^^^  ^^^^^^                                              â•‘
â•‘  Result: Reads ONLY the target data. 10x less I/O.                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## Question 3 â€” Lead Level (Scaling)

**"We have a write-heavy application (IoT sensors) sending 100k events per second. We are storing it in MySQL and it's crashing. How do you re-architect this? Do we just shard MySQL or move to NoSQL?"**

# Feedback First

## What You Nailed âœ…
*   **Bottleneck:** Identified that MySQL has a limit on Write IOPS (Vertical Scaling limit).
*   **Sharding:** Mentioned splitting MySQL as an option but complex to manage.
*   **NoSQL:** Suggested moving to Cassandra/DynamoDB for horizontal write scaling.
*   **Time Series:** Mentioned that sensor data is time-series, which fits well with specific DBs (TimescaleDB or Cassandra).

## What to Tighten Up ğŸ”§
*   *The Write Path:* Explain **LSM Trees (Log Structured Merge Trees)**. This is *why* Cassandra/RocksDB writes fast. They append to a log in memory and flush to disk sequentially. MySQL (B-Tree) has to jump around disk to insert into the middle of the index (Random I/O).
*   *Access Patterns:* Ask the followup: "How do you query this data?" If they need complex joins, NoSQL will fail. If they just need "Give me last hour of sensor X", NoSQL is perfect.

# Model Answer

---

"MySQL uses **B-Trees**, which are great for reads but expensive for random writes at scale (update-in-place). 100k TPS is pushing the limits of a single B-Tree index.

**My Plan:**
1.  **Analyze Queries:** Do we need Joins? If yes, we stick with SQL and shard. If no (just 'Select * where sensor_id=X'), we move to NoSQL.
2.  **Move to NoSQL (Cassandra/DynamoDB):** These use **LSM Trees**.
    *   Writes are strictly **Append-Only** (Sequential I/O).
    *   They land in memory (MemTable) and flush to disk (SSTable) later.
    *   This allows them to handle millions of writes per second.
3.  **Partitioning Strategy:**
    *   Partition Key: `sensor_id`
    *   Cluster Key: `timestamp` (Sort order)
    *   This ensures data for one sensor is kept together on disk, making the query 'Get history for Sensor A' incredibly fast."

# Diagrams

```text
B-TREE vs LSM TREE (Why NoSQL writes faster)
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  MYSQL (B-TREE) - Random I/O                                          â•‘
â•‘  To insert "5", I must traverse the tree, find the exact page,        â•‘
â•‘  load it, modify it, and write it back.                               â•‘
â•‘           [ 4  |  8 ]                                                 â•‘
â•‘          /     \                                                      â•‘
â•‘      [1,2,3]  [5,6,9]  <-- Modification happens deep here.            â•‘
â•‘                                                                       â•‘
â•‘  NO-SQL (LSM TREE) - Sequential I/O                                   â•‘
â•‘  To insert "5", I just append it to the log in memory. Done.          â•‘
â•‘  RAM: [ 5 ]                                                           â•‘
â•‘          â”‚                                                            â•‘
â•‘          â–¼ (Flush later)                                              â•‘
â•‘  DISK: [ 1, 2, 3, 4, 5, 6, 8, 9 ] (Sorted String Table)               â•‘
â•‘                                                                       â•‘
â•‘  Result: NoSQL writes are O(1) memory operations.                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```
