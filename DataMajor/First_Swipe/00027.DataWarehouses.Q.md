# Mock Interview â€” Data Warehousing (Redshift / Snowflake)

Same rules â€” answer in the room, model answer and diagrams after each response.

---

## Question 1 â€” Warming Up (The "Lake vs Warehouse" Check)

**"I see you have S3 (Data Lake) and Snowflake (Data Warehouse) on your resume. Why do we need both? Why can't we just run SQL on top of S3 using Athena and call it a day?"**

# Feedback First

## What You Nailed âœ…
*   **Structure:** Correctly identified that the Lake is for raw/unstructured data (Schema-on-Read) and the Warehouse is for curated/structured data (Schema-on-Write).
*   **Cost:** Mentioned that S3 storage is cheaper than Snowflake storage.
*   **Performance:** Correctly stated that a Warehouse with indexes/metadata is faster for complex joins than scanning raw JSON in S3.

## What to Tighten Up ğŸ”§
*   *The "Governance" argument:* This is key. A Warehouse is "Trusted." A Data Lake is a "Swamp." You don't let the CFO run financial reports off a raw JSON dump in S3 that might have duplicates. You need the ETL process to clean it first.
*   *ACID:* Warehouses support ACID transactions (updates/deletes). S3 is eventually consistent (mostly) and doesn't handle row-level updates well.

# Model Answer

---

"We use them for different stages of the data lifecycle.

**Data Lake (S3):**
The landing zone. It's cheap and can hold anything (Logs, Images, raw CSVs). We use it because we don't want to lose data just because we don't have a schema for it yet. But it's messy. **Schema-on-Read.**

**Data Warehouse (Snowflake):**
The trusted source. We ETL data from the Lake to the Warehouse to apply business logic, data quality checks, and structure (Star Schema).
We need Snowflake because:
1.  **Performance:** It uses micro-partitions and metadata to make queries 100x faster than Athena scanning S3.
2.  **Concurrency:** It can handle 1,000 analysts running queries at the same time.
3.  **Governance:** We can control row-level security and guarantee the numbers are correct for regulatory reporting."

# Diagrams

```text
THE MODERN DATA STACK (Lake -> Warehouse)
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  RAW DATA (The Swamp)            CURATED DATA (The Temple)            â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â•‘
â•‘  â”‚ S3 BUCKET         â”‚           â”‚ SNOWFLAKE         â”‚                â•‘
â•‘  â”‚ [log_v1.json]     â”‚ ETL Job   â”‚ [Fact_Sales]      â”‚                â•‘
â•‘  â”‚ [img_001.jpg]     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ (Clean, Dedupe,   â”‚                â•‘
â•‘  â”‚ [cust_dump.csv]   â”‚ Spark/dbt â”‚  Type Checked)    â”‚                â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â•‘
â•‘                                            â”‚                          â•‘
â•‘  Query Speed: Slow (Scan)                  â–¼                          â•‘
â•‘  Trust: Low (Duplicates?)        Query Speed: Fast (Indexed)          â•‘
â•‘  Cost: $                         Trust: High (Golden Source)          â•‘
â•‘                                  Cost: $$$                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## Question 2 â€” Data Engineer Level (The "Star Schema" Check)

**"Explain to me how you would design a Data Warehouse schema for a Retail Store. What are your Fact tables and what are your Dimension tables? Why not just put everything in one giant table?"**

# Feedback First

## What You Nailed âœ…
*   **Fact vs Dimension:** Correctly identified 'Sales' as the Fact and 'Products/Users' as Dimensions.
*   **Normalization:** Correctly explained that storing the User's address in every single Sales row is a waste of space and makes updates hard.

## What to Tighten Up ğŸ”§
*   *Grain:* You must use the word **Grain**. "The Grain of the Fact table is one row per line-item."
*   *Surrogate Keys:* A pro move. Don't join on `user_email` (Subject to change). Join on `user_sk` (System generated integer).
*   *Date Dimension:* Always mention a `Dim_Date`. It allows you to query "Weekends vs Weekdays" without complex SQL functions. It's the hallmark of a good warehouse design.

# Model Answer

---

"I would use a **Star Schema** centered around the business process: `Fact_Sales`.

**The Grain:**
One row per line item in a transaction.

**Fact Table (`Fact_Sales`):**
*   Keys: `date_key`, `product_key`, `store_key`, `customer_key`.
*   Metrics: `quantity`, `unit_price`, `total_discount`.

**Dimension Tables:**
*   `Dim_Product`: Name, Category, Manufacturer, Size.
*   `Dim_Store`: Address, Region, Manager Name.
*   `Dim_Date`: Day of Week, Is Holiday?, Quarter, Fiscal Year.

**Why not one giant table?**
If we denormalized everything, and a Store Manager changed their name, we would have to update 10 million rows of historical sales data. In a Star Schema, we update **one** row in `Dim_Store`, and all reports reflect the change instantly."

# Diagrams

```text
why STAR SCHEMA? (Because Updates are Hard)
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  SCENARIO: Manager of Store 1 changes from "Bob" to "Alice"           â•‘
â•‘                                                                       â•‘
â•‘  OPTION A: ONE GIANT TABLE (The Flat File Nightmare)                  â•‘
â•‘  Row 1: [Date, Prod, Store1, "Bob", $10]                              â•‘
â•‘  Row 2: [Date, Prod, Store1, "Bob", $50]                              â•‘
â•‘  ... (1 Million Rows) ...                                             â•‘
â•‘  ACTION: UPDATE BigTable SET Manager="Alice" WHERE Store=1            â•‘
â•‘  RESULT: Database locks up for 20 minutes updating 1M rows. ğŸ’¥        â•‘
â•‘                                                                       â•‘
â•‘  OPTION B: STAR SCHEMA (The Clean Way)                                â•‘
â•‘  [FACT_SALES]                    [DIM_STORE]                          â•‘
â•‘  Row 1: [Store_ID: 1, $10]       ID: 1                                â•‘
â•‘  Row 2: [Store_ID: 1, $50]       Manager: "Bob" -> "Alice"            â•‘
â•‘                                                                       â•‘
â•‘  ACTION: UPDATE DimStore SET Manager="Alice" WHERE ID=1               â•‘
â•‘  RESULT: 1 row updated. 0ms latency. All reports updated. âœ…          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## Question 3 â€” Lead Level (SCD Type 2)

**"We need to track history. If a user moves from Virginia to New York, we want all their *past* transactions to count towards Virginia, but new ones to count towards New York. How do you implement this in the Warehouse?"**

# Feedback First

## What You Nailed âœ…
*   **SCD:** Correctly identified this as a **Slowly Changing Dimension**.
*   **Type 2:** Correctly identified Type 2 (Add a new row) vs Type 1 (Overwrite).

## What to Tighten Up ğŸ”§
*   *The "Effective Date" Columns:* You need `start_date` and `end_date` columns to query the history correctly.
*   *The "Current Flag":* Mention adding an `is_current` boolean for fast "Current State" queries.
*   *Surrogate Keys:* This is critical here. You cannot join on `User_ID` anymore because there are now two rows for User 123. You must generate a unique `User_Key` for each version of that user.

# Model Answer

---

"This requires a **Slowly Changing Dimension (SCD) Type 2**.

We cannot simply overwrite the state from 'VA' to 'NY' (that would be Type 1), because it would rewrite history and make it look like they *always* lived in NY. Regulatory reporting would fail.

**Implementation:**
In `Dim_Users`, we add three columns: `start_date`, `end_date`, and `is_current`.

**When the user moves:**
1.  **Expire the old row:** Update the 'VA' row. Set `end_date = today` and `is_current = false`.
2.  **Insert the new row:** Insert a 'NY' row. Set `start_date = today`, `end_date = null`, and `is_current = true`.

**The Join:**
When querying sales history, we join `Fact_Sales.date` BETWEEN `Dim_Users.start_date` AND `Dim_Users.end_date`.
This ensures that a transaction from 2021 joins to the 'VA' record, and a transaction from 2022 joins to the 'NY' record. "

# Diagrams

```text
SCD TYPE 2 (Tracking History)
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  DIM_USERS TABLE                                                      â•‘
â•‘  â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â•‘
â•‘  â”‚ SK â”‚ ID   â”‚ State â”‚ Start_Date â”‚ End_Date   â”‚ Is_Current â”‚         â•‘
â•‘  â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â•‘
â•‘  â”‚ 101â”‚ U123 â”‚ VA    â”‚ 2020-01-01 â”‚ 2022-05-01 â”‚ false      â”‚         â•‘
â•‘  â”‚ 102â”‚ U123 â”‚ NY    â”‚ 2022-05-01 â”‚ NULL       â”‚ true       â”‚         â•‘
â•‘  â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â•‘
â•‘                                                                       â•‘
â•‘  QUERY TIMELINE:                                                      â•‘
â•‘                                                                       â•‘
â•‘  [Transaction Jan 2021] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Joins to Row 101 (VA) âœ…      â•‘
â•‘                                                                       â•‘
â•‘  [Transaction June 2022] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Joins to Row 102 (NY) âœ…      â•‘
â•‘                                                                       â•‘
â•‘  Result: History is preserved. "Point-in-time" correctness.           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```
