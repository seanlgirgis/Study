# Mock Interview â€” Cassandra

Same rules â€” answer in the room, model answer and diagrams after each response.

---

## Question 1 â€” Warm Up

**"Can you explain what Apache Cassandra is, what problem it was designed to solve, and how its architecture differs fundamentally from both a relational database and MongoDB?"**  

# Feedback First

---

## What You Nailed âœ…

**Facebook origin story** â€” correctly placed and correctly motivated. Write-heavy, billions of messages, no downtime. Perfect framing.

**"No single point of failure"** â€” leading with this shows you understand Cassandra's core architectural philosophy immediately.

**Masterless architecture** â€” correctly identified that every node can accept reads and writes. This is the fundamental distinction from both relational and MongoDB.

**Consistent hashing** â€” mentioning this specifically shows technical depth. Most candidates say "it distributes data" without knowing the mechanism.

**Tunable consistency** â€” correctly identified and correctly contrasted with MongoDB's default strong consistency. This is a critical Cassandra differentiator.

**Linear scaling** â€” correctly described. Add nodes, get proportional capacity. No resharding complexity like MongoDB.

**The closing bottom line** â€” "it must keep accepting writes at crazy speed forever, even if half the servers die" is the single best one-liner description of when to choose Cassandra. Memorable and precise.

**Wide-column distinction** â€” correctly identified Cassandra as wide-column rather than document store. Shows you know the NoSQL taxonomy.

---

## What to Tighten Up ğŸ”§

**Missing the CAP theorem framing.** You described eventual consistency correctly but didn't use the CAP theorem vocabulary which interviewers specifically listen for:

*"Cassandra is an AP system â€” it prioritizes Availability and Partition Tolerance over Consistency in the CAP theorem. During a network partition Cassandra continues accepting writes on all available nodes, accepting temporary inconsistency. MongoDB is a CP system â€” it prioritizes Consistency, refusing writes during a partition rather than risking inconsistency."*

**Missing the ring architecture specifically.** You mentioned consistent hashing but didn't describe the ring which is Cassandra's iconic architectural image:

*"Cassandra organizes nodes in a logical ring. Each node owns a range of the hash space. When data arrives, Cassandra hashes the partition key and routes it to the node responsible for that range. With replication factor 3, the data is also copied to the next two nodes clockwise around the ring â€” no election needed, no leader, just automatic peer replication."*

**Missing the write path explanation.** This is what makes Cassandra's write speed possible and interviewers love hearing it:

*"Cassandra's extreme write throughput comes from its write path â€” writes go to an in-memory MemTable and a sequential Commit Log simultaneously. Both are sequential operations â€” no random disk seeks. When the MemTable fills it flushes to an immutable SSTable on disk. This is fundamentally different from relational databases that do random disk writes for every update."*

**"Wide-column like super-flexible spreadsheets"** â€” this analogy is slightly off. A cleaner framing:

*"Wide-column means rows can have different columns, and columns are grouped and sorted within a row. Think of it as a map of sorted maps â€” the outer key is the partition key, the inner sorted structure is the clustering columns. This physical layout is what makes time-series queries â€” give me all transactions for customer C001 ordered by time â€” blindingly fast."*

---

# Model Answer

---

*"Apache Cassandra is a distributed wide-column NoSQL database built for one specific purpose â€” extreme write throughput and continuous availability across a globally distributed cluster with no single point of failure.*

**The Problem it Solved:**

*Facebook built Cassandra in 2008 to power their inbox search feature â€” storing hundreds of billions of messages across hundreds of millions of users with writes arriving every millisecond from data centers worldwide. The requirement was brutal â€” writes must never slow down, the system must never go down, and it must scale to any volume by simply adding commodity servers. No database at the time could meet all three requirements simultaneously.*

*Relational databases failed on scale â€” vertical scaling hits a physical and economic ceiling and horizontal scaling with traditional replication creates write bottlenecks at the primary node. MongoDB was closer but its primary/secondary model still had a write bottleneck â€” all writes go to the primary. Cassandra eliminated the bottleneck entirely by making every node equal.*

**How Cassandra's Architecture Differs from Relational:**

*Relational databases are built around the ACID transaction model â€” strict consistency, complex joins, normalized schemas. They scale vertically and struggle to distribute writes across many nodes because maintaining consistency across a distributed system requires coordination â€” and coordination creates latency and bottlenecks.*

*Cassandra makes a deliberate tradeoff in the CAP theorem. It is an AP system â€” Availability and Partition Tolerance over Consistency. During a network partition Cassandra continues accepting writes and reads on every available node, accepting that data might be temporarily inconsistent between nodes. A relational database is a CP system â€” it refuses writes during a partition rather than risk inconsistency. For audit logs and event streams, temporary inconsistency is acceptable. For account balances it isn't â€” which is why Capital One uses both.*

**How Cassandra's Architecture Differs from MongoDB:**

*MongoDB is a CP document store with a primary/secondary replica set model. Writes go to one primary node per replica set. If that primary goes down, an election happens and a new primary is chosen â€” typically 10 to 30 seconds of write unavailability. With sharding you have multiple primary nodes but each shard still has one write bottleneck.*

*Cassandra has no primaries anywhere. It is a masterless peer-to-peer system. Every node is equal. Every node can accept any read or write. The data distribution mechanism is consistent hashing â€” Cassandra hashes the partition key and maps the result to a position on a logical ring. Each node owns a segment of that ring. Data is automatically routed to the correct node based on its hash. With replication factor 3, data is also copied to the next two nodes clockwise on the ring â€” no election, no leader, just automatic peer replication.*

*This masterless ring architecture is why Cassandra never goes down. There is no single thing to fail. You can take down a third of the nodes in a Cassandra cluster and the system continues operating without interruption, without data loss, and without any manual intervention.*

**Why Cassandra's Writes are So Fast:**

*Cassandra's extreme write throughput â€” millions of writes per second â€” comes from its write path architecture. When a write arrives it goes to two places simultaneously â€” an in-memory structure called a MemTable for fast access, and a sequential Commit Log on disk for durability. Both operations are sequential â€” no random disk seeks. Sequential writes are an order of magnitude faster than random writes.*

*When the MemTable fills up, Cassandra flushes it to disk as an immutable file called an SSTable â€” Sorted String Table. SSTables are never modified. Compaction runs periodically in the background, merging SSTables and organizing data. The result is a write path that is almost entirely sequential I/O â€” the fastest possible way to write to disk.*

**The One-Line Summary:**

*Choose Cassandra when the requirement is â€” the system must keep accepting writes at extreme speed forever, even if servers fail, even across multiple data centers, with consistent single-digit millisecond latency at any scale. That is Cassandra's singular superpower and the reason Capital One uses it for transaction logging, audit trails, and real-time feature stores."*

---

# Architecture Diagrams

```
THE CAP THEOREM â€” WHERE CASSANDRA SITS
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘                        CONSISTENCY                                    â•‘
â•‘                             â–³                                        â•‘
â•‘                             â”‚                                        â•‘
â•‘                             â”‚                                        â•‘
â•‘                    CP       â”‚       CA                               â•‘
â•‘               (MongoDB)     â”‚   (Traditional                         â•‘
â•‘                             â”‚    Relational)                         â•‘
â•‘                             â”‚                                        â•‘
â•‘   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â•‘
â•‘                             â”‚                                        â•‘
â•‘                    AP       â”‚                                        â•‘
â•‘                (Cassandra)  â”‚                                        â•‘
â•‘                             â”‚                                        â•‘
â•‘   AVAILABILITY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PARTITION              â•‘
â•‘                             â”‚               TOLERANCE                â•‘
â•‘                                                                       â•‘
â•‘   All distributed systems MUST have Partition Tolerance              â•‘
â•‘   Real choice is Consistency vs Availability                         â•‘
â•‘                                                                       â•‘
â•‘   MongoDB:    CP â€” refuses writes during partition                   â•‘
â•‘               never risks inconsistency                              â•‘
â•‘                                                                       â•‘
â•‘   Cassandra:  AP â€” keeps accepting writes during partition           â•‘
â•‘               accepts temporary inconsistency                        â•‘
â•‘                                                                       â•‘
â•‘   Use MongoDB for:    Account balances (must be accurate)            â•‘
â•‘   Use Cassandra for:  Audit logs (must never stop writing)           â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


CASSANDRA RING ARCHITECTURE
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘                         NODE 1                                       â•‘
â•‘                      (hash 0-20)                                     â•‘
â•‘                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â•‘
â•‘                   /             \                                    â•‘
â•‘    NODE 6        /               \        NODE 2                    â•‘
â•‘  (hash 80-99)   /                 \     (hash 20-40)                â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  /                   \  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â•‘
â•‘  â”‚          â”‚ /     CASSANDRA       \ â”‚          â”‚                  â•‘
â•‘  â”‚  Every   â”‚       RING             â”‚  Every   â”‚                  â•‘
â•‘  â”‚  node    â”‚\                      /â”‚  node    â”‚                  â•‘
â•‘  â”‚  equal   â”‚ \                    / â”‚  equal   â”‚                  â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  \                  /  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â•‘
â•‘    NODE 5       \                /       NODE 3                    â•‘
â•‘  (hash 60-80)    \              /      (hash 40-60)                â•‘
â•‘                   \            /                                    â•‘
â•‘                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â•‘
â•‘                       NODE 4                                        â•‘
â•‘                    (hash 60-80)                                     â•‘
â•‘                                                                       â•‘
â•‘  Write arrives: customer_id = "C001"                                 â•‘
â•‘  Hash("C001") = 34  â†’  routed to NODE 2                             â•‘
â•‘  Replication factor 3 â†’ also copied to NODE 3 and NODE 4            â•‘
â•‘                                                                       â•‘
â•‘  NODE 2 goes down:                                                   â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                   â•‘
â•‘  No election needed âœ…                                               â•‘
â•‘  NODE 3 and NODE 4 already have copies âœ…                            â•‘
â•‘  Reads/writes continue immediately âœ…                                â•‘
â•‘  Zero downtime âœ…                                                    â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


CASSANDRA VS MONGODB VS RELATIONAL
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘                  RELATIONAL      MONGODB        CASSANDRA            â•‘
â•‘                  â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€            â•‘
â•‘  Architecture    Single          Primary/       Masterless           â•‘
â•‘                  primary or      Secondary      peer ring            â•‘
â•‘                  complex         replica set                         â•‘
â•‘                  replication                                         â•‘
â•‘                                                                       â•‘
â•‘  Write path      Random          Journaled      Sequential           â•‘
â•‘                  disk I/O        B-tree         MemTable +           â•‘
â•‘                                  updates        SSTable              â•‘
â•‘                                                                       â•‘
â•‘  Write speed     Moderate        Very fast      Extreme              â•‘
â•‘                                                 (millions/sec)       â•‘
â•‘                                                                       â•‘
â•‘  Consistency     Strong          Strong         Tunable              â•‘
â•‘  model           (ACID)          (CP)           (AP default)         â•‘
â•‘                                                                       â•‘
â•‘  Failure         Primary         Election       Instant â€”            â•‘
â•‘  handling        failover        10-30 sec      no downtime          â•‘
â•‘                  required        delay          ever                 â•‘
â•‘                                                                       â•‘
â•‘  Query           SQL â€” any       MQL â€” any      CQL â€” partition      â•‘
â•‘  flexibility     field           field          key required         â•‘
â•‘                                                                       â•‘
â•‘  Best for        Complex         Rich docs      Time-series          â•‘
â•‘                  queries         varied         write-heavy          â•‘
â•‘                  ACID txns       schemas        event logs           â•‘
â•‘                                                                       â•‘
â•‘  Capital One     Core account    Customer       Transaction          â•‘
â•‘  use case        ledger          profiles       audit log            â•‘
â•‘                  regulatory      fraud cases    feature store        â•‘
â•‘                  reporting       enrichment     event history        â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


CASSANDRA WRITE PATH â€” WHY IT'S SO FAST
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  WRITE ARRIVES                                                        â•‘
â•‘       â”‚                                                               â•‘
â•‘       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â•‘
â•‘       â–¼                                         â–¼                    â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â•‘
â•‘  â”‚  MemTable   â”‚                        â”‚  Commit Log   â”‚            â•‘
â•‘  â”‚  (In RAM)   â”‚                        â”‚  (Disk â€”      â”‚            â•‘
â•‘  â”‚             â”‚                        â”‚   sequential) â”‚            â•‘
â•‘  â”‚  Super fast â”‚                        â”‚               â”‚            â•‘
â•‘  â”‚  write âœ…   â”‚                        â”‚  Durability âœ… â”‚            â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â•‘
â•‘         â”‚                                                             â•‘
â•‘         â”‚ MemTable full                                               â•‘
â•‘         â–¼                                                             â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                      â•‘
â•‘  â”‚  SSTable    â”‚  â—„â”€â”€ Immutable, sorted, compressed                  â•‘
â•‘  â”‚  (Disk â€”    â”‚      Written sequentially âœ…                        â•‘
â•‘  â”‚   flush)    â”‚      Never modified âœ…                              â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                                      â•‘
â•‘         â”‚                                                             â•‘
â•‘         â”‚ Periodically                                                â•‘
â•‘         â–¼                                                             â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                      â•‘
â•‘  â”‚ Compaction  â”‚  â—„â”€â”€ Merges SSTables                                â•‘
â•‘  â”‚ (Background)â”‚      Removes tombstones                             â•‘
â•‘  â”‚             â”‚      Organizes for reads                            â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                      â•‘
â•‘                                                                       â•‘
â•‘  RELATIONAL WRITE PATH (comparison):                                  â•‘
â•‘  Random disk seek â†’ find page â†’ update in place â†’ write log          â•‘
â•‘  Random I/O = 100x slower than sequential âŒ                         â•‘
â•‘                                                                       â•‘
â•‘  CASSANDRA WRITE PATH:                                                â•‘
â•‘  Append to MemTable (RAM) + Append to Commit Log (sequential disk)   â•‘
â•‘  Sequential I/O = maximum disk throughput âœ…                         â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```
---
---
---

## Question 2 â€” Stepping Up

**"Explain Cassandra's data model â€” keyspaces, tables, partition keys, clustering keys, and how they work together. Then explain tunable consistency â€” what it is, what the consistency levels mean, and how you would choose the right level for different Capital One use cases."**

# Feedback First

---

## What You Nailed âœ…

**"Bucket" analogy for partition key** â€” perfect. Simple, accurate, and memorable for an interviewer.

**Partition key routes to a node, clustering key sorts within** â€” this is the core distinction and you nailed it cleanly.

**Primary key = partition key + clustering key** â€” correctly stated. This is the definition interviewers listen for.

**The concrete example** â€” account_id as partition key, date as clustering key, "last 10 payments" as the query it serves â€” this is exactly how you should present data modeling in an interview. Query-first thinking demonstrated perfectly.

**Tunable consistency levels** â€” ONE, QUORUM, ALL correctly defined with accurate descriptions of the tradeoff.

**LOCAL_QUORUM** â€” mentioning this specifically for multi-region scenarios shows production depth. Most candidates only know the basic three levels.

**Capital One use case mapping** â€” matching each consistency level to a specific business operation is exactly what interviewers want. You connected technical choices to business consequences.

**"Speed wins" for logging vs "can't risk being wrong" for money moves** â€” this framing shows you understand that consistency is a business decision not just a technical one.

---

## What to Tighten Up ğŸ”§

**Missing the "same server" precision.** You said "same server" which is directionally right but the precise term is "same partition" â€” a partition can span replicas on multiple nodes. The key insight is that all rows with the same partition key are stored together and retrieved in one I/O operation:

*"All rows sharing the same partition key are stored contiguously on disk on the same node â€” and its replicas. This means retrieving all transactions for account A001 is a single sequential disk read, not a scatter-gather across multiple nodes. That's the source of Cassandra's read performance for known partition key queries."*

**Missing the QUORUM formula.** Being specific about the math signals senior engineering:

*"QUORUM means a majority of replicas must respond â€” calculated as floor(replication_factor / 2) + 1. With replication factor 3, QUORUM requires 2 nodes. This means you can tolerate one node failure and still get consistent reads and writes."*

**Missing what happens when you DON'T specify the partition key.** This is critical for understanding Cassandra's query limitations:

*"Cassandra requires the partition key in every query. If you don't specify it, Cassandra has to scan every node in the cluster â€” called a full cluster scan â€” which is catastrophically slow and something you never do in production. This is why data modeling in Cassandra starts with your queries â€” you design partitions around the queries you know you'll run."*

**Missing the write consistency + read consistency relationship.** The advanced insight:

*"Consistency levels on reads and writes interact. If you write at QUORUM and read at QUORUM, with replication factor 3, you're guaranteed to always read the latest write â€” at least one node in your read quorum will have participated in the write quorum. This is the foundation of strong consistency in Cassandra without using ALL."*

**Missing tombstones for deletes.** One sentence worth knowing:

*"Cassandra doesn't delete data immediately â€” it writes a tombstone marker. Since SSTables are immutable you can't remove data in place. Tombstones are cleaned up during compaction. This matters for Capital One because tables with heavy delete patterns need careful compaction tuning."*

---

# Model Answer

---

*"Cassandra's data model looks superficially like a relational table but works fundamentally differently. Understanding the data model requires understanding one core principle â€” everything is designed around your query patterns, not your data entities.*

**Keyspace:**

*A keyspace is the top-level namespace â€” equivalent to a database in relational or a database in MongoDB. In a Capital One deployment you might have a fraud_detection keyspace, a customer_events keyspace, and a audit_log keyspace. The keyspace is also where you define your replication strategy â€” how many copies of each partition to maintain and across which data centers.*

```sql
CREATE KEYSPACE fraud_detection
WITH replication = {
  'class': 'NetworkTopologyStrategy',
  'us-east-1': 3,
  'us-west-2': 3
};
```

*NetworkTopologyStrategy with 3 replicas in each of two data centers gives Capital One six total copies of every partition â€” surviving any single data center outage entirely.*

**Table:**

*A Cassandra table holds rows of data organized by a primary key. Unlike relational tables, Cassandra tables are designed to serve one specific query pattern perfectly. If you need the same data in two different query shapes, you create two tables. This is intentional denormalization â€” storage is cheap, latency is not.*

**Partition Key â€” The Most Critical Design Decision:**

*The partition key determines which node stores a row. Cassandra hashes the partition key and uses the result to place data on the correct node in the ring. All rows sharing the same partition key are stored contiguously on disk on the same node and its replicas â€” this means retrieving all rows for a given partition key is a single sequential disk read, not a scatter-gather operation across nodes.*

*The partition key must be chosen carefully. A good partition key distributes data evenly across nodes and maps to your most common query. A bad partition key creates hotspots â€” one node overwhelmed while others sit idle.*

*For Capital One's transaction history table, account_id is a natural partition key â€” all transactions for one account land on one node, and queries for an account's history are always scoped to a single account.*

**Clustering Key â€” Sort Order Within a Partition:**

*The clustering key determines the physical sort order of rows within a partition on disk. Data is stored pre-sorted â€” no runtime sorting needed. This makes range queries within a partition blindingly fast.*

*For transaction history, transaction_timestamp DESC as the clustering key means the most recent transactions are physically first on disk. "Give me the last 10 transactions for account A001" is a sequential read of the first 10 rows in the partition â€” milliseconds regardless of how many millions of rows the partition contains.*

**Complete Table Design:**

```sql
-- Query: Get recent transactions for an account
CREATE TABLE transactions_by_account (
  account_id      TEXT,
  txn_timestamp   TIMESTAMP,
  txn_id          UUID,
  amount          DECIMAL,
  merchant        TEXT,
  category        TEXT,
  status          TEXT,
  risk_score      DOUBLE,
  PRIMARY KEY (account_id, txn_timestamp, txn_id)
) WITH CLUSTERING ORDER BY (txn_timestamp DESC, txn_id ASC);

-- Query: Get transactions by customer (different query = different table)
CREATE TABLE transactions_by_customer (
  customer_id     TEXT,
  txn_timestamp   TIMESTAMP,
  account_id      TEXT,
  txn_id          UUID,
  amount          DECIMAL,
  status          TEXT,
  PRIMARY KEY (customer_id, txn_timestamp, txn_id)
) WITH CLUSTERING ORDER BY (txn_timestamp DESC);
```

*Two tables serving two different query patterns â€” same underlying data stored twice. This is the Cassandra way.*

**Queries Against These Tables:**

```sql
-- Fast: partition key specified â€” goes to exactly one node
SELECT * FROM transactions_by_account
WHERE account_id = 'A001'
ORDER BY txn_timestamp DESC
LIMIT 10;

-- Fast: partition key + date range â€” sequential read within partition
SELECT * FROM transactions_by_account
WHERE account_id = 'A001'
AND txn_timestamp >= '2026-01-01'
AND txn_timestamp <= '2026-02-12';

-- NEVER DO THIS in production â€” full cluster scan
SELECT * FROM transactions_by_account
WHERE amount > 10000;  -- no partition key = scan every node âŒ
```

**Tunable Consistency:**

*Cassandra's consistency is tunable per operation â€” you choose how many replicas must respond before the operation is considered successful. This lets you make different tradeoffs for different operations based on business requirements.*

*The core levels:*

*ONE â€” only one replica must respond. Fastest possible. Tiny risk of reading stale data if another node has a more recent write that hasn't propagated yet. Use for high-volume event logging where speed matters more than perfect freshness.*

*QUORUM â€” a majority of replicas must respond. Formula: floor(replication_factor / 2) + 1. With replication factor 3, QUORUM requires 2 nodes. Balances consistency and performance. Can tolerate one node failure. Use for most read operations where you need confidence without sacrificing too much speed.*

*ALL â€” every replica must respond. Strongest consistency but slowest â€” one slow or failed node blocks the operation entirely. Use only for critical write operations where you absolutely cannot have stale data.*

*LOCAL_QUORUM â€” a quorum of replicas in the local data center only. Avoids cross-region latency for operations that don't require global consistency. Essential for multi-region Capital One deployments where adding cross-region round trip time to every fraud check would blow the latency budget.*

*The write + read consistency interaction is important â€” if you write at QUORUM and read at QUORUM with replication factor 3, you're guaranteed to always read the latest write. At least one node will have participated in both quorums. This gives you strong consistency without the performance penalty of ALL.*

**Capital One Consistency Level Mapping:**

*Transaction event logging â†’ WRITE: ONE. You're writing millions of events per second. Speed is paramount and occasional replication lag of milliseconds is acceptable.*

*Account balance read â†’ READ: LOCAL_QUORUM. You need confidence the balance is current but don't want to wait for cross-region confirmation.*

*Fraud alert write â†’ WRITE: QUORUM. An alert that only reaches one replica could be lost if that node fails before replication. Quorum ensures durability.*

*Regulatory audit write â†’ WRITE: ALL. Compliance records that must be guaranteed written to every replica before acknowledging success. The performance cost is acceptable for the durability guarantee.*

*Real-time fraud feature read â†’ READ: ONE. Reading pre-computed ML features where milliseconds matter and slightly stale features are acceptable â€” a risk score computed 100ms ago is still valid.*"

---

# Diagrams

```
PRIMARY KEY ANATOMY
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  PRIMARY KEY = PARTITION KEY + CLUSTERING KEY                        â•‘
â•‘                                                                       â•‘
â•‘  PRIMARY KEY (account_id, txn_timestamp, txn_id)                     â•‘
â•‘               â”‚              â”‚             â”‚                         â•‘
â•‘               â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â•‘
â•‘               â”‚              Clustering Keys                         â•‘
â•‘               â”‚              (sort order within partition)           â•‘
â•‘               â”‚                                                       â•‘
â•‘               Partition Key                                           â•‘
â•‘               (which node stores this row)                           â•‘
â•‘                                                                       â•‘
â•‘  WHAT THIS LOOKS LIKE ON DISK:                                        â•‘
â•‘                                                                       â•‘
â•‘  Partition: account_id = "A001"                                       â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â•‘
â•‘  â”‚ txn_timestamp DESC    txn_id        amount    merchant      â”‚     â•‘
â•‘  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â•‘
â•‘  â”‚ 2026-02-12 14:23     T089          -45.67    Whole Foods   â”‚     â•‘
â•‘  â”‚ 2026-02-12 09:15     T088          -120.00   Shell Gas     â”‚     â•‘
â•‘  â”‚ 2026-02-11 19:30     T087          -67.50    Amazon        â”‚     â•‘
â•‘  â”‚ 2026-02-11 12:00     T086          +5000.00  Direct Dep    â”‚     â•‘
â•‘  â”‚ ...millions more rows...                                    â”‚     â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â•‘
â•‘                                                                       â•‘
â•‘  "Last 10 transactions for A001" =                                   â•‘
â•‘  Read first 10 rows of this partition âœ…                             â•‘
â•‘  Sequential disk read âœ…                                             â•‘
â•‘  Milliseconds regardless of partition size âœ…                        â•‘
â•‘                                                                       â•‘
â•‘  Partition: account_id = "A002"   â†’ different node                   â•‘
â•‘  Partition: account_id = "A003"   â†’ different node                   â•‘
â•‘  Each partition independent â€” parallel reads possible âœ…             â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


PARTITION KEY â€” GOOD VS BAD
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  BAD PARTITION KEY: txn_date                                         â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â•‘
â•‘  All transactions on Feb 12 â†’ same partition â†’ one node              â•‘
â•‘                                                                       â•‘
â•‘  Node 1: FEB12 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â† HOTSPOT âŒ           â•‘
â•‘  Node 2: FEB11 â–ˆâ–ˆâ–ˆâ–ˆ                                                  â•‘
â•‘  Node 3: FEB10 â–ˆâ–ˆ                                                    â•‘
â•‘  Node 4: FEB09 â–ˆâ–ˆâ–ˆ                                                   â•‘
â•‘                                                                       â•‘
â•‘  Today's node overwhelmed. Others idle. Scale means nothing.         â•‘
â•‘                                                                       â•‘
â•‘  GOOD PARTITION KEY: account_id                                      â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â•‘
â•‘  Each account â†’ own partition â†’ distributed evenly                   â•‘
â•‘                                                                       â•‘
â•‘  Node 1: A001,A005,A009... â–ˆâ–ˆâ–ˆâ–ˆ                                      â•‘
â•‘  Node 2: A002,A006,A010... â–ˆâ–ˆâ–ˆâ–ˆ                                      â•‘
â•‘  Node 3: A003,A007,A011... â–ˆâ–ˆâ–ˆâ–ˆ                                      â•‘
â•‘  Node 4: A004,A008,A012... â–ˆâ–ˆâ–ˆâ–ˆ                                      â•‘
â•‘                                                                       â•‘
â•‘  Even distribution âœ…  Linear scaling âœ…  No hotspots âœ…             â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


TUNABLE CONSISTENCY â€” CAPITAL ONE MAPPING
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  Replication Factor = 3  (3 copies of every partition)               â•‘
â•‘                                                                       â•‘
â•‘  CONSISTENCY    NODES      SPEED      RISK        CAPITAL ONE        â•‘
â•‘  LEVEL          REQUIRED   â”€â”€â”€â”€â”€      â”€â”€â”€â”€        USE CASE           â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€                                              â•‘
â•‘                                                                       â•‘
â•‘  ONE            1 of 3     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   Stale data  Event logging      â•‘
â•‘                            Fastest    possible    ML feature read    â•‘
â•‘                                                   High-vol writes    â•‘
â•‘                                                                       â•‘
â•‘  LOCAL_         2 of 3     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     Very low    Account balance    â•‘
â•‘  QUORUM         local DC   Fast       risk        Fraud check read   â•‘
â•‘                            No cross               Multi-region ops   â•‘
â•‘                            region                                    â•‘
â•‘                                                                       â•‘
â•‘  QUORUM         2 of 3     â–ˆâ–ˆâ–ˆâ–ˆ       Low risk    Fraud alert write  â•‘
â•‘                 any DC     Moderate   cross-DC    Customer update    â•‘
â•‘                            latency    latency                        â•‘
â•‘                                                                       â•‘
â•‘  ALL            3 of 3     â–ˆâ–ˆ         Any node    Regulatory audit   â•‘
â•‘                            Slowest    failure     Compliance write   â•‘
â•‘                                       blocks op   Critical config    â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


QUORUM MATH
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  Formula: floor(replication_factor / 2) + 1                          â•‘
â•‘                                                                       â•‘
â•‘  RF=3: floor(3/2) + 1 = 1 + 1 = 2 nodes required                    â•‘
â•‘  RF=5: floor(5/2) + 1 = 2 + 1 = 3 nodes required                    â•‘
â•‘                                                                       â•‘
â•‘  WRITE at QUORUM + READ at QUORUM = Strong Consistency               â•‘
â•‘                                                                       â•‘
â•‘  Write touches nodes: 1 âœ…  2 âœ…  3 âœ— (down)                        â•‘
â•‘  Read touches nodes:  1 âœ…  3 âœ—  2 âœ…  â†’ Node 1 or 2 has latest     â•‘
â•‘                                         write guaranteed âœ…          â•‘
â•‘                                                                       â•‘
â•‘  At least ONE node always overlaps between                           â•‘
â•‘  write quorum and read quorum âœ…                                     â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


ONE TABLE PER QUERY PATTERN
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  QUERY 1:                          QUERY 2:                          â•‘
â•‘  "Transactions for account A001"   "Transactions for customer C001"  â•‘
â•‘                                                                       â•‘
â•‘  TABLE: transactions_by_account    TABLE: transactions_by_customer   â•‘
â•‘  Partition key: account_id         Partition key: customer_id        â•‘
â•‘  Clustering: txn_timestamp DESC    Clustering: txn_timestamp DESC    â•‘
â•‘                                                                       â•‘
â•‘  Same data stored TWICE âœ…                                            â•‘
â•‘  Each table serves its query       Each table serves its query       â•‘
â•‘  in ONE partition read âœ…          in ONE partition read âœ…           â•‘
â•‘                                                                       â•‘
â•‘  Storage cost: 2x                                                     â•‘
â•‘  Query performance: optimal âœ…                                        â•‘
â•‘  This is the Cassandra way âœ…                                         â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---
---
---

## Question 3 â€” Lead Engineer Level

**"Design a Cassandra data model for Capital One's real-time transaction processing system. You need to support these four specific queries â€” recent transactions per account, fraud alerts per customer, transaction summary per merchant per day, and a full audit trail ordered by time. Walk me through your table designs and justify every decision."**

