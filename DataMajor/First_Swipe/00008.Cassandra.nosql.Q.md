# Mock Interview â€” Cassandra

Same rules â€” answer in the room, model answer and diagrams after each response.

---

## Question 1 â€” Warm Up

**"Can you explain what Apache Cassandra is, what problem it was designed to solve, and how its architecture differs fundamentally from both a relational database and MongoDB?"**  

# Feedback First

---

## What You Nailed âœ…

**Facebook origin story** â€” correctly placed and correctly motivated. Write-heavy, billions of messages, no downtime. Perfect framing.

**"No single point of failure"** â€” leading with this shows you understand Cassandra's core architectural philosophy immediately.

**Masterless architecture** â€” correctly identified that every node can accept reads and writes. This is the fundamental distinction from both relational and MongoDB.

**Consistent hashing** â€” mentioning this specifically shows technical depth. Most candidates say "it distributes data" without knowing the mechanism.

**Tunable consistency** â€” correctly identified and correctly contrasted with MongoDB's default strong consistency. This is a critical Cassandra differentiator.

**Linear scaling** â€” correctly described. Add nodes, get proportional capacity. No resharding complexity like MongoDB.

**The closing bottom line** â€” "it must keep accepting writes at crazy speed forever, even if half the servers die" is the single best one-liner description of when to choose Cassandra. Memorable and precise.

**Wide-column distinction** â€” correctly identified Cassandra as wide-column rather than document store. Shows you know the NoSQL taxonomy.

---

## What to Tighten Up ðŸ”§

**Missing the CAP theorem framing.** You described eventual consistency correctly but didn't use the CAP theorem vocabulary which interviewers specifically listen for:

*"Cassandra is an AP system â€” it prioritizes Availability and Partition Tolerance over Consistency in the CAP theorem. During a network partition Cassandra continues accepting writes on all available nodes, accepting temporary inconsistency. MongoDB is a CP system â€” it prioritizes Consistency, refusing writes during a partition rather than risking inconsistency."*

**Missing the ring architecture specifically.** You mentioned consistent hashing but didn't describe the ring which is Cassandra's iconic architectural image:

*"Cassandra organizes nodes in a logical ring. Each node owns a range of the hash space. When data arrives, Cassandra hashes the partition key and routes it to the node responsible for that range. With replication factor 3, the data is also copied to the next two nodes clockwise around the ring â€” no election needed, no leader, just automatic peer replication."*

**Missing the write path explanation.** This is what makes Cassandra's write speed possible and interviewers love hearing it:

*"Cassandra's extreme write throughput comes from its write path â€” writes go to an in-memory MemTable and a sequential Commit Log simultaneously. Both are sequential operations â€” no random disk seeks. When the MemTable fills it flushes to an immutable SSTable on disk. This is fundamentally different from relational databases that do random disk writes for every update."*

**"Wide-column like super-flexible spreadsheets"** â€” this analogy is slightly off. A cleaner framing:

*"Wide-column means rows can have different columns, and columns are grouped and sorted within a row. Think of it as a map of sorted maps â€” the outer key is the partition key, the inner sorted structure is the clustering columns. This physical layout is what makes time-series queries â€” give me all transactions for customer C001 ordered by time â€” blindingly fast."*

---

# Model Answer

---

*"Apache Cassandra is a distributed wide-column NoSQL database built for one specific purpose â€” extreme write throughput and continuous availability across a globally distributed cluster with no single point of failure.*

**The Problem it Solved:**

*Facebook built Cassandra in 2008 to power their inbox search feature â€” storing hundreds of billions of messages across hundreds of millions of users with writes arriving every millisecond from data centers worldwide. The requirement was brutal â€” writes must never slow down, the system must never go down, and it must scale to any volume by simply adding commodity servers. No database at the time could meet all three requirements simultaneously.*

*Relational databases failed on scale â€” vertical scaling hits a physical and economic ceiling and horizontal scaling with traditional replication creates write bottlenecks at the primary node. MongoDB was closer but its primary/secondary model still had a write bottleneck â€” all writes go to the primary. Cassandra eliminated the bottleneck entirely by making every node equal.*

**How Cassandra's Architecture Differs from Relational:**

*Relational databases are built around the ACID transaction model â€” strict consistency, complex joins, normalized schemas. They scale vertically and struggle to distribute writes across many nodes because maintaining consistency across a distributed system requires coordination â€” and coordination creates latency and bottlenecks.*

*Cassandra makes a deliberate tradeoff in the CAP theorem. It is an AP system â€” Availability and Partition Tolerance over Consistency. During a network partition Cassandra continues accepting writes and reads on every available node, accepting that data might be temporarily inconsistent between nodes. A relational database is a CP system â€” it refuses writes during a partition rather than risk inconsistency. For audit logs and event streams, temporary inconsistency is acceptable. For account balances it isn't â€” which is why Capital One uses both.*

**How Cassandra's Architecture Differs from MongoDB:**

*MongoDB is a CP document store with a primary/secondary replica set model. Writes go to one primary node per replica set. If that primary goes down, an election happens and a new primary is chosen â€” typically 10 to 30 seconds of write unavailability. With sharding you have multiple primary nodes but each shard still has one write bottleneck.*

*Cassandra has no primaries anywhere. It is a masterless peer-to-peer system. Every node is equal. Every node can accept any read or write. The data distribution mechanism is consistent hashing â€” Cassandra hashes the partition key and maps the result to a position on a logical ring. Each node owns a segment of that ring. Data is automatically routed to the correct node based on its hash. With replication factor 3, data is also copied to the next two nodes clockwise on the ring â€” no election, no leader, just automatic peer replication.*

*This masterless ring architecture is why Cassandra never goes down. There is no single thing to fail. You can take down a third of the nodes in a Cassandra cluster and the system continues operating without interruption, without data loss, and without any manual intervention.*

**Why Cassandra's Writes are So Fast:**

*Cassandra's extreme write throughput â€” millions of writes per second â€” comes from its write path architecture. When a write arrives it goes to two places simultaneously â€” an in-memory structure called a MemTable for fast access, and a sequential Commit Log on disk for durability. Both operations are sequential â€” no random disk seeks. Sequential writes are an order of magnitude faster than random writes.*

*When the MemTable fills up, Cassandra flushes it to disk as an immutable file called an SSTable â€” Sorted String Table. SSTables are never modified. Compaction runs periodically in the background, merging SSTables and organizing data. The result is a write path that is almost entirely sequential I/O â€” the fastest possible way to write to disk.*

**The One-Line Summary:**

*Choose Cassandra when the requirement is â€” the system must keep accepting writes at extreme speed forever, even if servers fail, even across multiple data centers, with consistent single-digit millisecond latency at any scale. That is Cassandra's singular superpower and the reason Capital One uses it for transaction logging, audit trails, and real-time feature stores."*

---

# Architecture Diagrams

```
THE CAP THEOREM â€” WHERE CASSANDRA SITS
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘                        CONSISTENCY                                    â•‘
â•‘                             â–³                                        â•‘
â•‘                             â”‚                                        â•‘
â•‘                             â”‚                                        â•‘
â•‘                    CP       â”‚       CA                               â•‘
â•‘               (MongoDB)     â”‚   (Traditional                         â•‘
â•‘                             â”‚    Relational)                         â•‘
â•‘                             â”‚                                        â•‘
â•‘   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â•‘
â•‘                             â”‚                                        â•‘
â•‘                    AP       â”‚                                        â•‘
â•‘                (Cassandra)  â”‚                                        â•‘
â•‘                             â”‚                                        â•‘
â•‘   AVAILABILITY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PARTITION              â•‘
â•‘                             â”‚               TOLERANCE                â•‘
â•‘                                                                       â•‘
â•‘   All distributed systems MUST have Partition Tolerance              â•‘
â•‘   Real choice is Consistency vs Availability                         â•‘
â•‘                                                                       â•‘
â•‘   MongoDB:    CP â€” refuses writes during partition                   â•‘
â•‘               never risks inconsistency                              â•‘
â•‘                                                                       â•‘
â•‘   Cassandra:  AP â€” keeps accepting writes during partition           â•‘
â•‘               accepts temporary inconsistency                        â•‘
â•‘                                                                       â•‘
â•‘   Use MongoDB for:    Account balances (must be accurate)            â•‘
â•‘   Use Cassandra for:  Audit logs (must never stop writing)           â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


CASSANDRA RING ARCHITECTURE
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘                         NODE 1                                       â•‘
â•‘                      (hash 0-20)                                     â•‘
â•‘                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â•‘
â•‘                   /             \                                    â•‘
â•‘    NODE 6        /               \        NODE 2                    â•‘
â•‘  (hash 80-99)   /                 \     (hash 20-40)                â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  /                   \  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â•‘
â•‘  â”‚          â”‚ /     CASSANDRA       \ â”‚          â”‚                  â•‘
â•‘  â”‚  Every   â”‚       RING             â”‚  Every   â”‚                  â•‘
â•‘  â”‚  node    â”‚\                      /â”‚  node    â”‚                  â•‘
â•‘  â”‚  equal   â”‚ \                    / â”‚  equal   â”‚                  â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  \                  /  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â•‘
â•‘    NODE 5       \                /       NODE 3                    â•‘
â•‘  (hash 60-80)    \              /      (hash 40-60)                â•‘
â•‘                   \            /                                    â•‘
â•‘                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â•‘
â•‘                       NODE 4                                        â•‘
â•‘                    (hash 60-80)                                     â•‘
â•‘                                                                       â•‘
â•‘  Write arrives: customer_id = "C001"                                 â•‘
â•‘  Hash("C001") = 34  â†’  routed to NODE 2                             â•‘
â•‘  Replication factor 3 â†’ also copied to NODE 3 and NODE 4            â•‘
â•‘                                                                       â•‘
â•‘  NODE 2 goes down:                                                   â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                   â•‘
â•‘  No election needed âœ…                                               â•‘
â•‘  NODE 3 and NODE 4 already have copies âœ…                            â•‘
â•‘  Reads/writes continue immediately âœ…                                â•‘
â•‘  Zero downtime âœ…                                                    â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


CASSANDRA VS MONGODB VS RELATIONAL
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘                  RELATIONAL      MONGODB        CASSANDRA            â•‘
â•‘                  â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€            â•‘
â•‘  Architecture    Single          Primary/       Masterless           â•‘
â•‘                  primary or      Secondary      peer ring            â•‘
â•‘                  complex         replica set                         â•‘
â•‘                  replication                                         â•‘
â•‘                                                                       â•‘
â•‘  Write path      Random          Journaled      Sequential           â•‘
â•‘                  disk I/O        B-tree         MemTable +           â•‘
â•‘                                  updates        SSTable              â•‘
â•‘                                                                       â•‘
â•‘  Write speed     Moderate        Very fast      Extreme              â•‘
â•‘                                                 (millions/sec)       â•‘
â•‘                                                                       â•‘
â•‘  Consistency     Strong          Strong         Tunable              â•‘
â•‘  model           (ACID)          (CP)           (AP default)         â•‘
â•‘                                                                       â•‘
â•‘  Failure         Primary         Election       Instant â€”            â•‘
â•‘  handling        failover        10-30 sec      no downtime          â•‘
â•‘                  required        delay          ever                 â•‘
â•‘                                                                       â•‘
â•‘  Query           SQL â€” any       MQL â€” any      CQL â€” partition      â•‘
â•‘  flexibility     field           field          key required         â•‘
â•‘                                                                       â•‘
â•‘  Best for        Complex         Rich docs      Time-series          â•‘
â•‘                  queries         varied         write-heavy          â•‘
â•‘                  ACID txns       schemas        event logs           â•‘
â•‘                                                                       â•‘
â•‘  Capital One     Core account    Customer       Transaction          â•‘
â•‘  use case        ledger          profiles       audit log            â•‘
â•‘                  regulatory      fraud cases    feature store        â•‘
â•‘                  reporting       enrichment     event history        â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


CASSANDRA WRITE PATH â€” WHY IT'S SO FAST
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  WRITE ARRIVES                                                        â•‘
â•‘       â”‚                                                               â•‘
â•‘       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â•‘
â•‘       â–¼                                         â–¼                    â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â•‘
â•‘  â”‚  MemTable   â”‚                        â”‚  Commit Log   â”‚            â•‘
â•‘  â”‚  (In RAM)   â”‚                        â”‚  (Disk â€”      â”‚            â•‘
â•‘  â”‚             â”‚                        â”‚   sequential) â”‚            â•‘
â•‘  â”‚  Super fast â”‚                        â”‚               â”‚            â•‘
â•‘  â”‚  write âœ…   â”‚                        â”‚  Durability âœ… â”‚            â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â•‘
â•‘         â”‚                                                             â•‘
â•‘         â”‚ MemTable full                                               â•‘
â•‘         â–¼                                                             â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                      â•‘
â•‘  â”‚  SSTable    â”‚  â—„â”€â”€ Immutable, sorted, compressed                  â•‘
â•‘  â”‚  (Disk â€”    â”‚      Written sequentially âœ…                        â•‘
â•‘  â”‚   flush)    â”‚      Never modified âœ…                              â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                                      â•‘
â•‘         â”‚                                                             â•‘
â•‘         â”‚ Periodically                                                â•‘
â•‘         â–¼                                                             â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                      â•‘
â•‘  â”‚ Compaction  â”‚  â—„â”€â”€ Merges SSTables                                â•‘
â•‘  â”‚ (Background)â”‚      Removes tombstones                             â•‘
â•‘  â”‚             â”‚      Organizes for reads                            â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                      â•‘
â•‘                                                                       â•‘
â•‘  RELATIONAL WRITE PATH (comparison):                                  â•‘
â•‘  Random disk seek â†’ find page â†’ update in place â†’ write log          â•‘
â•‘  Random I/O = 100x slower than sequential âŒ                         â•‘
â•‘                                                                       â•‘
â•‘  CASSANDRA WRITE PATH:                                                â•‘
â•‘  Append to MemTable (RAM) + Append to Commit Log (sequential disk)   â•‘
â•‘  Sequential I/O = maximum disk throughput âœ…                         â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```
---
---
---

## Question 2 â€” Stepping Up

**"Explain Cassandra's data model â€” keyspaces, tables, partition keys, clustering keys, and how they work together. Then explain tunable consistency â€” what it is, what the consistency levels mean, and how you would choose the right level for different Capital One use cases."**

# Feedback First

---

## What You Nailed âœ…

**"Bucket" analogy for partition key** â€” perfect. Simple, accurate, and memorable for an interviewer.

**Partition key routes to a node, clustering key sorts within** â€” this is the core distinction and you nailed it cleanly.

**Primary key = partition key + clustering key** â€” correctly stated. This is the definition interviewers listen for.

**The concrete example** â€” account_id as partition key, date as clustering key, "last 10 payments" as the query it serves â€” this is exactly how you should present data modeling in an interview. Query-first thinking demonstrated perfectly.

**Tunable consistency levels** â€” ONE, QUORUM, ALL correctly defined with accurate descriptions of the tradeoff.

**LOCAL_QUORUM** â€” mentioning this specifically for multi-region scenarios shows production depth. Most candidates only know the basic three levels.

**Capital One use case mapping** â€” matching each consistency level to a specific business operation is exactly what interviewers want. You connected technical choices to business consequences.

**"Speed wins" for logging vs "can't risk being wrong" for money moves** â€” this framing shows you understand that consistency is a business decision not just a technical one.

---

## What to Tighten Up ðŸ”§

**Missing the "same server" precision.** You said "same server" which is directionally right but the precise term is "same partition" â€” a partition can span replicas on multiple nodes. The key insight is that all rows with the same partition key are stored together and retrieved in one I/O operation:

*"All rows sharing the same partition key are stored contiguously on disk on the same node â€” and its replicas. This means retrieving all transactions for account A001 is a single sequential disk read, not a scatter-gather across multiple nodes. That's the source of Cassandra's read performance for known partition key queries."*

**Missing the QUORUM formula.** Being specific about the math signals senior engineering:

*"QUORUM means a majority of replicas must respond â€” calculated as floor(replication_factor / 2) + 1. With replication factor 3, QUORUM requires 2 nodes. This means you can tolerate one node failure and still get consistent reads and writes."*

**Missing what happens when you DON'T specify the partition key.** This is critical for understanding Cassandra's query limitations:

*"Cassandra requires the partition key in every query. If you don't specify it, Cassandra has to scan every node in the cluster â€” called a full cluster scan â€” which is catastrophically slow and something you never do in production. This is why data modeling in Cassandra starts with your queries â€” you design partitions around the queries you know you'll run."*

**Missing the write consistency + read consistency relationship.** The advanced insight:

*"Consistency levels on reads and writes interact. If you write at QUORUM and read at QUORUM, with replication factor 3, you're guaranteed to always read the latest write â€” at least one node in your read quorum will have participated in the write quorum. This is the foundation of strong consistency in Cassandra without using ALL."*

**Missing tombstones for deletes.** One sentence worth knowing:

*"Cassandra doesn't delete data immediately â€” it writes a tombstone marker. Since SSTables are immutable you can't remove data in place. Tombstones are cleaned up during compaction. This matters for Capital One because tables with heavy delete patterns need careful compaction tuning."*

---

# Model Answer

---

*"Cassandra's data model looks superficially like a relational table but works fundamentally differently. Understanding the data model requires understanding one core principle â€” everything is designed around your query patterns, not your data entities.*

**Keyspace:**

*A keyspace is the top-level namespace â€” equivalent to a database in relational or a database in MongoDB. In a Capital One deployment you might have a fraud_detection keyspace, a customer_events keyspace, and a audit_log keyspace. The keyspace is also where you define your replication strategy â€” how many copies of each partition to maintain and across which data centers.*

```sql
CREATE KEYSPACE fraud_detection
WITH replication = {
  'class': 'NetworkTopologyStrategy',
  'us-east-1': 3,
  'us-west-2': 3
};
```

*NetworkTopologyStrategy with 3 replicas in each of two data centers gives Capital One six total copies of every partition â€” surviving any single data center outage entirely.*

**Table:**

*A Cassandra table holds rows of data organized by a primary key. Unlike relational tables, Cassandra tables are designed to serve one specific query pattern perfectly. If you need the same data in two different query shapes, you create two tables. This is intentional denormalization â€” storage is cheap, latency is not.*

**Partition Key â€” The Most Critical Design Decision:**

*The partition key determines which node stores a row. Cassandra hashes the partition key and uses the result to place data on the correct node in the ring. All rows sharing the same partition key are stored contiguously on disk on the same node and its replicas â€” this means retrieving all rows for a given partition key is a single sequential disk read, not a scatter-gather operation across nodes.*

*The partition key must be chosen carefully. A good partition key distributes data evenly across nodes and maps to your most common query. A bad partition key creates hotspots â€” one node overwhelmed while others sit idle.*

*For Capital One's transaction history table, account_id is a natural partition key â€” all transactions for one account land on one node, and queries for an account's history are always scoped to a single account.*

**Clustering Key â€” Sort Order Within a Partition:**

*The clustering key determines the physical sort order of rows within a partition on disk. Data is stored pre-sorted â€” no runtime sorting needed. This makes range queries within a partition blindingly fast.*

*For transaction history, transaction_timestamp DESC as the clustering key means the most recent transactions are physically first on disk. "Give me the last 10 transactions for account A001" is a sequential read of the first 10 rows in the partition â€” milliseconds regardless of how many millions of rows the partition contains.*

**Complete Table Design:**

```sql
-- Query: Get recent transactions for an account
CREATE TABLE transactions_by_account (
  account_id      TEXT,
  txn_timestamp   TIMESTAMP,
  txn_id          UUID,
  amount          DECIMAL,
  merchant        TEXT,
  category        TEXT,
  status          TEXT,
  risk_score      DOUBLE,
  PRIMARY KEY (account_id, txn_timestamp, txn_id)
) WITH CLUSTERING ORDER BY (txn_timestamp DESC, txn_id ASC);

-- Query: Get transactions by customer (different query = different table)
CREATE TABLE transactions_by_customer (
  customer_id     TEXT,
  txn_timestamp   TIMESTAMP,
  account_id      TEXT,
  txn_id          UUID,
  amount          DECIMAL,
  status          TEXT,
  PRIMARY KEY (customer_id, txn_timestamp, txn_id)
) WITH CLUSTERING ORDER BY (txn_timestamp DESC);
```

*Two tables serving two different query patterns â€” same underlying data stored twice. This is the Cassandra way.*

**Queries Against These Tables:**

```sql
-- Fast: partition key specified â€” goes to exactly one node
SELECT * FROM transactions_by_account
WHERE account_id = 'A001'
ORDER BY txn_timestamp DESC
LIMIT 10;

-- Fast: partition key + date range â€” sequential read within partition
SELECT * FROM transactions_by_account
WHERE account_id = 'A001'
AND txn_timestamp >= '2026-01-01'
AND txn_timestamp <= '2026-02-12';

-- NEVER DO THIS in production â€” full cluster scan
SELECT * FROM transactions_by_account
WHERE amount > 10000;  -- no partition key = scan every node âŒ
```

**Tunable Consistency:**

*Cassandra's consistency is tunable per operation â€” you choose how many replicas must respond before the operation is considered successful. This lets you make different tradeoffs for different operations based on business requirements.*

*The core levels:*

*ONE â€” only one replica must respond. Fastest possible. Tiny risk of reading stale data if another node has a more recent write that hasn't propagated yet. Use for high-volume event logging where speed matters more than perfect freshness.*

*QUORUM â€” a majority of replicas must respond. Formula: floor(replication_factor / 2) + 1. With replication factor 3, QUORUM requires 2 nodes. Balances consistency and performance. Can tolerate one node failure. Use for most read operations where you need confidence without sacrificing too much speed.*

*ALL â€” every replica must respond. Strongest consistency but slowest â€” one slow or failed node blocks the operation entirely. Use only for critical write operations where you absolutely cannot have stale data.*

*LOCAL_QUORUM â€” a quorum of replicas in the local data center only. Avoids cross-region latency for operations that don't require global consistency. Essential for multi-region Capital One deployments where adding cross-region round trip time to every fraud check would blow the latency budget.*

*The write + read consistency interaction is important â€” if you write at QUORUM and read at QUORUM with replication factor 3, you're guaranteed to always read the latest write. At least one node will have participated in both quorums. This gives you strong consistency without the performance penalty of ALL.*

**Capital One Consistency Level Mapping:**

*Transaction event logging â†’ WRITE: ONE. You're writing millions of events per second. Speed is paramount and occasional replication lag of milliseconds is acceptable.*

*Account balance read â†’ READ: LOCAL_QUORUM. You need confidence the balance is current but don't want to wait for cross-region confirmation.*

*Fraud alert write â†’ WRITE: QUORUM. An alert that only reaches one replica could be lost if that node fails before replication. Quorum ensures durability.*

*Regulatory audit write â†’ WRITE: ALL. Compliance records that must be guaranteed written to every replica before acknowledging success. The performance cost is acceptable for the durability guarantee.*

*Real-time fraud feature read â†’ READ: ONE. Reading pre-computed ML features where milliseconds matter and slightly stale features are acceptable â€” a risk score computed 100ms ago is still valid.*"

---

# Diagrams

```
PRIMARY KEY ANATOMY
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  PRIMARY KEY = PARTITION KEY + CLUSTERING KEY                        â•‘
â•‘                                                                       â•‘
â•‘  PRIMARY KEY (account_id, txn_timestamp, txn_id)                     â•‘
â•‘               â”‚              â”‚             â”‚                         â•‘
â•‘               â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â•‘
â•‘               â”‚              Clustering Keys                         â•‘
â•‘               â”‚              (sort order within partition)           â•‘
â•‘               â”‚                                                       â•‘
â•‘               Partition Key                                           â•‘
â•‘               (which node stores this row)                           â•‘
â•‘                                                                       â•‘
â•‘  WHAT THIS LOOKS LIKE ON DISK:                                        â•‘
â•‘                                                                       â•‘
â•‘  Partition: account_id = "A001"                                       â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â•‘
â•‘  â”‚ txn_timestamp DESC    txn_id        amount    merchant      â”‚     â•‘
â•‘  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â•‘
â•‘  â”‚ 2026-02-12 14:23     T089          -45.67    Whole Foods   â”‚     â•‘
â•‘  â”‚ 2026-02-12 09:15     T088          -120.00   Shell Gas     â”‚     â•‘
â•‘  â”‚ 2026-02-11 19:30     T087          -67.50    Amazon        â”‚     â•‘
â•‘  â”‚ 2026-02-11 12:00     T086          +5000.00  Direct Dep    â”‚     â•‘
â•‘  â”‚ ...millions more rows...                                    â”‚     â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â•‘
â•‘                                                                       â•‘
â•‘  "Last 10 transactions for A001" =                                   â•‘
â•‘  Read first 10 rows of this partition âœ…                             â•‘
â•‘  Sequential disk read âœ…                                             â•‘
â•‘  Milliseconds regardless of partition size âœ…                        â•‘
â•‘                                                                       â•‘
â•‘  Partition: account_id = "A002"   â†’ different node                   â•‘
â•‘  Partition: account_id = "A003"   â†’ different node                   â•‘
â•‘  Each partition independent â€” parallel reads possible âœ…             â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


PARTITION KEY â€” GOOD VS BAD
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  BAD PARTITION KEY: txn_date                                         â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â•‘
â•‘  All transactions on Feb 12 â†’ same partition â†’ one node              â•‘
â•‘                                                                       â•‘
â•‘  Node 1: FEB12 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â† HOTSPOT âŒ           â•‘
â•‘  Node 2: FEB11 â–ˆâ–ˆâ–ˆâ–ˆ                                                  â•‘
â•‘  Node 3: FEB10 â–ˆâ–ˆ                                                    â•‘
â•‘  Node 4: FEB09 â–ˆâ–ˆâ–ˆ                                                   â•‘
â•‘                                                                       â•‘
â•‘  Today's node overwhelmed. Others idle. Scale means nothing.         â•‘
â•‘                                                                       â•‘
â•‘  GOOD PARTITION KEY: account_id                                      â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â•‘
â•‘  Each account â†’ own partition â†’ distributed evenly                   â•‘
â•‘                                                                       â•‘
â•‘  Node 1: A001,A005,A009... â–ˆâ–ˆâ–ˆâ–ˆ                                      â•‘
â•‘  Node 2: A002,A006,A010... â–ˆâ–ˆâ–ˆâ–ˆ                                      â•‘
â•‘  Node 3: A003,A007,A011... â–ˆâ–ˆâ–ˆâ–ˆ                                      â•‘
â•‘  Node 4: A004,A008,A012... â–ˆâ–ˆâ–ˆâ–ˆ                                      â•‘
â•‘                                                                       â•‘
â•‘  Even distribution âœ…  Linear scaling âœ…  No hotspots âœ…             â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


TUNABLE CONSISTENCY â€” CAPITAL ONE MAPPING
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  Replication Factor = 3  (3 copies of every partition)               â•‘
â•‘                                                                       â•‘
â•‘  CONSISTENCY    NODES      SPEED      RISK        CAPITAL ONE        â•‘
â•‘  LEVEL          REQUIRED   â”€â”€â”€â”€â”€      â”€â”€â”€â”€        USE CASE           â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€                                              â•‘
â•‘                                                                       â•‘
â•‘  ONE            1 of 3     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   Stale data  Event logging      â•‘
â•‘                            Fastest    possible    ML feature read    â•‘
â•‘                                                   High-vol writes    â•‘
â•‘                                                                       â•‘
â•‘  LOCAL_         2 of 3     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     Very low    Account balance    â•‘
â•‘  QUORUM         local DC   Fast       risk        Fraud check read   â•‘
â•‘                            No cross               Multi-region ops   â•‘
â•‘                            region                                    â•‘
â•‘                                                                       â•‘
â•‘  QUORUM         2 of 3     â–ˆâ–ˆâ–ˆâ–ˆ       Low risk    Fraud alert write  â•‘
â•‘                 any DC     Moderate   cross-DC    Customer update    â•‘
â•‘                            latency    latency                        â•‘
â•‘                                                                       â•‘
â•‘  ALL            3 of 3     â–ˆâ–ˆ         Any node    Regulatory audit   â•‘
â•‘                            Slowest    failure     Compliance write   â•‘
â•‘                                       blocks op   Critical config    â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


QUORUM MATH
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  Formula: floor(replication_factor / 2) + 1                          â•‘
â•‘                                                                       â•‘
â•‘  RF=3: floor(3/2) + 1 = 1 + 1 = 2 nodes required                    â•‘
â•‘  RF=5: floor(5/2) + 1 = 2 + 1 = 3 nodes required                    â•‘
â•‘                                                                       â•‘
â•‘  WRITE at QUORUM + READ at QUORUM = Strong Consistency               â•‘
â•‘                                                                       â•‘
â•‘  Write touches nodes: 1 âœ…  2 âœ…  3 âœ— (down)                        â•‘
â•‘  Read touches nodes:  1 âœ…  3 âœ—  2 âœ…  â†’ Node 1 or 2 has latest     â•‘
â•‘                                         write guaranteed âœ…          â•‘
â•‘                                                                       â•‘
â•‘  At least ONE node always overlaps between                           â•‘
â•‘  write quorum and read quorum âœ…                                     â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


ONE TABLE PER QUERY PATTERN
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  QUERY 1:                          QUERY 2:                          â•‘
â•‘  "Transactions for account A001"   "Transactions for customer C001"  â•‘
â•‘                                                                       â•‘
â•‘  TABLE: transactions_by_account    TABLE: transactions_by_customer   â•‘
â•‘  Partition key: account_id         Partition key: customer_id        â•‘
â•‘  Clustering: txn_timestamp DESC    Clustering: txn_timestamp DESC    â•‘
â•‘                                                                       â•‘
â•‘  Same data stored TWICE âœ…                                            â•‘
â•‘  Each table serves its query       Each table serves its query       â•‘
â•‘  in ONE partition read âœ…          in ONE partition read âœ…           â•‘
â•‘                                                                       â•‘
â•‘  Storage cost: 2x                                                     â•‘
â•‘  Query performance: optimal âœ…                                        â•‘
â•‘  This is the Cassandra way âœ…                                         â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---
---
---

## Question 3 â€” Lead Engineer Level

**"Design a Cassandra data model for Capital One's real-time transaction processing system. You need to support these four specific queries â€” recent transactions per account, fraud alerts per customer, transaction summary per merchant per day, and a full audit trail ordered by time. Walk me through your table designs and justify every decision."**

# Feedback First

---

## What You Nailed âœ…

**One table per query pattern** â€” you opened with this principle immediately. Shows you've internalized the most important Cassandra design concept.

**All four tables correctly identified** â€” transactions_by_account, fraud_alerts_by_customer, tx_summary_by_merchant_day, audit_trail_by_date. All four correct and appropriately named.

**Partition key reasoning for each table** â€” every table had a clear justification for why that partition key was chosen. This is exactly what interviewers want to hear.

**Compound partition key for merchant summary** â€” merchant_id + date together is exactly right. This prevents the hotspot of all merchant data landing in one partition while keeping daily summaries grouped. Advanced thinking.

**Counter columns for merchant summary** â€” mentioning Cassandra's native counter type for aggregations shows real Cassandra knowledge. Most candidates would miss this entirely.

**"Bucket gets too big" awareness for audit trail** â€” correctly identified that partitioning by date prevents unbounded partition growth. This is a production concern that junior engineers miss.

**"Copying data is fine"** â€” correctly embraced Cassandra's denormalization philosophy without apologizing for it.

**QUORUM for bank writes** â€” correct consistency level recommendation with correct justification.

---

## What to Tighten Up ðŸ”§

**Missing the partition size problem for audit_trail_by_date.** You correctly identified splitting by date but didn't quantify why it matters:

*"At 10,000 transactions per second, a single day produces 864 million transactions. Storing all of those in one date partition would create a partition approaching gigabytes in size â€” Cassandra recommends keeping partitions under 100MB for optimal performance. Partitioning by date is the first step but at Capital One scale I'd use a bucket strategy â€” partition key of date + bucket_number where bucket is transaction_id modulo 100 â€” spreading each day's data across 100 partitions."*

**Missing the actual CQL for each table.** In a Lead Engineer interview showing the CREATE TABLE statements demonstrates you can actually implement what you're describing:

```sql
CREATE TABLE transactions_by_account (
  account_id    TEXT,
  txn_time      TIMESTAMP,
  txn_id        UUID,
  amount        DECIMAL,
  merchant      TEXT,
  status        TEXT,
  risk_score    DOUBLE,
  PRIMARY KEY (account_id, txn_time, txn_id)
) WITH CLUSTERING ORDER BY (txn_time DESC);
```

**Missing TTL â€” Time To Live.** For audit and fraud data, retention policies are a compliance requirement:

*"For the fraud_alerts table I'd set a TTL of 7 years â€” Cassandra's native TTL feature automatically expires records after a defined period. This handles regulatory retention requirements without manual cleanup jobs."*

**Missing the write path for counter tables.** Counter columns have specific rules:

*"Cassandra counter columns are special â€” you can only increment or decrement them, never set them to an absolute value. The table can only contain the partition key, clustering key, and counter columns â€” no mixing with regular columns. This means merchant summary data that needs both counters and descriptive fields requires two separate tables."*

---

# Model Answer

---

*"Cassandra data modeling starts with the queries, not the data. Four queries means four tables â€” one per access pattern. I'll walk through each table, its design decisions, and the CQL to implement it.*

**Query 1 â€” Recent transactions per account:**

```sql
CREATE TABLE transactions_by_account (
  account_id    TEXT,
  txn_time      TIMESTAMP,
  txn_id        UUID,
  amount        DECIMAL,
  merchant      TEXT,
  merchant_cat  TEXT,
  country       TEXT,
  status        TEXT,
  risk_score    DOUBLE,
  channel       TEXT,
  PRIMARY KEY (account_id, txn_time, txn_id)
) WITH CLUSTERING ORDER BY (txn_time DESC, txn_id ASC)
  AND default_time_to_live = 220752000;  -- 7 years in seconds
```

*Partition key is account_id â€” all transactions for one account land on one node, retrieved in one sequential disk read. Clustering key is txn_time DESC so newest transactions are physically first on disk â€” "last 10 transactions" reads the first 10 rows without scanning. txn_id is added as a tiebreaker to guarantee uniqueness when two transactions arrive in the same millisecond. TTL of 7 years handles regulatory retention automatically.*

*Query:*
```sql
SELECT * FROM transactions_by_account
WHERE account_id = 'A001'
LIMIT 10;
-- Returns newest 10 transactions instantly âœ…
```

**Query 2 â€” Fraud alerts per customer:**

```sql
CREATE TABLE fraud_alerts_by_customer (
  customer_id   TEXT,
  alert_time    TIMESTAMP,
  alert_id      UUID,
  account_id    TEXT,
  txn_id        TEXT,
  alert_type    TEXT,
  risk_score    DOUBLE,
  status        TEXT,  -- open, resolved, false_positive
  assigned_to   TEXT,
  notes         TEXT,
  PRIMARY KEY (customer_id, alert_time, alert_id)
) WITH CLUSTERING ORDER BY (alert_time DESC)
  AND default_time_to_live = 220752000;  -- 7 years
```

*Fraud alerts are separate from transactions for two reasons. First, the query pattern is different â€” fraud investigations look up by customer not account. Second, alerts have a lifecycle â€” status changes, notes get added, assignments change â€” and isolating them prevents those updates from touching the immutable transaction table.*

*Query:*
```sql
SELECT * FROM fraud_alerts_by_customer
WHERE customer_id = 'C001'
AND alert_time >= '2026-01-01'
LIMIT 20;
```

**Query 3 â€” Transaction summary per merchant per day:**

*Counter columns in Cassandra are special â€” they can only be incremented or decremented, never set absolutely, and cannot coexist with regular data columns. So I use two tables â€” one for counters, one for descriptive data.*

```sql
-- Counter table
CREATE TABLE merchant_daily_counters (
  merchant_id       TEXT,
  txn_date          DATE,
  total_count       COUNTER,
  approved_count    COUNTER,
  declined_count    COUNTER,
  flagged_count     COUNTER,
  total_amount_cents COUNTER,  -- store as cents to avoid decimal issues
  PRIMARY KEY (merchant_id, txn_date)
) WITH CLUSTERING ORDER BY (txn_date DESC);

-- Update on every transaction
UPDATE merchant_daily_counters
SET total_count = total_count + 1,
    approved_count = approved_count + 1,
    total_amount_cents = total_amount_cents + 4567
WHERE merchant_id = 'M001'
AND txn_date = '2026-02-13';
```

*Compound partition key of merchant_id plus txn_date prevents hotspots â€” if merchant_id alone were the partition key, a high-volume merchant like Amazon would have one massive partition growing forever. Adding txn_date caps each partition at one day's worth of data and naturally expires old partitions as they age out of the retention window.*

**Query 4 â€” Full audit trail ordered by time:**

*The naive design â€” partition by date â€” has a serious problem at Capital One scale. At 10,000 transactions per second a single day produces 864 million transactions. One partition holding 864 million rows approaches gigabytes. Cassandra recommends keeping partitions under 100MB for optimal read performance. I solve this with a bucketing strategy:*

```sql
CREATE TABLE audit_trail (
  txn_date      DATE,
  bucket        INT,            -- txn_id.hashCode() % 100
  txn_time      TIMESTAMP,
  txn_id        UUID,
  account_id    TEXT,
  customer_id   TEXT,
  amount        DECIMAL,
  merchant      TEXT,
  status        TEXT,
  risk_score    DOUBLE,
  processed_by  TEXT,           -- which Flink node processed this
  kafka_offset  BIGINT,         -- traceability back to Kafka
  PRIMARY KEY ((txn_date, bucket), txn_time, txn_id)
) WITH CLUSTERING ORDER BY (txn_time ASC)
  AND default_time_to_live = 220752000;
```

*The compound partition key of txn_date plus bucket spreads each day across 100 partitions. Each partition holds approximately 8.64 million transactions â€” well within Cassandra's performance envelope. To read the full audit trail for a day, query all 100 buckets in parallel and merge the results. The kafka_offset field provides complete traceability â€” any auditor can trace a transaction from the audit table back to the exact Kafka partition and offset it came from.*

*Query for full day audit â€” run in parallel for all 100 buckets:*
```sql
SELECT * FROM audit_trail
WHERE txn_date = '2026-02-13'
AND bucket = ?  -- 0 through 99, queried in parallel
ORDER BY txn_time ASC;
```

**Consistency Levels Per Table:**

```
TABLE                        WRITE           READ
â”€â”€â”€â”€â”€                        â”€â”€â”€â”€â”€           â”€â”€â”€â”€
transactions_by_account      QUORUM          LOCAL_QUORUM
fraud_alerts_by_customer     QUORUM          QUORUM
merchant_daily_counters      LOCAL_QUORUM    LOCAL_QUORUM
audit_trail                  QUORUM          ONE
```

*Audit trail reads use ONE because compliance queries are bulk historical reads â€” slightly stale data is acceptable and speed matters when pulling millions of records. Transaction and fraud alert reads use LOCAL_QUORUM because they feed real-time decisions and need confidence without cross-region latency."*

---

# Complete Design Diagram

```
FOUR QUERIES â†’ FOUR TABLES
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  INCOMING TRANSACTION                                                 â•‘
â•‘  account_id=A001, customer_id=C001, merchant=Amazon,                 â•‘
â•‘  amount=$45.67, timestamp=2026-02-13 14:23:11                        â•‘
â•‘         â”‚                                                             â•‘
â•‘         â”‚  Written to FOUR tables simultaneously                      â•‘
â•‘         â”‚                                                             â•‘
â•‘    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â•‘
â•‘    â”‚         â”‚              â”‚                â”‚        â”‚               â•‘
â•‘    â–¼         â–¼              â–¼                â–¼        â”‚               â•‘
â•‘                                                       â”‚               â•‘
â•‘ TABLE 1   TABLE 2        TABLE 3           TABLE 4    â”‚               â•‘
â•‘ txns_by   fraud_alerts   merchant_daily   audit_      â”‚               â•‘
â•‘ _account  _by_customer   _counters        trail       â”‚               â•‘
â•‘                                                       â”‚               â•‘
â•‘ PK:       PK:            PK:              PK:         â”‚               â•‘
â•‘ account_  customer_id    merchant_id      txn_date +  â”‚               â•‘
â•‘ id                       + txn_date       bucket      â”‚               â•‘
â•‘                                                       â”‚               â•‘
â•‘ CK:       CK:            CK:              CK:         â”‚               â•‘
â•‘ txn_time  alert_time     txn_date         txn_time    â”‚               â•‘
â•‘ DESC      DESC           DESC             ASC         â”‚               â•‘
â•‘                                                       â”‚               â•‘
â•‘ Query:    Query:         Query:           Query:      â”‚               â•‘
â•‘ Last 10   Recent alerts  Merchant daily   Full audit  â”‚               â•‘
â•‘ for acct  for customer   totals           by time     â”‚               â•‘
â•‘                                                       â”‚               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


BUCKET STRATEGY FOR AUDIT TRAIL
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  WITHOUT BUCKETING â€” DANGEROUS                                        â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â•‘
â•‘  Partition: date=2026-02-13                                           â•‘
â•‘  864,000,000 rows â†’ gigabytes â†’ performance collapse âŒ               â•‘
â•‘                                                                       â•‘
â•‘  WITH BUCKETING â€” SAFE                                                â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â•‘
â•‘  Partition: (2026-02-13, bucket=0)  â†’ 8.64M rows â†’ ~80MB âœ…          â•‘
â•‘  Partition: (2026-02-13, bucket=1)  â†’ 8.64M rows â†’ ~80MB âœ…          â•‘
â•‘  Partition: (2026-02-13, bucket=2)  â†’ 8.64M rows â†’ ~80MB âœ…          â•‘
â•‘  ...                                                                  â•‘
â•‘  Partition: (2026-02-13, bucket=99) â†’ 8.64M rows â†’ ~80MB âœ…          â•‘
â•‘                                                                       â•‘
â•‘  100 partitions Ã— 80MB = 8GB total for one day                       â•‘
â•‘  Spread across cluster nodes âœ…                                       â•‘
â•‘  Each partition within performance envelope âœ…                        â•‘
â•‘                                                                       â•‘
â•‘  Read full day: query 100 buckets IN PARALLEL                        â•‘
â•‘  Merge results in application layer                                   â•‘
â•‘  Total time: milliseconds âœ…                                          â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


COUNTER TABLE â€” HOW IT WORKS
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                       â•‘
â•‘  Transaction arrives: Amazon, $45.67, approved                       â•‘
â•‘         â”‚                                                             â•‘
â•‘         â–¼                                                             â•‘
â•‘  UPDATE merchant_daily_counters                                       â•‘
â•‘  SET total_count       = total_count + 1                             â•‘
â•‘      approved_count    = approved_count + 1                          â•‘
â•‘      total_amount_cents = total_amount_cents + 4567                  â•‘
â•‘  WHERE merchant_id = 'Amazon'                                        â•‘
â•‘  AND   txn_date    = '2026-02-13'                                    â•‘
â•‘                                                                       â•‘
â•‘  Result after millions of transactions:                               â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â•‘
â•‘  â”‚merchant  â”‚ txn_date   â”‚ total_count â”‚ total_amount     â”‚          â•‘
â•‘  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â•‘
â•‘  â”‚ Amazon   â”‚ 2026-02-13 â”‚  4,521,000  â”‚ $203,445,000.00  â”‚          â•‘
â•‘  â”‚ Walmart  â”‚ 2026-02-13 â”‚  2,103,000  â”‚  $94,635,000.00  â”‚          â•‘
â•‘  â”‚ Shell    â”‚ 2026-02-13 â”‚    891,000  â”‚  $44,550,000.00  â”‚          â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â•‘
â•‘                                                                       â•‘
â•‘  No aggregation query needed âœ…                                       â•‘
â•‘  One row read = instant daily summary âœ…                              â•‘
â•‘  Counters updated atomically by Cassandra âœ…                          â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

