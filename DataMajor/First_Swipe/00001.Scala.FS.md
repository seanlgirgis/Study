Great call â€” let's zoom out first. Understanding the *why* behind Scala makes the code make much more sense.

---

## What is Scala?

Scala stands for **Scalable Language**. It was created by **Martin Odersky** at EPFL (Switzerland) and released in **2003**. It runs on the **JVM (Java Virtual Machine)**, which means it compiles to the same bytecode as Java and can use any Java library natively.

Think of it as Java, but redesigned from the ground up to be more expressive, concise, and powerful.

---

## Why Was Scala Created?

Odersky was frustrated with Java's verbosity and limitations. He wanted a language that unified two programming paradigms that were traditionally separate:

**Object-Oriented Programming (OOP)** â€” everything is an object, like Java or Python.

**Functional Programming (FP)** â€” functions are first-class citizens, immutability is preferred, side effects are minimized.

Scala does both simultaneously, which is rare and powerful.

---

## Where is Scala Used?

This is the most important part for your Capital One prep:

**Big Data / Data Engineering** â€” This is Scala's biggest domain. Apache Spark was written entirely in Scala. When you write PySpark, Python is just a wrapper â€” the actual execution engine underneath is Scala. So when you write Spark in Scala, you're working directly with the native API, which means better performance and access to features that sometimes lag in PySpark.

**Financial Services** â€” Companies like Morgan Stanley, Goldman Sachs, Barclays, and yes, **Capital One** use Scala heavily for high-performance backend systems, risk engines, and data pipelines. The JVM gives them the reliability and performance Java is known for, while Scala gives them cleaner, more maintainable code.

**Distributed Systems** â€” Akka, a powerful toolkit for building concurrent and distributed applications, is built in Scala. Twitter, LinkedIn, and Netflix have used it extensively.

**Streaming** â€” Kafka Streams and Apache Flink have strong Scala APIs. Since Kafka is also on your prep list, this connection matters.

**Web Backend** â€” The Play Framework is a popular Scala web framework, though this is less relevant for your role.

---

## Scala vs Java vs Python â€” How They Compare

Java is verbose and strict. You write a lot of boilerplate to do simple things. It's rock solid and everywhere in enterprise.

Python is concise and readable but slow for heavy computation, and it's dynamically typed so errors show up at runtime rather than compile time.

Scala sits in the middle â€” concise like Python, fast and type-safe like Java. Here's a simple example showing the same logic in all three:

```java
// Java â€” very verbose
public int add(int a, int b) {
    return a + b;
}
```

```python
# Python â€” clean and simple
def add(a, b):
    return a + b
```

```scala
// Scala â€” concise AND type-safe
def add(a: Int, b: Int): Int = a + b
```

---

## Key Characteristics of Scala You'll Hear About

**Statically typed** â€” types are checked at compile time, not runtime. This catches bugs early, which is why financial companies love it.

**Type inference** â€” even though it's statically typed, you rarely have to declare types. Scala figures it out.

**Immutability by default** â€” the language encourages you not to change data, which makes distributed computing much safer. This is why Spark embraces it.

**Concise** â€” Scala code is typically 2-3x shorter than equivalent Java code doing the same thing.

**Interoperable with Java** â€” you can use any Java library in Scala without any wrappers or adapters.

**Strongly backed by the ecosystem** â€” SBT (Simple Build Tool) is Scala's build tool, similar to Maven or Gradle in Java.

---

## Scala in the Context of Your Capital One Role

When the job description says Scala, they almost certainly mean **Scala + Spark**. Data engineers at companies like Capital One use Scala to:

Write Spark jobs that process billions of transactions, build and maintain data pipelines that feed ML models, optimize performance-critical ETL processes where PySpark would be too slow, and work with streaming data coming through Kafka.

Your PySpark experience is a genuine asset here. The mental model is identical â€” you already understand DataFrames, transformations, actions, and lazy evaluation. You're essentially just learning new syntax for concepts you already know.

---

## One Important Thing to Know

Scala has a reputation for being **hard to learn**. That reputation is partly deserved â€” its advanced type system and functional programming features can get deeply complex. But for data engineering purposes, you don't need the deep end. You need maybe 30% of the language to be highly effective with Spark, and that 30% is very approachable, especially coming from Python.

---

Ready to get back into the code now that you have the full picture? We were just getting into higher-order functions and pattern matching â€” the exact features you'll use constantly in Spark jobs. ðŸŽ¯

---

# Phase 1 â€” Scala Fundamentals

## 1. The Basics â€” `val` vs `var`

In Scala, you declare variables in two ways:

```scala
val name = "John"   // immutable â€” like Python's constants (can't reassign)
var age = 30        // mutable â€” can be reassigned
age = 31            // âœ… fine
name = "Jane"       // âŒ error â€” val cannot be reassigned
```

In data engineering, you'll use `val` almost exclusively. Immutability is a core Scala principle.

---

## 2. Type System

Scala is **statically typed**, but type inference means you rarely need to declare types explicitly:

```scala
val name: String = "John"   // explicit type
val name = "John"           // inferred â€” Scala knows it's a String
val age = 30                // inferred as Int
val salary = 95000.50       // inferred as Double
val isActive = true         // inferred as Boolean
```

---

## 3. Functions

```scala
// Basic function
def greet(name: String): String = {
  "Hello, " + name
}

// Single-line (no braces needed)
def greet(name: String): String = "Hello, " + name

// Calling it
greet("John")   // returns "Hello, John"
```

Compare to Python:
```python
def greet(name: str) -> str:
    return "Hello, " + name
```

Very similar â€” just different syntax.

---

## 4. String Interpolation

```scala
val name = "John"
val age = 30

println(s"My name is $name and I am $age years old")
// "My name is John and I am 30 years old"

// Expression inside string
println(s"Next year I'll be ${age + 1}")
```

The `s` before the string enables interpolation â€” similar to Python's f-strings.

---

## 5. Collections â€” The Big Three

```scala
// List â€” ordered, immutable
val fruits = List("apple", "banana", "cherry")

// Map â€” key-value pairs
val scores = Map("Alice" -> 95, "Bob" -> 87)

// Seq â€” general sequence (List is a Seq)
val nums = Seq(1, 2, 3, 4, 5)
```

---

## Quick Exercise

Before we move on, what do you think this outputs?

```scala
val nums = List(1, 2, 3, 4, 5)
val doubled = nums.map(x => x * 2)
println(doubled)
```

Close, but not quite! The output would be:

```
List(2, 4, 6, 8, 10)
```

The key distinction â€” `map()` on a `List` **returns a List**, not a Map. It transforms each element and keeps the same collection type.

A `Map` in Scala is a key-value structure like `Map("a" -> 1)`. What you described would actually look like this:

```scala
val nums = List(1, 2, 3, 4, 5)
val doubled = nums.map(x => (x, x * 2))  // creates tuples
println(doubled.toMap)
// Output: Map(1 -> 2, 2 -> 4, 3 -> 6, 4 -> 8, 5 -> 10)
```

That's actually a great instinct though â€” you were thinking like a data engineer, turning it into key-value pairs. That pattern comes up a lot!

---

## Higher-Order Functions

This is where Scala gets powerful. These will feel familiar from PySpark.

```scala
val nums = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

// map â€” transform every element
val doubled = nums.map(x => x * 2)
// List(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)

// filter â€” keep elements that match condition
val evens = nums.filter(x => x % 2 == 0)
// List(2, 4, 6, 8, 10)

// reduce â€” collapse list into single value
val sum = nums.reduce((a, b) => a + b)
// 55

// chaining â€” this is where it gets elegant
val result = nums
  .filter(x => x % 2 == 0)   // keep evens: 2,4,6,8,10
  .map(x => x * 3)            // multiply by 3: 6,12,18,24,30
  .reduce((a, b) => a + b)    // sum: 90

println(result)  // 90
```

Notice how chaining reads like a pipeline â€” **filter â†’ transform â†’ aggregate**. Sound familiar? That's exactly how PySpark works under the hood.

---

## Shorthand Syntax

Scala lets you shorten lambda expressions using `_` as a placeholder:

```scala
val nums = List(1, 2, 3, 4, 5)

nums.map(x => x * 2)   // full form
nums.map(_ * 2)         // shorthand â€” same result

nums.filter(x => x > 3)  // full form
nums.filter(_ > 3)        // shorthand
```

You'll see `_` everywhere in Scala code, so good to recognize it early.

---

## Quick Exercise

What does this return?

```scala
val words = List("spark", "scala", "kafka", "hadoop", "hive")

val result = words
  .filter(w => w.length > 4)
  .map(w => w.toUpperCase)

println(result)
```

---


# Pattern Matching

Think of it as a **supercharged switch statement** â€” but way more powerful than anything you've seen in Python or Java.

---

## Basic Syntax

```scala
val day = "Monday"

val result = day match {
  case "Monday"  => "Start of the work week"
  case "Friday"  => "Almost weekend!"
  case "Sunday"  => "Rest day"
  case _         => "Just another day"  // default â€” like else
}

println(result)  // Start of the work week
```

The `_` is the **wildcard** â€” it catches anything that didn't match above. Always put it last.

---

## Matching on Types

This is where it gets powerful for data engineering â€” handling multiple data types coming through a pipeline:

```scala
def describe(x: Any): String = x match {
  case i: Int     => s"Integer: $i"
  case s: String  => s"String: $s"
  case d: Double  => s"Double: $d"
  case b: Boolean => s"Boolean: $b"
  case _          => "Unknown type"
}

println(describe(42))       // Integer: 42
println(describe("Scala"))  // String: Scala
println(describe(3.14))     // Double: 3.14
println(describe(true))     // Boolean: true
```

Imagine data coming in from Kafka with mixed types â€” this pattern lets you handle each type cleanly in one block.

---

## Matching with Conditions (Guards)

You can add `if` conditions inside your cases:

```scala
def classify(score: Int): String = score match {
  case s if s >= 90 => "A"
  case s if s >= 80 => "B"
  case s if s >= 70 => "C"
  case s if s >= 60 => "D"
  case _            => "F"
}

println(classify(95))  // A
println(classify(72))  // C
println(classify(45))  // F
```

Compare this to Python:
```python
# Python way â€” works but not as elegant
if score >= 90:
    return "A"
elif score >= 80:
    return "B"
# ...and so on
```

---

## Matching on Case Classes (Very Important for Spark)

This is the real power. First, what is a **case class**?

```scala
// A case class is like a data container â€” similar to Python dataclass
case class Transaction(id: String, amount: Double, status: String)

val t1 = Transaction("T001", 5000.00, "approved")
val t2 = Transaction("T002", 15000.00, "flagged")
val t3 = Transaction("T003", 200.00, "declined")
```

Now pattern match on it:

```scala
def processTransaction(t: Transaction): String = t match {
  case Transaction(id, amount, "approved") if amount > 10000 =>
    s"$id: Large approved transaction â€” flag for review"
  case Transaction(id, amount, "approved") =>
    s"$id: Normal approved transaction of $$$amount"
  case Transaction(id, _, "flagged") =>
    s"$id: ALERT â€” flagged transaction!"
  case Transaction(id, _, "declined") =>
    s"$id: Declined â€” notify customer"
  case _ =>
    "Unknown transaction state"
}

println(processTransaction(t1))  // T001: Normal approved transaction of $5000.0
println(processTransaction(t2))  // T002: Large approved transaction â€” flag for review
println(processTransaction(t3))  // T003: Declined â€” notify customer
```

Notice how this reads almost like **business rules** â€” which is exactly how Capital One would use this in a transaction processing pipeline.

---

## Matching on Collections

```scala
def describeList(lst: List[Int]): String = lst match {
  case Nil          => "Empty list"
  case x :: Nil     => s"One element: $x"
  case x :: y :: Nil => s"Two elements: $x and $y"
  case x :: rest    => s"Starts with $x, has ${rest.length} more elements"
}

println(describeList(List()))         // Empty list
println(describeList(List(1)))        // One element: 1
println(describeList(List(1, 2)))     // Two elements: 1 and 2
println(describeList(List(1,2,3,4)))  // Starts with 1, has 3 more elements
```

The `::` operator is called **cons** â€” it splits a list into head and tail. Very common in Scala.

---

## Real World Data Engineering Example

Putting it all together â€” imagine you're processing Capital One transaction records:

```scala
case class Transaction(id: String, amount: Double, 
                       category: String, country: String)

def riskScore(t: Transaction): String = t match {
  case Transaction(_, amount, _, country) 
    if amount > 50000 && country != "US" =>
      "HIGH RISK â€” large foreign transaction"
      
  case Transaction(_, amount, "gambling", _) 
    if amount > 1000 =>
      "MEDIUM RISK â€” large gambling transaction"
      
  case Transaction(_, _, _, country) 
    if country != "US" =>
      "LOW RISK â€” foreign transaction"
      
  case _ => 
    "NORMAL"
}
```

This is clean, readable, and exactly the kind of logic you'd write in a Spark job processing millions of transactions.

---

## Quick Exercise

What does this print?

```scala
val numbers = List(1, 2, 3, 4, 5)

numbers.foreach { n =>
  val label = n match {
    case 1 => "one"
    case 2 => "two"
    case n if n % 2 == 0 => "even"
    case _ => "odd"
  }
  println(s"$n is $label")
}
```

---

You're right, I apologize â€” my previous response was just repeating itself. The exercise answer was already complete. Let's keep moving!

---

# Case Classes and OOP in Scala

## What is a Case Class?

A case class is Scala's most used data structure for modeling data. Think of it like a **Python dataclass or namedtuple** â€” but with superpowers built in.

```scala
// Python dataclass equivalent
# Python
from dataclasses import dataclass

@dataclass
class Transaction:
    id: str
    amount: float
    status: str

# Scala equivalent
case class Transaction(id: String, amount: Double, status: String)
```

One line in Scala gives you a lot for free:
- A constructor
- Getters for all fields
- `toString` method
- `equals` and `hashCode`
- `copy` method
- Pattern matching support

---

## Creating and Using Case Classes

```scala
case class Transaction(id: String, amount: Double, status: String)

// Creating instances â€” no "new" keyword needed
val t1 = Transaction("T001", 5000.00, "approved")
val t2 = Transaction("T002", 15000.00, "flagged")

// Accessing fields
println(t1.id)      // T001
println(t1.amount)  // 5000.0
println(t1.status)  // approved

// toString is automatic
println(t1)  // Transaction(T001,5000.0,approved)
```

---

## The copy() Method

This is huge in data engineering. Since case classes are immutable, `copy()` lets you create a modified version without changing the original:

```scala
val t1 = Transaction("T001", 5000.00, "pending")

// Create new transaction with only status changed
val t2 = t1.copy(status = "approved")

println(t1)  // Transaction(T001,5000.0,pending)   â€” unchanged
println(t2)  // Transaction(T001,5000.0,approved)  â€” new copy
```

This immutability pattern is exactly how Spark handles data â€” never modify, always create new.

---

## Regular Classes vs Case Classes

Scala also has regular classes for when you need behavior and logic, not just data:

```scala
// Regular class â€” for behavior and logic
class TransactionProcessor(threshold: Double) {
  
  def isHighRisk(t: Transaction): Boolean = {
    t.amount > threshold
  }
  
  def process(t: Transaction): String = {
    if (isHighRisk(t)) s"${t.id} flagged for review"
    else s"${t.id} processed normally"
  }
}

// Using it
val processor = new TransactionProcessor(10000.00)  // note: needs "new"
val t1 = Transaction("T001", 15000.00, "approved")

println(processor.process(t1))  // T001 flagged for review
```

The rule of thumb is simple â€” **case class for data, regular class for behavior**.

---

## Traits â€” Scala's Version of Interfaces

Traits are like Java interfaces but more powerful. They define shared behavior across multiple classes:

```scala
// Define the trait
trait Auditable {
  def auditLog(): String
}

trait Serializable {
  def toJson(): String
}

// Case class implementing traits
case class Transaction(id: String, amount: Double, status: String) 
  extends Auditable with Serializable {
  
  def auditLog(): String = 
    s"Transaction $id for $$$amount was $status"
  
  def toJson(): String = 
    s"""{"id":"$id","amount":$amount,"status":"$status"}"""
}

val t1 = Transaction("T001", 5000.00, "approved")
println(t1.auditLog())  // Transaction T001 for $5000.0 was approved
println(t1.toJson())    // {"id":"T001","amount":5000.0,"status":"approved"}
```

---

## Putting It All Together â€” A Mini Pipeline

Here's a realistic example combining everything we've covered so far:

```scala
// Data model
case class Transaction(
  id: String, 
  amount: Double, 
  category: String, 
  country: String
)

// Processor class with business logic
class RiskEngine(highAmountThreshold: Double) {

  def classify(t: Transaction): String = t match {
    case Transaction(_, amount, _, country) 
      if amount > highAmountThreshold && country != "US" =>
        "HIGH RISK"
    case Transaction(_, _, "gambling", _) =>
        "MEDIUM RISK"
    case Transaction(_, _, _, country) if country != "US" =>
        "LOW RISK"
    case _ => 
        "NORMAL"
  }

  def processAll(transactions: List[Transaction]): List[String] = {
    transactions
      .map(t => s"${t.id}: ${classify(t)}")
  }
}

// Run it
val transactions = List(
  Transaction("T001", 75000.00, "wire",     "UK"),
  Transaction("T002", 500.00,   "gambling", "US"),
  Transaction("T003", 1200.00,  "retail",   "CA"),
  Transaction("T004", 45.00,    "grocery",  "US")
)

val engine = new RiskEngine(50000.00)
engine.processAll(transactions).foreach(println)
```

Output:
```
T001: HIGH RISK
T002: MEDIUM RISK
T003: LOW RISK
T004: NORMAL
```

This is essentially a simplified version of what Capital One's fraud detection pipelines do at scale â€” just with Spark distributing it across billions of records.

---

## Quick Exercise

Given this code, what is the output?

```scala
case class Employee(name: String, role: String, salary: Double)

val employees = List(
  Employee("Alice", "Engineer",  95000.00),
  Employee("Bob",   "Manager",   120000.00),
  Employee("Carol", "Engineer",  98000.00),
  Employee("Dave",  "Analyst",   75000.00)
)

val result = employees
  .filter(e => e.role == "Engineer")
  .map(e => e.copy(salary = e.salary * 1.10))
  .map(e => f"${e.name}: $$${e.salary}%.2f")

result.foreach(println)
```

---


# Option, Try, and Either

These three are Scala's answer to one of the biggest problems in data pipelines â€” **nulls, missing data, and errors crashing your job at 2am.** ðŸ˜…

---

## The Problem First

In Python or Java you've probably seen this:

```python
# Python â€” this crashes if key doesn't exist
value = my_dict["some_key"]  # KeyError!

# Or worse â€” silent null passing through your pipeline
value = None
result = value.strip()  # AttributeError!
```

Scala says â€” let's make the possibility of missing data **explicit in the type system** so you're forced to handle it.

---

## Option â€” Maybe a Value, Maybe Nothing

`Option` has exactly two states:
- `Some(value)` â€” there is a value
- `None` â€” there is no value

```scala
// Instead of returning null, return Option
def findEmployee(id: String, db: Map[String, String]): Option[String] = {
  db.get(id)  // returns Some(value) or None automatically
}

val database = Map(
  "E001" -> "Alice",
  "E002" -> "Bob",
  "E003" -> "Carol"
)

val result1 = findEmployee("E001", database)  // Some(Alice)
val result2 = findEmployee("E999", database)  // None
```

Now you're **forced** to handle both cases:

```scala
// Pattern matching on Option
result1 match {
  case Some(name) => println(s"Found employee: $name")
  case None       => println("Employee not found")
}
// Found employee: Alice

result2 match {
  case Some(name) => println(s"Found employee: $name")
  case None       => println("Employee not found")
}
// Employee not found
```

---

## Option Methods â€” Very Useful in Pipelines

```scala
val salary: Option[Double] = Some(95000.00)
val noSalary: Option[Double] = None

// getOrElse â€” provide a default if None
println(salary.getOrElse(0.0))    // 95000.0
println(noSalary.getOrElse(0.0))  // 0.0

// map â€” transform if Some, skip if None
val bonus = salary.map(s => s * 0.10)
println(bonus)  // Some(9500.0)

val noBonus = noSalary.map(s => s * 0.10)
println(noBonus)  // None â€” no crash!

// isDefined / isEmpty
println(salary.isDefined)   // true
println(noSalary.isEmpty)   // true
```

This is huge in Spark â€” when reading CSVs or JSONs with missing fields, Option prevents your entire job from crashing on one bad record.

---

## Try â€” Success or Failure

`Option` handles missing values. `Try` handles **operations that might throw exceptions**:

- `Success(value)` â€” operation worked
- `Failure(exception)` â€” operation failed

```scala
import scala.util.{Try, Success, Failure}

def parseAmount(s: String): Try[Double] = {
  Try(s.toDouble)  // wraps the risky operation
}

val good = parseAmount("5000.00")   // Success(5000.0)
val bad  = parseAmount("abc")       // Failure(NumberFormatException)

// Pattern match on the result
good match {
  case Success(amount) => println(s"Valid amount: $amount")
  case Failure(ex)     => println(s"Parse error: ${ex.getMessage}")
}
// Valid amount: 5000.0

bad match {
  case Success(amount) => println(s"Valid amount: $amount")
  case Failure(ex)     => println(s"Parse error: ${ex.getMessage}")
}
// Parse error: For input string: "abc"
```

---

## Try in a Real Pipeline

```scala
import scala.util.{Try, Success, Failure}

case class RawRecord(id: String, amountStr: String, country: String)
case class Transaction(id: String, amount: Double, country: String)

def parseRecord(raw: RawRecord): Try[Transaction] = Try {
  Transaction(
    id      = raw.id,
    amount  = raw.amountStr.toDouble,  // risky â€” might fail
    country = raw.country
  )
}

val rawRecords = List(
  RawRecord("T001", "5000.00", "US"),
  RawRecord("T002", "abc",     "UK"),  // bad data
  RawRecord("T003", "1200.00", "CA"),
  RawRecord("T004", "??",      "US")   // bad data
)

val results = rawRecords.map(r => parseRecord(r))

// Separate good records from bad
val good = results.collect { case Success(t) => t }
val bad  = results.collect { case Failure(e) => e.getMessage }

println("Good records:")
good.foreach(println)

println("\nBad records:")
bad.foreach(println)
```

Output:
```
Good records:
Transaction(T001,5000.0,US)
Transaction(T003,1200.0,CA)

Bad records:
For input string: "abc"
For input string: "??"
```

This pattern â€” parse, separate good from bad, process good, log bad â€” is the backbone of every production data pipeline.

---

## Either â€” Success or a Meaningful Error

`Try` gives you the exception. `Either` lets you return a **custom error message** which is more useful in business logic:

- `Right(value)` â€” success (think "right" = correct)
- `Left(error)`  â€” failure with your own error description

```scala
def validateTransaction(id: String, amount: Double): Either[String, Transaction] = {
  if (amount <= 0)
    Left(s"$id: Amount must be positive, got $amount")
  else if (amount > 1000000)
    Left(s"$id: Amount exceeds maximum limit: $amount")
  else
    Right(Transaction(id, amount, "US"))
}

val r1 = validateTransaction("T001", 5000.00)
val r2 = validateTransaction("T002", -100.00)
val r3 = validateTransaction("T003", 2000000.00)

List(r1, r2, r3).foreach {
  case Right(t)    => println(s"Valid: ${t.id} â€” $${t.amount}")
  case Left(error) => println(s"Invalid: $error")
}
```

Output:
```
Valid: T001 â€” $5000.0
Invalid: T002: Amount must be positive, got -100.0
Invalid: T003: Amount exceeds maximum limit: 2000000.0
```

---

## When to Use Which

| Situation | Use |
|---|---|
| Value might be missing | `Option` |
| Operation might throw exception | `Try` |
| Need custom error messages | `Either` |

---

## Quick Exercise

What does this print?

```scala
import scala.util.{Try, Success, Failure}

val inputs = List("100.0", "200.0", "oops", "400.0", "??")

val results = inputs
  .map(s => Try(s.toDouble))
  .collect { case Success(v) => v }
  .filter(v => v > 150.0)
  .map(v => v * 2)

println(results)
```

---

# Scala with Spark

This is where your existing PySpark knowledge becomes a superpower. The concepts are identical â€” you're just writing Scala syntax. Let's make that connection explicit at every step.

---

## Setting Up a Spark Session

The entry point to everything in Spark:

```scala
import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder()
  .appName("CapitalOne Transaction Pipeline")
  .master("local[*]")  // use all available cores locally
  .getOrCreate()

// Always import implicits â€” enables DataFrame magic
import spark.implicits._
```

Compare to PySpark:
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("CapitalOne Transaction Pipeline") \
    .master("local[*]") \
    .getOrCreate()
```

Nearly identical â€” just Scala syntax.

---

## Creating DataFrames

```scala
// Method 1 â€” from a case class (most common in Scala)
case class Transaction(
  id:       String,
  amount:   Double,
  category: String,
  country:  String,
  status:   String
)

val transactions = Seq(
  Transaction("T001", 5000.00,  "retail",   "US", "approved"),
  Transaction("T002", 75000.00, "wire",      "UK", "approved"),
  Transaction("T003", 200.00,   "gambling",  "US", "approved"),
  Transaction("T004", 1200.00,  "retail",    "CA", "declined"),
  Transaction("T005", 450.00,   "grocery",   "US", "approved")
).toDF()  // converts to DataFrame using implicits

transactions.show()
```

Output:
```
+----+--------+--------+-------+--------+
|  id|  amount|category|country|  status|
+----+--------+--------+-------+--------+
|T001|  5000.0|  retail|     US|approved|
|T002| 75000.0|    wire|     UK|approved|
|T003|   200.0|gambling|     US|approved|
|T004|  1200.0|  retail|     CA|declined|
|T005|   450.0| grocery|     US|approved|
+----+--------+--------+-------+--------+
```

---

## Basic DataFrame Operations

Everything you know from PySpark works the same way:

```scala
import org.apache.spark.sql.functions._

// select
transactions.select("id", "amount", "country").show()

// filter
transactions.filter(col("amount") > 1000).show()

// filter with AND condition
transactions
  .filter(col("amount") > 1000 && col("country") === "US")
  .show()

// withColumn â€” add or modify a column
transactions
  .withColumn("amountInGBP", col("amount") * 0.79)
  .show()

// groupBy and aggregate
transactions
  .groupBy("country")
  .agg(
    count("id").as("total_transactions"),
    sum("amount").as("total_amount"),
    avg("amount").as("avg_amount")
  )
  .show()
```

Output of groupBy:
```
+-------+--------------------+------------+----------+
|country|total_transactions  |total_amount|avg_amount|
+-------+--------------------+------------+----------+
|     US|                   3|    5650.0  |  1883.33 |
|     UK|                   1|   75000.0  | 75000.0  |
|     CA|                   1|    1200.0  |  1200.0  |
+-------+--------------------+------------+----------+
```

---

## Datasets â€” Scala's Secret Weapon over PySpark

This is where Scala Spark pulls ahead of PySpark. A **Dataset** is a typed DataFrame â€” you get compile-time type safety:

```scala
// DataFrame â€” untyped, like PySpark
val df = transactions  // DataFrame[Row]

// Dataset â€” typed, Scala only
val ds = transactions.as[Transaction]  // Dataset[Transaction]

// Now you can use case class fields directly
ds.filter(t => t.amount > 1000 && t.country == "US")
  .map(t => t.copy(status = "flagged"))
  .show()
```

The difference in production:

```scala
// DataFrame â€” error shows at RUNTIME (bad)
df.filter(col("amountt") > 1000)  // typo â€” crashes when job runs

// Dataset â€” error shows at COMPILE TIME (good)
ds.filter(t => t.amountt > 1000)  // typo â€” caught before job even runs!
```

At Capital One scale â€” billions of records â€” catching errors at compile time instead of runtime saves hours of debugging.

---

## Reading and Writing Data

```scala
// Reading CSV
val df = spark.read
  .option("header", "true")
  .option("inferSchema", "true")
  .csv("s3://capitalone-bucket/transactions/")

// Reading JSON
val df = spark.read
  .json("s3://capitalone-bucket/transactions/")

// Reading Parquet â€” most common in production
val df = spark.read
  .parquet("s3://capitalone-bucket/transactions/")

// Writing to Parquet with partitioning
df.write
  .mode("overwrite")
  .partitionBy("country", "status")
  .parquet("s3://capitalone-bucket/output/")
```

Compare to PySpark:
```python
# PySpark â€” identical concept
df = spark.read \
    .option("header", "true") \
    .csv("s3://capitalone-bucket/transactions/")

df.write \
    .mode("overwrite") \
    .partitionBy("country", "status") \
    .parquet("s3://capitalone-bucket/output/")
```

Exactly the same â€” just Scala syntax.

---

## User Defined Functions (UDFs)

When built-in Spark functions aren't enough, you write your own:

```scala
import org.apache.spark.sql.functions.udf

// Define the function
val riskScoreUDF = udf((amount: Double, country: String) => {
  (amount, country) match {
    case (a, c) if a > 50000 && c != "US" => "HIGH"
    case (a, _) if a > 10000              => "MEDIUM"
    case _                                 => "LOW"
  }
})

// Register and use it
val result = transactions
  .withColumn("risk_score", riskScoreUDF(col("amount"), col("country")))

result.show()
```

Output:
```
+----+--------+--------+-------+--------+----------+
|  id|  amount|category|country|  status|risk_score|
+----+--------+--------+-------+--------+----------+
|T001|  5000.0|  retail|     US|approved|       LOW|
|T002| 75000.0|    wire|     UK|approved|      HIGH|
|T003|   200.0|gambling|     US|approved|       LOW|
|T004|  1200.0|  retail|     CA|declined|       LOW|
|T005|   450.0| grocery|     US|approved|       LOW|
+----+--------+--------+-------+--------+----------+
```

---

## Joins â€” Critical for Data Engineering

```scala
case class Customer(id: String, name: String, tier: String)

val customers = Seq(
  Customer("C001", "Alice Johnson", "Premium"),
  Customer("C002", "Bob Smith",     "Standard"),
  Customer("C003", "Carol White",   "Premium")
).toDF()

case class Transaction2(id: String, customerId: String, amount: Double)

val transactions2 = Seq(
  Transaction2("T001", "C001", 5000.00),
  Transaction2("T002", "C001", 3000.00),
  Transaction2("T003", "C002", 1500.00),
  Transaction2("T004", "C999", 750.00)   // no matching customer
).toDF()

// Inner join
val inner = transactions2
  .join(customers, transactions2("customerId") === customers("id"), "inner")
  .select("id", "customerId", "amount", "name", "tier")

inner.show()

// Left join â€” keeps all transactions even without customer match
val left = transactions2
  .join(customers, transactions2("customerId") === customers("id"), "left")
  .select("id", "customerId", "amount", "name", "tier")

left.show()
```

Inner join output:
```
+----+----------+------+-------------+--------+
|  id|customerId|amount|         name|    tier|
+----+----------+------+-------------+--------+
|T001|      C001|5000.0|Alice Johnson| Premium|
|T002|      C001|3000.0|Alice Johnson| Premium|
|T003|      C002|1500.0|    Bob Smith|Standard|
+----+----------+------+-------------+--------+
```

Left join output â€” T004 kept with nulls:
```
+----+----------+------+-------------+--------+
|  id|customerId|amount|         name|    tier|
+----+----------+------+-------------+--------+
|T001|      C001|5000.0|Alice Johnson| Premium|
|T002|      C001|3000.0|Alice Johnson| Premium|
|T003|      C002|1500.0|    Bob Smith|Standard|
|T004|      C999| 750.0|         null|    null|
+----+----------+------+-------------+--------+
```

---

## A Complete Pipeline â€” Putting It All Together

This is what a real Scala Spark job looks like at Capital One:

```scala
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import scala.util.{Try, Success, Failure}

object TransactionPipeline {

  case class Transaction(
    id:         String,
    customerId: String,
    amount:     Double,
    category:   String,
    country:    String,
    status:     String
  )

  def main(args: Array[String]): Unit = {

    val spark = SparkSession.builder()
      .appName("Transaction Risk Pipeline")
      .getOrCreate()

    import spark.implicits._

    // 1. Read raw data
    val rawDf = spark.read
      .option("header", "true")
      .option("inferSchema", "true")
      .csv("s3://capitalone/raw/transactions/")

    // 2. Cast to typed Dataset
    val transactions = rawDf.as[Transaction]

    // 3. Define risk scoring UDF
    val riskUDF = udf((amount: Double, country: String, category: String) => {
      (amount, country, category) match {
        case (a, c, _) if a > 50000 && c != "US" => "HIGH"
        case (_, _, cat) if cat == "gambling"     => "MEDIUM"
        case (a, _, _)  if a > 10000              => "MEDIUM"
        case _                                     => "LOW"
      }
    })

    // 4. Transform
    val enriched = transactions
      .filter(col("status") === "approved")
      .withColumn("risk_score", riskUDF(col("amount"), col("country"), col("category")))
      .withColumn("processed_date", current_date())

    // 5. Aggregate for reporting
    val summary = enriched
      .groupBy("country", "risk_score")
      .agg(
        count("id").as("transaction_count"),
        sum("amount").as("total_amount"),
        avg("amount").as("avg_amount")
      )
      .orderBy(desc("total_amount"))

    // 6. Write output
    summary.write
      .mode("overwrite")
      .partitionBy("country")
      .parquet("s3://capitalone/processed/risk_summary/")

    spark.stop()
  }
}
```

This is a production-grade Spark job. Notice how everything we learned feeds into it â€” case classes, pattern matching, Option/Try concepts, higher-order functions, all working together.

---

## Quick Exercise

Given this code what does the output look like?

```scala
case class Sale(region: String, product: String, amount: Double)

val sales = Seq(
  Sale("North", "Laptop",  1200.00),
  Sale("South", "Phone",    800.00),
  Sale("North", "Tablet",   600.00),
  Sale("South", "Laptop",  1100.00),
  Sale("North", "Phone",    750.00)
).toDF()

val result = sales
  .groupBy("region")
  .agg(
    count("product").as("total_sales"),
    sum("amount").as("total_revenue")
  )
  .withColumn("avg_sale", col("total_revenue") / col("total_sales"))
  .orderBy(desc("total_revenue"))

result.show()
```
---

Perfect! ðŸŽ¯ 100% correct!

```
North: 1200 + 600 + 750 = 2550.0  âœ…
South: 800 + 1100        = 1900.0  âœ…

North avg: 2550 / 3 = 850.0  âœ…
South avg: 1900 / 2 = 950.0  âœ…

Ordered by total_revenue descending â€” North first âœ…
```

You nailed every detail including the column order, formatting, and sort order. You're thinking in Spark now. ðŸ’ª

---

# SBT â€” Scala Build Tool

Think of SBT as Scala's version of **Maven** in Java or **pip + setup.py** in Python. It manages your dependencies, compiles your code, and runs your jobs.

---

## Project Structure

A standard Scala Spark project looks like this:

```
my-spark-project/
â”‚
â”œâ”€â”€ build.sbt                  â† project definition and dependencies
â”œâ”€â”€ project/
â”‚   â””â”€â”€ build.properties       â† SBT version
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main/
â”‚   â”‚   â””â”€â”€ scala/
â”‚   â”‚       â””â”€â”€ com/capitalone/
â”‚   â”‚           â”œâ”€â”€ TransactionPipeline.scala
â”‚   â”‚           â”œâ”€â”€ models/
â”‚   â”‚           â”‚   â””â”€â”€ Transaction.scala
â”‚   â”‚           â””â”€â”€ utils/
â”‚   â”‚               â””â”€â”€ RiskEngine.scala
â”‚   â”‚
â”‚   â””â”€â”€ test/
â”‚       â””â”€â”€ scala/
â”‚           â””â”€â”€ com/capitalone/
â”‚               â””â”€â”€ TransactionPipelineTest.scala
â”‚
â””â”€â”€ data/
    â””â”€â”€ sample_transactions.csv
```

This structure will look familiar â€” it's the same convention as any Java/Maven project.

---

## build.sbt â€” The Heart of Your Project

```scala
// Project metadata
name         := "capitalone-transaction-pipeline"
version      := "1.0.0"
scalaVersion := "2.12.15"  // Spark 3.x requires Scala 2.12

// Dependencies
libraryDependencies ++= Seq(

  // Spark core
  "org.apache.spark" %% "spark-core" % "3.3.0" % "provided",
  "org.apache.spark" %% "spark-sql"  % "3.3.0" % "provided",

  // Testing
  "org.scalatest"    %% "scalatest"  % "3.2.9" % "test"
)

// Build a fat JAR with all dependencies included
assembly / assemblyMergeStrategy := {
  case PathList("META-INF", _*) => MergeStrategy.discard
  case _                        => MergeStrategy.first
}
```

The `%%` means SBT automatically appends the Scala version to the dependency name. The `"provided"` scope means Spark is available on the cluster â€” don't bundle it in your JAR.

---

## Common SBT Commands

```bash
# Compile your code
sbt compile

# Run tests
sbt test

# Run your main class
sbt run

# Package into a JAR for deployment
sbt package

# Build a fat JAR (includes all dependencies)
sbt assembly

# Start interactive Scala console
sbt console

# Clean build artifacts
sbt clean
```

At Capital One you'd mostly use:
```bash
sbt clean compile test assembly
```
That one command cleans, compiles, tests, and packages â€” ready for deployment to EMR or Databricks.

---

## Organizing Your Code â€” Multiple Files

**models/Transaction.scala**
```scala
package com.capitalone.models

case class Transaction(
  id:         String,
  customerId: String,
  amount:     Double,
  category:   String,
  country:    String,
  status:     String
)

case class Customer(
  id:   String,
  name: String,
  tier: String
)
```

**utils/RiskEngine.scala**
```scala
package com.capitalone.utils

import com.capitalone.models.Transaction
import org.apache.spark.sql.functions.udf

object RiskEngine {

  // Reusable risk scoring UDF
  val riskScoreUDF = udf((amount: Double, country: String, category: String) => {
    (amount, country, category) match {
      case (a, c, _) if a > 50000 && c != "US" => "HIGH"
      case (_, _, cat) if cat == "gambling"     => "MEDIUM"
      case (a, _, _)  if a > 10000              => "MEDIUM"
      case _                                     => "LOW"
    }
  })

  // Pure Scala function â€” no Spark needed
  def classifyTransaction(t: Transaction): String = t match {
    case Transaction(_, _, a, _, c, _) if a > 50000 && c != "US" => "HIGH"
    case Transaction(_, _, _, "gambling", _, _)                   => "MEDIUM"
    case Transaction(_, _, a, _, _, _) if a > 10000               => "MEDIUM"
    case _                                                         => "LOW"
  }
}
```

**TransactionPipeline.scala**
```scala
package com.capitalone

import com.capitalone.models.Transaction
import com.capitalone.utils.RiskEngine
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

object TransactionPipeline {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession.builder()
      .appName("Transaction Risk Pipeline")
      .getOrCreate()

    import spark.implicits._

    // Read
    val transactions = spark.read
      .option("header", "true")
      .option("inferSchema", "true")
      .csv(args(0))          // path passed as argument
      .as[Transaction]       // typed Dataset

    // Transform
    val result = transactions
      .filter(col("status") === "approved")
      .withColumn("risk_score",
        RiskEngine.riskScoreUDF(
          col("amount"),
          col("country"),
          col("category")
        )
      )

    // Write
    result.write
      .mode("overwrite")
      .partitionBy("country", "risk_score")
      .parquet(args(1))      // output path passed as argument

    spark.stop()
  }
}
```

Notice how clean this is â€” each file has one responsibility. Models define data, utils contain logic, pipeline orchestrates everything.

---

## Submitting to a Cluster

Once you build your JAR with `sbt assembly`, you submit it to a cluster like this:

```bash
# Submit to EMR or any Spark cluster
spark-submit \
  --class com.capitalone.TransactionPipeline \
  --master yarn \
  --deploy-mode cluster \
  --executor-memory 4G \
  --num-executors 10 \
  target/scala-2.12/capitalone-transaction-pipeline-1.0.0.jar \
  s3://capitalone/raw/transactions/ \
  s3://capitalone/processed/output/
```

The `--class` flag tells Spark which `object` with a `main` method to run. The last two arguments are `args(0)` and `args(1)` from inside your code.

---

## Testing with ScalaTest

Production code needs tests. Here's how you test a Spark job:

```scala
package com.capitalone

import com.capitalone.models.Transaction
import com.capitalone.utils.RiskEngine
import org.apache.spark.sql.SparkSession
import org.scalatest.funsuite.AnyFunSuite
import org.scalatest.BeforeAndAfterAll

class TransactionPipelineTest extends AnyFunSuite with BeforeAndAfterAll {

  // Create a local Spark session just for testing
  val spark: SparkSession = SparkSession.builder()
    .appName("TestSuite")
    .master("local[*]")
    .getOrCreate()

  import spark.implicits._

  test("RiskEngine classifies HIGH risk correctly") {
    val t = Transaction("T001", "C001", 75000.00, "wire", "UK", "approved")
    assert(RiskEngine.classifyTransaction(t) == "HIGH")
  }

  test("RiskEngine classifies gambling as MEDIUM risk") {
    val t = Transaction("T002", "C002", 500.00, "gambling", "US", "approved")
    assert(RiskEngine.classifyTransaction(t) == "MEDIUM")
  }

  test("Pipeline filters declined transactions") {
    val transactions = Seq(
      Transaction("T001", "C001", 5000.00, "retail", "US", "approved"),
      Transaction("T002", "C002", 3000.00, "retail", "US", "declined"),
      Transaction("T003", "C003", 1000.00, "retail", "US", "approved")
    ).toDF()

    val approved = transactions
      .filter($"status" === "approved")

    assert(approved.count() == 2)
  }

  override def afterAll(): Unit = {
    spark.stop()
  }
}
```

Run with:
```bash
sbt test
```

---

You've now covered the full Scala + Spark stack:

- âœ… Scala fundamentals
- âœ… Pattern matching and case classes
- âœ… Option, Try, Either
- âœ… Spark DataFrames and Datasets
- âœ… UDFs
- âœ… Joins
- âœ… Complete pipeline structure
- âœ… SBT project organization
- âœ… Testing

---

