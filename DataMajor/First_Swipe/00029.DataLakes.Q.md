# Mock Interview â€” Data Lakes (S3 / Delta Lake)

Same rules â€” answer in the room, model answer and diagrams after each response.

---

## Question 1 â€” Warming Up (The "Database vs Lake" Check)

**"I see you've used S3 a lot. Why don't we just store everything in a Postgres database? Why do we need a Data Lake at all, given that databases are faster for queries?"**

# Feedback First

## What You Nailed âœ…
*   **Scale:** Correctly identified that RDBMS (Vertical Scaling) hits a ceiling at 10-20TB, while S3 handles Petabytes easily.
*   **Cost:** Correctly mentioned that S3 storage is pennies per GB compared to expensive SSDs for databases.
*   **Data Types:** Mentioned that databases struggle with videos, images, and unstructured logs.

## What to Tighten Up ğŸ”§
*   *Schema Flexibility:* This is the #1 reason. "Schema-on-Read." We don't know what the data looks like yet, so we can't create a `CREATE TABLE` statement. We trap it first, analyze later.
*   *Decoupling:* A database couples Storage and Compute. A Data Lake decouples them. I can have 1PetaByte of data and 0 servers running (Zero Cost for compute).

# Model Answer

---

"It's about **Flexibility and Cost**.

**1. Schema-on-Read vs Write:**
To put data in Postgres, I need to know the exact schema today. If the upstream app adds a column, my ETL breaks. With a Data Lake (S3), I just land the raw file. I figure out the schema later when I read it. It prevents data loss from schema mismatches.

**2. Decoupling Compute and Storage:**
In a database, if I fill up the disk, I have to buy a bigger server (with more CPU/RAM I don't need).
In a Data Lake, I can store 10 Petabytes for cheap, and only spin up Compute (Spark) when I actually run a query.

**3. Unstructured Data:**
I can't store pdfs, images, or raw JSON blobs efficiently in a relational DB. S3 handles any object type natively."

# Diagrams

```text
COUPLING vs DECOUPLING
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  DATABASE (Coupled)              DATA LAKE (Decoupled)                â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â•‘
â•‘  â”‚ SERVER       â”‚                â”‚ STORAGE (S3) â”‚                     â•‘
â•‘  â”‚ [CPU] [RAM]  â”‚                â”‚ [PB of Data] â”‚                     â•‘
â•‘  â”‚ [DISK]       â”‚                â””â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”˜                     â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚                             â•‘
â•‘                                         â”‚ (Network)                   â•‘
â•‘  "To add 10TB of disk,                  â–¼                             â•‘
â•‘   I must buy more CPU."          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â•‘
â•‘                                  â”‚ COMPUTE      â”‚                     â•‘
â•‘                                  â”‚ (EC2/Spark)  â”‚                     â•‘
â•‘                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â•‘
â•‘                                  "I can turn this off to save money"  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## Question 2 â€” Data Engineer Level (The "Small File" Problem)

**"We are streaming data into our Data Lake (S3) using Kinesis. It creates a new file every second. Now our Spark jobs are running extremely slow. What is happening and how do you fix it?"**

# Feedback First

## What You Nailed âœ…
*   **The Problem:** Correctly identified "Small File Problem". Too many tiny files choke the system.
*   **Metadata Overhead:** Mentioned that S3/HDFS spends more time listing files than reading data.
*   **Compaction:** Calculated that reading 10,000 files of 1KB is 100x slower than reading 1 file of 10MB.

## What to Tighten Up ğŸ”§
*   *The Fix (Compaction):* Be specific. "I would run a nightly compaction job."
    *   `df.repartition(1).write.mode("overwrite")`
*   *Streaming Architecture:* Suggest increasing the **Buffer Interval** on the stream. "Instead of writing every 1 second, buffer for 15 minutes or 128MB."
*   *Partitioning:* Maybe you are over-partitioning? If you partition by `second`, you create millions of folders. Partition by `hour` or `day`.

# Model Answer

---

"This is the **Small File Problem**.
S3 and Spark are optimized for large, throughput-heavy reads (128MB+ blocks). When you have millions of 1KB files:
1.  **Metadata Overhead:** The driver spends minutes just listing the objects in S3.
2.  **Connection Overhead:** Opening a HTTP connection to S3 takes 50ms. Reading 1KB takes 1ms. You are spending 98% of your time opening connections.

**The Fix:**
1.  **Compaction Job:** I would run a periodic Spark job (e.g., hourly) that reads the tiny files, coalesces them (`df.coalesce(1)`), and writes them back as larger Parquet files (Target size: 128MB - 1GB).
2.  **Adjust Ingestion:** I would tune the Kinesis Firehose buffer settings to wait for at least 60 seconds or 100MB before flushing to S3."

# Diagrams

```text
THE SMALL FILE DEATH SPIRAL
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  SCENARIO A: 1,000 files (1KB each)                                   â•‘
â•‘  [Open][Read][Close]  [Open][Read][Close]  [Open][Read][Close] ...    â•‘
â•‘  Time: 50ms + 1ms + 5ms = 56ms per file                               â•‘
â•‘  Total: 56,000ms (56 seconds)                                         â•‘
â•‘                                                                       â•‘
â•‘  SCENARIO B: 1 file (1MB)                                             â•‘
â•‘  [Open] [Read .....................................] [Close]          â•‘
â•‘  Time: 50ms + 20ms + 5ms = 75ms                                       â•‘
â•‘  Total: 0.075 seconds                                                 â•‘
â•‘                                                                       â•‘
â•‘  Speedup: ~750x faster                                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## Question 3 â€” Lead Level (Delta Lake / ACID)

**"S3 is eventually consistent and immutable. If I have a GDPR request to delete a user's data from a 1TB Data Lake, how do I do it without rewriting the entire Petabyte? Explain Delta Lake."**

# Feedback First

## What You Nailed âœ…
*   **Immutability:** Correctly stated that you cannot edit a file in S3. You must rewrite it.
*   **The Pain:** Identified that finding one row in a PB of data is like finding a needle in a haystack.
*   **Delta Lake/Hudi:** Mentioned that these tools add a transaction layer.

## What to Tighten Up ğŸ”§
*   *The Logs:* Delta Lake works by maintaining a **Transaction Log** (`_delta_log`).
*   *Copy-on-Write:* Explain the mechanism. When you DELETE a row, Delta finds the specific Parquet file containing that row, rewrites *just that file* without the row, and marks the old file as "stale" in the log.
*   *Vacuum:* Mention that you eventually need to run `VACUUM` to physically delete the old files to reclaim space and comply with the "Right to be Forgotten."

# Model Answer

---

"Native S3 doesn't support row-level updates. To delete one user, I would have to read the entire dataset, filter out that user, and rewrite perfectly good data. This is cost-prohibitive.

I would implement **Delta Lake** (or format=delta).
Delta adds a **Transaction Log** on top of S3.

**The DELETE Process:**
1.  **Metadata Lookup:** Delta checks the log to find which specific parquet files contain `User_ID = 123`. (Z-Ordering/Skipping helps here).
2.  **Copy-on-Write:** It reads those few files, filters out the user, and writes new version files.
3.  **Atomic Swap:** It updates the `_delta_log` to point to the new files and ignore the old ones.
4.  **Vacuum:** To strictly comply with GDPR, I run `VACUUM RETENTION 0 HOURS` to physically remove the old underlying files from S3."

# Diagrams

```text
DELTA LAKE "DELETE"
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Step 1: THE DATA FILES (Parquet)                                     â•‘
â•‘  [File A (User 1, 2)]   [File B (User 3, 4)]   [File C (User 5)]      â•‘
â•‘                                                                       â•‘
â•‘  Step 2: DELETE WHERE ID = 3                                          â•‘
â•‘  Delta Log says: "User 3 is in File B."                               â•‘
â•‘                                                                       â•‘
â•‘  Step 3: REWRITE ONLY FILE B                                          â•‘
â•‘  Reads File B -> Removes User 3 -> Writes [File B_v2 (User 4)]        â•‘
â•‘                                                                       â•‘
â•‘  Step 4: UPDATE LOG                                                   â•‘
â•‘  Log: "File A is valid. File C is valid. File B_v2 is valid."         â•‘
â•‘       "File B is INVALID (Tombstoned)."                               â•‘
â•‘                                                                       â•‘
â•‘  Result: You only rewrote 33% of data, not 100%.                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```
