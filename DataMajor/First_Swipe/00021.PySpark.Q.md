# Mock Interview â€” PySpark & Spark Internals

Same rules â€” answer in the room, model answer and diagrams after each response.

---

## Question 1 â€” Warming Up (The "Why Spark" Check)

**"I see you've used both Pandas and PySpark. When would you choose one over the other? At what point does Pandas break, and how does Spark solve that problem architecture-wise?"**

# Feedback First

## What You Nailed âœ…
*   **Memory Bound vs CPU Bound:** Correctly identified that Pandas is limited by the single machine's RAM. "If it doesn't fit in memory, Pandas crashes."
*   **Horizontal Scaling:** Correctly explained that Spark distributes data across multiple nodes (horizontal scaling) while Pandas relies on a single node (vertical scaling).
*   **Lazy Evaluation:** Mentioned that Spark doesn't do anything until an action is called, allowing for optimization.

## What to Tighten Up ğŸ”§
*   *The "Small Data" Penalty:* You should mention that for small datasets (e.g., < 1GB), Pandas is actually **faster** than Spark. Spark has overhead (JVM startup, DAG construction, task scheduling). Don't use a semi-truck to buy groceries.
*   *Single Threaded vs Multi-Threaded:* Pandas typically uses only one CPU core (due to Python GIL). Spark (being JVM-based) uses all available cores on all machines.

# Model Answer

---

"The decision comes down to **Data Volume vs. Overhead**.

**Pandas (Small Data / < 10GB):**
I use Pandas for anything that fits comfortably in a single machine's RAM. It's faster for small data because there's zero network overhead and no cluster startup time. It's eager executionâ€”immediate feedback.

**PySpark (Big Data / > 10GB):**
Pandas fails when data exceeds memory (MemoryError) or when single-threaded processing takes too long.
Spark solves this by:
1.  **Horizontal Scaling:** It partitions the dataset across 50 nodes. 10GB becomes 200MB per node.
2.  **Parallelism:** It processes those partitions simultaneously.
3.  **Lazy Evaluation:** It optimizes the query plan (Catalyst) before running, unlike Pandas which executes line-by-line blindly.

**My Rule of Thumb:** If I can open it on my laptop, I use Pandas. If I need a cluster, I use Spark."

# Diagrams

```text
PANDAS VS SPARK EXECUTION
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  PANDAS (Single Node)             SPARK (Distributed Cluster)         â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â” â•‘
â•‘  â”‚ 16GB RAM Laptop   â”‚            â”‚ Node 1  â”‚  â”‚ Node 2  â”‚  â”‚ Node 3â”‚ â•‘
â•‘  â”‚ [CORE 1] [....]   â”‚            â”‚ [Part 1]â”‚  â”‚ [Part 2]â”‚  â”‚ [Part]â”‚ â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â–²â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â–²â”€â”€â”€â”€â”˜  â””â”€â”€â”€â–²â”€â”€â”€â”˜ â•‘
â•‘            â”‚                           â”‚            â”‚           â”‚     â•‘
â•‘  DATA: 20GB CSV                        â”‚            â”‚           â”‚     â•‘
â•‘  RESULT: MemoryError ğŸ’¥           DATA: 20GB CSV (Distributed)        â•‘
â•‘                                   RESULT: Processed in 10s âœ…         â•‘
â•‘                                                                       â•‘
â•‘  "Vertical Scaling Limit"         "Horizontal Scaling (Infinite)"     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## Question 2 â€” Data Engineer Level (The Internals)

**"Explain to me what actually happens when I run a `groupBy().count()` in PySpark. Walk me through the stages, the role of the Driver, and what happens during the Shuffle."**

# Feedback First

## What You Nailed âœ…
*   **Driver's Role:** Correctly identified the Driver as the planner (DAG creator) and scheduler.
*   **Narrow vs Wide Transformation:** Correctly identified that `map` is narrow (local) and `groupBy` is wide (shuffle).
*   **Shuffle Cost:** Correctly noted that Shuffle is expensive because it involves disk I/O and network transfer.

## What to Tighten Up ğŸ”§
*   *The Exchange:* You missed the specific mechanism. The Shuffle involves an **Exchange**.
    *   *Map Side:* Executors write data to local disk, partitioned by the hash of the key.
    *   *Reduce Side:* Other Executors pull (fetch) those specific partitions over the network.
*   *Task Boundaries:* Stages are broken *at the shuffle*. This is a key concept. "Spark cannot proceed to Stage 2 until Stage 1 (Map/Write) is complete."

# Model Answer

---

"When you run a `groupBy().count()`, Spark executes a **Wide Transformation**, which triggers a **Shuffle**.

**Phase 1: The Plan (Driver)**
The Driver looks at the code. `groupBy` requires all records with the same key (e.g., 'ZipCode') to be on the same machine. It creates a DAG with two **Stages**.

**Phase 2: Stage 1 (The Map Side)**
The Executors read their local data partitions. They calculate partial counts (combiner optimization). Then, they write these intermediate results to **Local Disk**, sorted into buckets based on the hash of the ZipCode. This is the **Shuffle Write**.

**Phase 3: The Exchange (Network)**
This is the bottleneck. Stage 2 Executors invoke a **Shuffle Fetch**. They reach out over the network and pull the specific buckets they are responsible for (e.g., Executor B pulls all 'ZipCode=22102' chunks from all other nodes).

**Phase 4: Stage 2 (The Reduce Side)**
Now that Executor B has *all* the data for 'ZipCode=22102', it performs the final aggregation (`sum(counts)`) in memory and sends the result back to the Driver or writes to S3."

# Diagrams

```text
THE ANATOMY OF A SHUFFLE
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  STAGE 1: MAP (Local Compute)                                         â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â•‘
â•‘  â”‚ Executor A   â”‚                 â”‚ Executor B   â”‚                    â•‘
â•‘  â”‚ [Zip: 22102] â”‚                 â”‚ [Zip: 90210] â”‚                    â•‘
â•‘  â”‚ [Zip: 90210] â”‚                 â”‚ [Zip: 22102] â”‚                    â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                    â•‘
â•‘         â”‚ Hashes keys & writes           â”‚                            â•‘
â•‘         â–¼ to local disk buckets          â–¼                            â•‘
â•‘  [Disk A: Bkt1, Bkt2]             [Disk B: Bkt1, Bkt2]                â•‘
â•‘                                                                       â•‘
â•‘  ================= SHUFFLE BARRIER (Network Speed) ================== â•‘
â•‘         Condition: ALL Map Tasks must finish first!                   â•‘
â•‘  ==================================================================== â•‘
â•‘                                                                       â•‘
â•‘  STAGE 2: REDUCE (Aggregation)                                        â•‘
â•‘         â”‚                                â”‚                            â•‘
â•‘         â”‚ Executor C reads Bkt1          â”‚ Executor D reads Bkt2      â•‘
â•‘         â”‚ from A and B                   â”‚ from A and B               â•‘
â•‘         â–¼                                â–¼                            â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â•‘
â•‘  â”‚ Executor C   â”‚                 â”‚ Executor D   â”‚                    â•‘
â•‘  â”‚ (Holds all   â”‚                 â”‚ (Holds all   â”‚                    â•‘
â•‘  â”‚  22102 data) â”‚                 â”‚  90210 data) â”‚                    â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â•‘
â•‘       Output:                          Output:                        â•‘
â•‘     22102 -> 500 records             90210 -> 300 records             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## Question 3 â€” Lead Level (Optimization)

**"I have a PySpark job joining a small table (100MB) with a massive table (1TB). It's running slow due to shuffling. How do you optimize this?"**

# Feedback First

## What You Nailed âœ…
*   **Broadcast Join:** This is the exact answer. "Send the small table to everyone to avoid moving the big table."
*   **Spark Config:** Mentioning `spark.sql.autoBroadcastJoinThreshold`.

## What to Tighten Up ğŸ”§
*   *The "Why":* Explain *why* it's faster. It converts a **Sort-Merge Join** (Wide Transformation, Shuffle required) into a **Broadcast Hash Join** (Narrow Transformation, Map-side only).
*   *Memory Risk:* A Lead Engineer would mention the risk: If that "small" table turns out to be bigger than Executor memory + Overhead, you will get an OutOfMemory (OOM) error and crash the cluster.
*   *Skew:* Even if you don't broadcast, mention **Data Skew**. If one key has most of the data, one executor will work while others wait. Salting solves this.

# Model Answer

---

"This is the classic use case for a **Broadcast Hash Join**.

** The Problem (Sort-Merge Join):**
By default, Spark tries to shuffle the 1TB table across the network to align keys with the 100MB table. Moving 1TB takes hours. This is inefficient because we are moving a mountain to meet a pebble.

**The Solution (Broadcast Join):**
We force Spark to copy the 100MB table to **every single executor**.
`df_big.join(broadcast(df_small), "id")`

**Why It Wins:**
1.  **No Shuffle:** The 1TB table stays exactly where it is (Data Locality).
2.  **Narrow Transformation:** Each executor joins its local slice of the big table against its local copy of the small table entirely in memory.
3.  **Speedup:** This turns an O(Network) operation into an O(Memory) operation. It's often 10x-50x faster.

**Caveat:** The small table *must* fit in driver and executor memory. If it grows to 5GB, this will crash the cluster (OOM). I would check the `spark.sql.autoBroadcastJoinThreshold` (default is 10MB) to ensure it's tuned correctly for my cluster size."

# Diagrams

```text
SORT-MERGE JOIN vs. BROADCAST JOIN
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  STANDARD JOIN (Sort-Merge)    |  BROADCAST JOIN (Map-Side)           â•‘
â•‘  (Moves the 1TB Table)         |  (Moves the 100MB Table)             â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    |  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â•‘
â•‘                                |                                      â•‘
â•‘  Node 1: [Big Data] â”€â”€â–º Net    |  Node 1: [Big Data]  â—„â”€â”€ [Small copy]â•‘
â•‘  Node 2: [Big Data] â”€â”€â–º Net    |  Node 2: [Big Data]  â—„â”€â”€ [Small copy]â•‘
â•‘  Node 3: [Big Data] â”€â”€â–º Net    |  Node 3: [Big Data]  â—„â”€â”€ [Small copy]â•‘
â•‘                                |                                      â•‘
â•‘      SHUFFLE (Slow ğŸ¢)         |      NO SHUFFLE (Fast ğŸ‡)            â•‘
â•‘                                |                                      â•‘
â•‘  Result:                       |  Result:                             â•‘
â•‘  All data moves over wire.     |  Only small data moves.              â•‘
â•‘  Network Bottleneck.           |  Cpu Bound (Fast).                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```
