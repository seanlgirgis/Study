# Mock Interview â€” Scala Spark

Same rules â€” answer in the room, model answer and diagrams after each response.

---

## Question 1 â€” Warming Up (The "Why Scala" Check)

**"I see you have PySpark on your resume, but this is a Scala shop. Why would Capital One prefer Scala over Python for its core data pipelines?"**

# Feedback First

## What You Nailed âœ…
*   **Performance:** Correctly identified that Scala runs natively on the JVM, avoiding the serialization overhead of PySpark.
*   **Type Safety:** You nailed the "Compile-time Error vs Runtime Error" argument. "Catching bugs before deploying to production."
*   **Functional Programming:** Mentioned immutability and concurrency safety as key benefits.

## What to Tighten Up ğŸ”§
*   *Datasets vs DataFrames:* Mention **Datasets**. In Python, you only have DataFrames (untyped rows). In Scala, you have `Dataset[Transaction]`. This allows the compiler to check your data transformations.
*   *UDF Performance:* If you write a Python UDF, it's slow (serialization). If you write a Scala UDF, it's native. This is the #1 reason to switch for complex logic.

# Model Answer

---

"There are two main reasons: **Type Safety and Performance**.

**1. Type Safety (Compile Time vs Runtime):**
In PySpark, if I misspell a column name or try to add a String to an Integer, the code might run for 3 hours before crashing. In Scala, the compiler catches that error immediately.
With `Dataset[Transaction]`, I know exactly what my data looks like. This prevents 'Schema Drift' and silent data corruption in production pipelines.

**2. Performance (Native Execution):**
Spark is written in Scala. When I write Scala code, it compiles to Java Bytecode and runs directly on the executor JVM.
When I use PySpark, there's a serialization layer (Py4J) where data has to move between the JVM and the Python processes. For standard ETL, Catalyst optimizes this away. But for complex UDFs or custom logic, Scala is significantly faster because it avoids that overhead."

# Diagrams

```text
THE SAFETY NET: DATASETS vs DATAFRAMES
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  PYTHON (DataFrame)                SCALA (Dataset[T])                 â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â•‘
â•‘  â”‚ df.filter(        â”‚             â”‚ ds.filter(        â”‚              â•‘
â•‘  â”‚   col("amount")   â”‚             â”‚   t => t.amount   â”‚              â•‘
â•‘  â”‚   > "100"         â”‚             â”‚   > 100           â”‚              â•‘
â•‘  â”‚ )                 â”‚             â”‚ )                 â”‚              â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â•‘
â•‘  Runtime: CRASH ğŸ’¥                 Compile Time: ERROR ğŸ›‘             â•‘
â•‘  "String is not Int"               "Type Mismatch"                    â•‘
â•‘                                    (Refuses to build)                 â•‘
â•‘  Result:                           Result:                            â•‘
â•‘  Production Incident               Fixed by Developer                 â•‘
â•‘  at 3:00 AM                        at 2:00 PM                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## Question 2 â€” Data Engineer Level (The "Option" Pattern)

**"In Java, we deal with NullPointerExceptions constantly. How does Scala handle nulls differently in a Spark Dataset? Explain the `Option` type."**

# Feedback First

## What You Nailed âœ…
*   **The Container:** Correctly described `Option` as a "Container" or "Box" that may or may not have a value.
*   **No Nulls:** Correctly stated that you should almost never use the word `null` in Scala code.
*   **Pattern Matching:** Valid syntax `case Some(x) => ... case None => ...`.

## What to Tighten Up ğŸ”§
*   *Monadic Operations:* You handled it with `match`, but a Senior dev uses `map` and `flatMap`.
    *   *Junior:* `if (opt.isDefined) opt.get()`
    *   *Senior:* `opt.map(x => x * 2).getOrElse(0)`
    *   Explain that you can perform operations *on the box* without opening it. If it's empty, the operation just doesn't happen.

# Model Answer

---

"Scala solves the Billion Dollar Mistake (Nulls) using the **Option Monad**.

Instead of a variable being `String` (which might be null), it is defined as `Option[String]`. This forces the developer to handle both cases:
1.  `Some("value")` â€” The data exists.
2.  `None` â€” The data is missing.

**Why this matters in Spark:**
When processing dirty data, nulls are inevitable. In Java, a null check is easy to forget, leading to a NullPointerException that crashes the executor.
In Scala, I can write safe transformations:
`val result = potentialValue.map(v => calculate(v)).getOrElse(0)`

If the data is there, it calculates. If it's `None`, it safely returns a default. The compiler forces me to handle the missing data case, making the pipeline robust against bad input."

# Diagrams

```text
THE OPTION MONAD (THE MAGIC BOX)
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   VARIABLE: Option[Int]                                                â•‘
â•‘                                                                       â•‘
â•‘  SCENARIO A: Has Data              SCARIO B: No Data                  â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â•‘
â•‘  â”‚ Some(10)     â”‚                  â”‚ None         â”‚                   â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                   â•‘
â•‘         â”‚ .map(x => x * 2)                â”‚ .map(x => x * 2)          â•‘
â•‘         â–¼                                 â–¼                           â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â•‘
â•‘  â”‚ Some(20)     â”‚                  â”‚ None         â”‚                   â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â•‘
â•‘         â”‚                                 â”‚                           â•‘
â•‘         â”‚ NO CRASH âœ…                    â”‚ NO CRASH âœ…               â•‘
â•‘         â”‚ Logic Applied                   â”‚ Logic Skipped             â•‘
â•‘                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## Question 3 â€” Lead Level (Optimization)

**"I have a Scala Spark job that uses a custom `UserDefinedFunction` (UDF) to encrypt PII data. It's running slow. How do you optimize this? Do we switch to Pandas UDFs?"**

# Feedback First

## What You Nailed âœ…
*   **UDF Black Box:** Correctly identified that UDFs are opaque to Catalyst. Spark can't optimize what happens inside the function.
*   **Pandas UDF:** Correctly identified that Pandas UDF (Arrow) is for PySpark, not Scala. In Scala, a native UDF is already fast.

## What to Tighten Up ğŸ”§
*   *Native Spark Functions:* The first answer should always be: **"Can we do this without a UDF?"**
    *   Use `spark.sql.functions.sha2` instead of a custom encryption function. Native functions generate optimized bytecode via CodeGen.
*   *Typed Transformations:* Since we are in Scala, use `Dataset.map()` instead of a registered UDF. It's safer and allows for JVM optimization (Just-In-Time compilation).
*   *Code Generation:* Explain that Catalyst generates Java styling code for native expressions. It can't do that for your custom Scala function.

# Model Answer

---

"The first rule of Spark Optimization is: **Don't use UDFs if you can avoid them.**

**Step 1: Check Native Functions**
Catalyst optimizer cannot see inside a UDF. It can't reorder filters or push down predicates. It treats your UDF as a black box.
If I'm just encrypting data, I should check if `org.apache.spark.sql.functions` has a native method (like `sha2`, `base64`, `aes_encrypt`). Native functions are optimized by Tungsten code generation.

**Step 2: Use Dataset Transformations**
If I *must* use custom logic, I prefer a Typed Dataset transformation (`ds.map(user => encrypt(user))`) over registering a SQL UDF.
*   It's type-safe.
*   It allows the JIT (Just-In-Time) compiler to optimize the bytecode at runtime.

**Step 3: Avoid Object Creation**
Inside the UDF, are we creating a new `Encryption` object for every row? That's millions of objects for Garbage Collection. I would move the initialization outside the map function (using `mapPartitions`) so we reuse the encryption object for the entire partition."

# Diagrams

```text
UDF vs NATIVE FUNCTIONS
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  QUERY: df.select(my_udf(col("id")))                                  â•‘
â•‘                                                                       â•‘
â•‘  SCENARIO A: CUSTOM UDF            SCENARIO B: NATIVE FUNCTION        â•‘
â•‘  (Black Box)                       (Transparent Box)                  â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â•‘
â•‘  â”‚ Catalyst     â”‚                  â”‚ Catalyst     â”‚                   â•‘
â•‘  â”‚ Optimizer    â”‚                  â”‚ Optimizer    â”‚                   â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                   â•‘
â•‘         â”‚                                 â”‚                           â•‘
â•‘         â–¼                                 â–¼                           â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
â•‘  â”‚ Row -> JVM   â”‚                  â”‚ Generates optimized Java Code â”‚  â•‘
â•‘  â”‚ Call Functionâ”‚                  â”‚ (Whole-Stage CodeGen)         â”‚  â•‘
â•‘  â”‚ Return Row   â”‚                  â”‚ "while(rows) { hash(id) }"    â”‚  â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
â•‘         â”‚                                 â”‚                           â•‘
â•‘         â–¼                                 â–¼                           â•‘
â•‘  SLOW (Row-by-Row overhead)        FAST (CPU-optimized loop)          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```
