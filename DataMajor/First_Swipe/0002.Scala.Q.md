## Question 1 â€” Warm Up

**"Can you explain the difference between `val` and `var` in Scala, and why immutability matters in a distributed computing environment like Spark?"**



**Core definition** â€” val vs var distinction was clean and correct.

**Thread safety** â€” excellent point, exactly what interviewers want to hear in a distributed computing context.

**Serialization** â€” this is an advanced point that most candidates miss. Bringing this up shows you understand how Spark actually ships code across nodes. Strong.

---

## What to Tighten Up ðŸ”§

**Lineage** â€” you mentioned it but didn't connect it clearly. The stronger way to say it is:

*"Spark builds a DAG of transformations called a lineage graph. Because data is immutable, Spark can recompute any partition from scratch if a node fails â€” it just replays the transformation chain. If data were mutable, that fault tolerance guarantee would break."*

**"Predictive side effects"** â€” I think you meant **predictable** â€” no side effects. The correct term is **referential transparency** â€” a function always returns the same output for the same input, with no hidden state changes. That term will impress an interviewer.

---

## Model Answer for Reference

*"Val creates an immutable reference â€” once assigned it cannot be reassigned. Var creates a mutable variable. In Spark specifically, immutability matters for three reasons. First, thread safety â€” when Spark distributes work across hundreds of executors running in parallel, immutable data eliminates race conditions. Second, fault tolerance â€” Spark's lineage mechanism can recompute lost partitions by replaying transformations, but only because the underlying data never changed. Third, serialization â€” immutable objects are simpler and safer to serialize and ship across the network to worker nodes."*


---
---
---

## Question 2 â€” Stepping Up

**"What is the difference between a DataFrame and a Dataset in Spark, and when would you choose one over the other in a production pipeline?"**


# Feedback

---

## What You Nailed âœ…

**Type safety distinction** â€” perfectly explained. Compile time vs runtime errors is exactly the right framing.

**JVM objects** â€” correctly identified Dataset as strongly typed JVM objects. Shows deep understanding.

**Serialization cost** â€” excellent that you mentioned the serialize/deserialize overhead. Most candidates miss this completely.

**Language availability** â€” correctly identified DataFrame works across Python, R, Java, Scala while Dataset is Scala/Java only.

**Decision framework** â€” you gave a clear when-to-use-which breakdown which is exactly what interviewers want.

**Hybrid approach** â€” this is a senior-level insight. Mentioning that production pipelines often mix both shows real world thinking.

---

## What to Tighten Up ðŸ”§

**The serialization nuance** â€” you got the direction right but flip it slightly. DataFrames are actually faster because Spark uses its own internal **Tungsten binary format** and **Catalyst optimizer** to optimize execution. Datasets bypass some of those optimizations because they work with JVM objects directly. So the fuller explanation is:

*"DataFrames leverage Spark's Catalyst optimizer and Tungsten execution engine for maximum performance. Datasets, because they work with typed JVM objects, introduce encoder/decoder overhead when converting between JVM objects and Spark's internal binary format."*

**One missing point** â€” **Catalyst Optimizer**. Mentioning that DataFrames benefit from Spark's query optimizer while Datasets partially bypass it would elevate your answer significantly.

---

## Model Answer Addition

*"One thing worth adding â€” under the hood a DataFrame is actually a Dataset[Row]. So they share the same execution engine. The difference is purely at the API level. In production at scale I'd typically read raw data as a DataFrame, validate and cast to a typed Dataset for business logic and transformations, then write back out as a DataFrame for performance. That hybrid pattern gives you type safety where it matters most without sacrificing performance on I/O heavy operations."*
---
---
---

## Question 3 â€” Getting Serious

**"Walk me through how you would handle bad or malformed data in a production Scala Spark pipeline processing millions of Capital One transactions daily. What tools does Scala give you and how would you implement it?"**

---
# Feedback

---

## What You Nailed âœ…

**Dead Letter Queue pattern** â€” excellent opening. Naming the architectural pattern first before diving into implementation shows senior-level thinking. Interviewers love when candidates lead with architecture.

**READ modes** â€” correctly identified PERMISSIVE mode and `_corrupt_record`. Shows real Spark production knowledge.

**Never use inferSchema = true** â€” this is a golden rule in production that junior engineers constantly violate. Saying this unprompted signals experience.

**Amazon Deequ** â€” this is an advanced library that most candidates have never heard of. Mentioning it, especially in a Capital One context where it was actually built by Amazon for their own data quality needs, is a very strong signal.

---

## What to Tighten Up ðŸ”§

**You mentioned the three pieces but didn't connect them into a flow.** Interviewers want to see the pipeline end to end. How does PERMISSIVE mode feed into Deequ? Where does the dead letter queue live? What happens to bad records after they land there?

**Missing the other READ modes** â€” you mentioned PERMISSIVE but didn't contrast it with DROPMALFORMED and FAILFAST. Knowing when to use each one is important:

- `PERMISSIVE` â€” keep bad records, set nulls, store in `_corrupt_record` â€” use in production where data loss is unacceptable
- `DROPMALFORMED` â€” silently drop bad rows â€” use when bad records are expected and unimportant
- `FAILFAST` â€” throw exception immediately on bad data â€” use in testing or critical pipelines where any bad data is unacceptable

**Option and Try not mentioned** â€” we covered these earlier. In a Scala-specific interview you should connect the language features to the pipeline design. Using `Try` to wrap risky parsing operations and `Either` to return meaningful error messages are Scala-specific tools that differentiate your answer from a PySpark answer.

---

## Model Answer â€” Full Connected Flow

*"I'd implement a four-layer defense strategy.*

*First, define an explicit schema â€” never inferSchema in production. This catches structural issues immediately at the schema level.*

*Second, use PERMISSIVE read mode so malformed records don't kill the job. Spark stores those rows in the `_corrupt_record` column while nulling out bad fields, keeping the pipeline running.*

```scala
val schema = StructType(Seq(
  StructField("id",       StringType,  nullable = false),
  StructField("amount",   DoubleType,  nullable = false),
  StructField("country",  StringType,  nullable = true),
  StructField("status",   StringType,  nullable = true)
))

val df = spark.read
  .schema(schema)
  .option("mode", "PERMISSIVE")
  .option("columnNameOfCorruptRecord", "_corrupt_record")
  .csv("s3://capitalone/raw/transactions/")
```

*Third, split good records from bad immediately:*

```scala
val goodRecords = df.filter(col("_corrupt_record").isNull)
val badRecords  = df.filter(col("_corrupt_record").isNotNull)

// Route bad records to Dead Letter Queue
badRecords.write
  .mode("append")
  .partitionBy("year", "month", "day")
  .json("s3://capitalone/dead-letter-queue/")
```

*Fourth, run Amazon Deequ validation on good records for business rule violations â€” things the schema can't catch like negative amounts or invalid country codes:*

```scala
import com.amazon.deequ.VerificationSuite
import com.amazon.deequ.checks.{Check, CheckLevel}

val verificationResult = VerificationSuite()
  .onData(goodRecords)
  .addCheck(
    Check(CheckLevel.Error, "Transaction Validation")
      .isComplete("id")
      .isComplete("amount")
      .isNonNegative("amount")
      .isContainedIn("status", Array("approved", "declined", "flagged"))
      .isContainedIn("country", Array("US", "UK", "CA", "DE", "FR"))
  )
  .run()
```

*Finally use Try and Either at the record level for any custom parsing logic:*

```scala
def parseTransaction(raw: Row): Either[String, Transaction] = {
  Try {
    Transaction(
      id       = raw.getAs[String]("id"),
      amount   = raw.getAs[Double]("amount"),
      country  = raw.getAs[String]("country"),
      status   = raw.getAs[String]("status")
    )
  } match {
    case Success(t)  => Right(t)
    case Failure(ex) => Left(s"Parse failed for row $raw: ${ex.getMessage}")
  }
}
```

*This gives you four layers â€” schema validation, structural parsing, business rule validation, and record level error handling. Bad records never silently corrupt your output and are always traceable back to the source."*

---
---
---

## Question 4 â€” Lead Engineer Level

**"You have a Scala Spark job running on EMR that processes 500 million Capital One transactions daily. It's been running fine for months but suddenly starts taking 3x longer to complete. How do you diagnose and fix it?"**
