## Question 1 â€” Warm Up

**"Can you explain the difference between `val` and `var` in Scala, and why immutability matters in a distributed computing environment like Spark?"**



**Core definition** â€” val vs var distinction was clean and correct.

**Thread safety** â€” excellent point, exactly what interviewers want to hear in a distributed computing context.

**Serialization** â€” this is an advanced point that most candidates miss. Bringing this up shows you understand how Spark actually ships code across nodes. Strong.

---

## What to Tighten Up ðŸ”§

**Lineage** â€” you mentioned it but didn't connect it clearly. The stronger way to say it is:

*"Spark builds a DAG of transformations called a lineage graph. Because data is immutable, Spark can recompute any partition from scratch if a node fails â€” it just replays the transformation chain. If data were mutable, that fault tolerance guarantee would break."*

**"Predictive side effects"** â€” I think you meant **predictable** â€” no side effects. The correct term is **referential transparency** â€” a function always returns the same output for the same input, with no hidden state changes. That term will impress an interviewer.

---

## Model Answer for Reference

*"Val creates an immutable reference â€” once assigned it cannot be reassigned. Var creates a mutable variable. In Spark specifically, immutability matters for three reasons. First, thread safety â€” when Spark distributes work across hundreds of executors running in parallel, immutable data eliminates race conditions. Second, fault tolerance â€” Spark's lineage mechanism can recompute lost partitions by replaying transformations, but only because the underlying data never changed. Third, serialization â€” immutable objects are simpler and safer to serialize and ship across the network to worker nodes."*


---
---
---

## Question 2 â€” Stepping Up

**"What is the difference between a DataFrame and a Dataset in Spark, and when would you choose one over the other in a production pipeline?"**
